<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.269">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Probability (BST 230) Notes - 7&nbsp; Week 6</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../week5/week5.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-title">Week 6</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Probability (BST 230) Notes</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Methods (BST 232) Notes</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../week1/week1.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Week 1</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../week2/week2.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Week 2</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../week3/week3.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Week 3</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../week4/week4.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Week 4</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../week5/week5.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Week 5</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../week6/week6.html" class="sidebar-item-text sidebar-link active"><span class="chapter-title">Week 6</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#recap" id="toc-recap" class="nav-link active" data-scroll-target="#recap">Recap</a></li>
  <li><a href="#dependence-and-independence" id="toc-dependence-and-independence" class="nav-link" data-scroll-target="#dependence-and-independence">Dependence and Independence</a>
  <ul class="collapse">
  <li><a href="#monty-hall-example" id="toc-monty-hall-example" class="nav-link" data-scroll-target="#monty-hall-example">Monty Hall Example</a></li>
  <li><a href="#joint-cdfs" id="toc-joint-cdfs" class="nav-link" data-scroll-target="#joint-cdfs">Joint CDFs</a></li>
  <li><a href="#properties-of-independent-random-variables" id="toc-properties-of-independent-random-variables" class="nav-link" data-scroll-target="#properties-of-independent-random-variables">Properties of independent random variables</a></li>
  <li><a href="#conditional-expectations" id="toc-conditional-expectations" class="nav-link" data-scroll-target="#conditional-expectations">Conditional Expectations</a>
  <ul class="collapse">
  <li><a href="#caveats-interpretation" id="toc-caveats-interpretation" class="nav-link" data-scroll-target="#caveats-interpretation">Caveats, Interpretation</a></li>
  </ul></li>
  <li><a href="#conditional-distributions" id="toc-conditional-distributions" class="nav-link" data-scroll-target="#conditional-distributions">Conditional Distributions</a>
  <ul class="collapse">
  <li><a href="#basic-properties" id="toc-basic-properties" class="nav-link" data-scroll-target="#basic-properties">Basic Properties</a></li>
  <li><a href="#conditional-expectations-as-random-variables" id="toc-conditional-expectations-as-random-variables" class="nav-link" data-scroll-target="#conditional-expectations-as-random-variables">Conditional expectations as random variables</a></li>
  <li><a href="#law-of-total-expectation-law-of-total-variance" id="toc-law-of-total-expectation-law-of-total-variance" class="nav-link" data-scroll-target="#law-of-total-expectation-law-of-total-variance">Law of Total Expectation &amp; Law of Total Variance</a></li>
  </ul></li>
  </ul></li>
  <li><a href="#review-for-the-midterm" id="toc-review-for-the-midterm" class="nav-link" data-scroll-target="#review-for-the-midterm">Review for the Midterm</a>
  <ul class="collapse">
  <li><a href="#sigma-algebras" id="toc-sigma-algebras" class="nav-link" data-scroll-target="#sigma-algebras">Sigma Algebras</a></li>
  <li><a href="#inverse-probability-transform" id="toc-inverse-probability-transform" class="nav-link" data-scroll-target="#inverse-probability-transform">Inverse Probability Transform</a></li>
  <li><a href="#location-scale-families" id="toc-location-scale-families" class="nav-link" data-scroll-target="#location-scale-families">Location Scale Families</a></li>
  <li><a href="#positive-and-negative-part-of-expectation" id="toc-positive-and-negative-part-of-expectation" class="nav-link" data-scroll-target="#positive-and-negative-part-of-expectation">Positive and Negative Part of Expectation</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-title">Week 6</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<section id="recap" class="level1">
<h1>Recap</h1>
<p>Marginal and Conditional pdfs</p>
<p>If <span class="math inline">\((X,Y)\)</span> is continuous, then <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are continuous random variables and their marginal pfs are just their pdfs as random variables.</p>
<p>Marginal pdfs can be expressed in terms of the joint pdf:</p>
<p><span class="math display">\[
f_X(x) = p(x) = \int p(x,y) dy
\]</span></p>
<p><span class="math display">\[
f_Y(y) = p(y) = \int p(x,y) dx
\]</span></p>
<p>This is similar to the discrete case, but with integrals.</p>
<p>For <span class="math inline">\(y\)</span> such that <span class="math inline">\(p(y) &gt; 0\)</span>, the conditional pdf of <span class="math inline">\(X\)</span> given <span class="math inline">\(Y\)</span> is defined as:</p>
<p><span class="math display">\[
f_{X|Y} (x | y) = p(x|y) = \frac{p(x,y)}{p(y)}.
\]</span></p>
<p>Likewise, for <span class="math inline">\(x\)</span> such that <span class="math inline">\(p(x) &gt; 0\)</span>,</p>
<p><span class="math display">\[
f_{Y|X}(y|x) = p(y|x) = \frac{p(x,y)}{p(x)}.
\]</span></p>
<p>Thus, technically the conditional pdf is only defined when the marginal pdf is <span class="math inline">\(&gt;0.\)</span></p>
<div class="cooltip">
<p>Recall that <span class="math inline">\(f(y) \neq P(Y=y)\)</span> because if <span class="math inline">\(Y\)</span> is a continuous random variable, the probability <span class="math inline">\(Y\)</span> takes on a particular value is vanishing (0). During our lectures and notes we might write that <span class="math inline">\(p(x|y) = p(x|Y=y)\)</span> but we don’t mean this literally. For now, we can think about it as “like” conditioning on <span class="math inline">\(p(y)\)</span> but it’s quite a bit more subtle mathematically and requires a measure-theoretic treatment that we won’t get into in this course.</p>
</div>
</section>
<section id="dependence-and-independence" class="level1">
<h1>Dependence and Independence</h1>
<p>Random variables <span class="math inline">\(X_1, …, X_n\)</span> are <span class="vocab">independent</span> if</p>
<p><span class="math display">\[
P(X_1 \in A_1, ..., X_n \in A_n) = P(X_1 \in A_1) \cdots P(X_n \in A_n)
\]</span></p>
<p>for all (measurable) subsets <span class="math inline">\(A_1, …, A_n \subset \mathbb R\)</span>.</p>
<p>Intuitively, independence captures the idea that <span class="math inline">\(X_1, …, X_n\)</span> contain no information about one another.</p>
<p>In terms of the pdf/pmf, <span class="math inline">\(X_1, …, X_n\)</span> are independent if</p>
<p><span class="math display">\[
f(x_1, ..., x_d) = f_{X_1}(x_1) \cdots f_{X_n}(x_d)
\]</span></p>
<p>for all <span class="math inline">\(x_1, …, x_d\)</span>.</p>
<p>This isn’t strictly an if-and-only-if statement. For pdfs, we won’t always be able to factor the pdf doesn’t mean that <span class="math inline">\(X_1, …, X_n\)</span> are independent.</p>
<div class="cooltip">
<p>Recall that pdfs are not unique. We can modify them on measure zero sets to come up with a different pdf that describes the same random variable. There may be some pdfs for independent variables that do not factor, but there should be some expression of these variables that does factor. This makes this a relatively unimportant, though subtle, point.</p>
</div>
<p>Let <span class="math inline">\(X, Y \sim \text{Uniform}(0,1)\)</span> be independent. Then we have that</p>
<p><span class="math display">\[f(x,y) = \mathbb 1(0 &lt; x,y&lt; 1)\]</span></p>
<p><span class="math display">\[
f(x) = \mathbb 1(0 &lt; x &lt; 1)
\]</span></p>
<p><span class="math display">\[
f(y) = \mathbb 1(0 &lt; y &lt; 1)
\]</span></p>
<p><span class="math display">\[
\tilde{f}(x) = \mathbb 1(0 &lt; x &lt; 1) + \underline{\mathbb 1 \left(x = \frac{1}{2}\right)}
\]</span></p>
<p>The underlined part adds nothing to the pdf since it occurs with measure zero. We couldn’t necessarily factor <span class="math inline">\(f(x,y) = \tilde{f}(x) f(y)\)</span>.</p>
<p>The opposite of the statement above would require an additional caveat of “if there exists <span class="math inline">\(f_{X_1}, … f_{X_d}\)</span>.”</p>
<section id="monty-hall-example" class="level3">
<h3 class="anchored" data-anchor-id="monty-hall-example">Monty Hall Example</h3>
<p>What would the joint distribution be if <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> had the same marginal distributions as before, but they were independent?</p>
<p>Independence requires <span class="math inline">\(p(x,y) = p(x)p(y)\)</span> for all <span class="math inline">\(x,y\)</span>. Therefore:</p>
<table class="table">
<thead>
<tr class="header">
<th></th>
<th>Monty opens #1</th>
<th>Monty opens #2</th>
<th>Monty opens #3</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Car is behind 1</td>
<td>0</td>
<td>1/6</td>
<td>1/6</td>
</tr>
<tr class="even">
<td>Car is behind 2</td>
<td>0</td>
<td>1/6</td>
<td>1/6</td>
</tr>
<tr class="odd">
<td>Car is behind 3</td>
<td>0</td>
<td>1/6</td>
<td>1/6</td>
</tr>
</tbody>
</table>
<p>The joint-pdf or joint-pmf should look like a concept from linear algebra: an outer product of the probabilities at each realizable value.</p>
<p>If we look at a heatmap of the joint pmf of random variables <span class="math inline">\(x,y\)</span>, we would expect to see striation in a heatmap of independent probabilities.</p>
</section>
<section id="joint-cdfs" class="level3">
<h3 class="anchored" data-anchor-id="joint-cdfs">Joint CDFs</h3>
<p>The joint cdf of a random vector <span class="math inline">\((X_1, …, X_d)\)</span> is</p>
<p><span class="math display">\[
F(x_1, ..., x_d) = P(X_1 \leq x_1, ..., X_d \leq x_d).
\]</span></p>
<p>For instance, in the bivariate case, the joint cdf of <span class="math inline">\((X,Y)\)</span> is</p>
<p><span class="math display">\[
F(x,y) = P(X \leq x, Y \leq y).
\]</span></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="standalone_figures/joint_cdf/joint_cdf.svg" class="img-fluid figure-img" alt="The cdf of a joint distribution on two variables is an integral over a rectangular region that stretches from -infinity up to the point considered in both x,y"></p>
</figure>
</div>
</section>
<section id="properties-of-independent-random-variables" class="level3">
<h3 class="anchored" data-anchor-id="properties-of-independent-random-variables">Properties of independent random variables</h3>
<p><span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent if and only if for all (measurable) functions <span class="math inline">\(g(x)\)</span> and <span class="math inline">\(h(y)\)</span>,</p>
<p><span class="math display">\[
\mathbb E(g(X)h(Y)) = (\mathbb Eg(X))(\mathbb Eh(Y)).
\]</span></p>
<p>Proof:</p>
<p>First we’ll look at the forward direction:</p>
<p>Let’s consider the case where we define <span class="math inline">\(g(x)\)</span> and <span class="math inline">\(h(x)\)</span> such that it applies in the setting of:</p>
<p><span class="math display">\[
P(X \in A, Y \in B) = P(X \in A)P(Y \in B).
\]</span></p>
<p>In other words, we’re considering <span class="math inline">\(g(x) = \mathbb 1(x \in A)\)</span> and <span class="math inline">\(h(y) = \mathbb 1(y \in B)\)</span>.</p>
<p>It’s useful to recall that the probability of any event is equal to the probability of the indicator of that event.</p>
<p>Thus</p>
<p><span class="math display">\[
P(X \in A, Y \in B) \text{ becomes } \mathbb E(\mathbb 1(X \in A, Y \in B)).
\]</span></p>
<p>One may want to recall and apply an earlier definition for probability of events to see that :</p>
<p><span class="math display">\[
P(X \in A, Y \in B) \stackrel{def}{=} P(\{ s \in S \colon X(s) \in A, Y(s) \in B \}).
\]</span></p>
<p>Which we can rewrite:</p>
<p><span class="math display">\[
= \mathbb E(\mathbb 1(X \in A) \mathbb 1(Y \in B)) = (\mathbb E(\mathbb 1(X \in A)) (\mathbb E (\mathbb 1(Y \in B))).
\]</span></p>
<p><span class="math display">\[
= P(X \in A) P(Y \in B)
\]</span></p>
<p>Now for the other direction.</p>
<p>Assume that <span class="math inline">\(X,Y\)</span> are independent: do we get that <span class="math inline">\(f(x,y) = f(x) f(y)\)</span>.</p>
<p><span class="math display">\[
\underbrace{\mathbb E g(X) h(Y)}_{=\int g(x) h(y) f(x,y) dxdy} \stackrel{?}{=} (\mathbb E g(X)) (\mathbb E h(Y))
\]</span></p>
<p><span class="math display">\[
=\int g(x) h(y) f(x) f(y) dx dy
\]</span></p>
<p><span class="math display">\[
= \int \left( \int g(x) f(x) dx \right) h(y) f(y) dy
\]</span></p>
<p><span class="math display">\[
= \left( \int g(x) f(x) dx \right) \left( \int h(y) f(y) dy \right)
\]</span></p>
<p><span class="math display">\[
= \left( \mathbb E g(X) \right) \left( \mathbb E h(Y) \right).
\]</span></p>
<p>If <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent, then</p>
<p><span class="math display">\[
\text{Var}(X + Y) = \text{Var}(X) + \text{Var}(Y).
\]</span></p>
<p>If <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are independent then</p>
<p><span class="math display">\[
M_{X+Y}(t) = M_X(t) M_Y(t).
\]</span></p>
<p>We can see that this would be the case from the fact that we can apply the statement above (<span class="math inline">\(\mathbb E (g(X) h(Y)) = \mathbb E (g(X)) \mathbb E (h(Y))\)</span>).</p>
<p><span class="math display">\[
M_{X+Y}(t) = \mathbb E \exp (t(X+Y)) = \mathbb E \exp(tX) \exp (tY) = \mathbb E \exp (tX) \mathbb E (tY) = M_X(t) M_Y(t).
\]</span></p>
<p>A useful observation is how this applies in the setting of the fact that a random variable <span class="math inline">\(X\)</span> is not independent of itself. If it were, then we’d have that <span class="math inline">\(\text{Var}(2X) = \text{Var}(X+X) = \text{Var}(X) + \text{Var}(X) = 2 \text{Var}(X)\)</span>, but instead we have that <span class="math inline">\(\text{Var}(2X) = 4\text{Var}(X)\)</span>.</p>
<div class="cooltip">
<p>Recall that the variance of dependent random variables is</p>
<p><span class="math display">\[\text{Var}(X+Y) = \text{Var}(X) + \text{Var}(Y) + 2\text{Cov}(X,Y).\]</span></p>
</div>
<p>This property of mgfs can simplify derivations of the distribution of a sum of independent random variables.</p>
<p>The mgf of <span class="math inline">\(X \sim \mathcal N(\mu, \sigma^2)\)</span> is</p>
<p><span class="math display">\[
M_X(t) = \exp(\mu t + \frac{1}{s} \sigma^2 t^2).
\]</span></p>
<p>Suppose <span class="math inline">\(X_1 \sim \mathcal N(\mu_1, \sigma^2_1)\)</span> and <span class="math inline">\(X_2 \sim \mathcal N(\mu_2, \sigma^2_2)\)</span> independently. Then</p>
<p><span class="math display">\[
M_{X_1 + X_2}(t) = M_{X_1}(t)M_{X_2}(t)
\]</span></p>
<p><span class="math display">\[
=\exp\left( (\mu_1 + \mu_2) t + \frac{1}{2}(\sigma_1^2 + \sigma^2_2) t^2\right),
\]</span></p>
<p>which is the mgf of <span class="math inline">\(\mathcal N(\mu_1 + \mu_2, \sigma^2_1 + \sigma^2_2)\)</span> for all <span class="math inline">\(t\)</span>.</p>
<p>Therefore</p>
<p><span class="math display">\[
X_1 + X_2 \sim \mathcal N(\mu_1 + \mu_2, \sigma^2_1 + \sigma^2_2).
\]</span></p>
<p>Recall that if two variables have the mgfs that are equal and finite around some interval of the origin, then they are the variables are equal in distribution. (It turns out that if two mgfs are finite and equal on a neighborhood around zero, they must be equal everywhere).</p>
<p>Therefore we’re allowed to make the jump that <span class="math display">\[
Y \sim \mathcal N(\mu_1 + \mu_2, \sigma_1^2 + \sigma_2^2) \Rightarrow X_1 + X_2 \stackrel{d}{=} Y.
\]</span></p>
</section>
<section id="conditional-expectations" class="level2">
<h2 class="anchored" data-anchor-id="conditional-expectations">Conditional Expectations</h2>
<p>Suppose <span class="math inline">\((X,Y)\)</span> is a random vector and <span class="math inline">\(g(x)\)</span> is a measurable function. The conditional expectation of <span class="math inline">\(g(X)\)</span> given that <span class="math inline">\(Y = y\)</span> is</p>
<p><span class="math display">\[
\mathbb E(g(X) | Y = y) = \sum_{x \in \mathcal X} g(x) f_{X|Y} (x|y)
\]</span></p>
<p>in the discrete case and <span class="math display">\[
\mathbb E(g(X) | Y=y) = \int g(x) f_{X|Y} (x|y) dx
\]</span></p>
<p>in the continuous case.</p>
<p>Often we abbreviate $<span class="math inline">\(\mathbb E (g(X) | Y = y)\)</span> as <span class="math inline">\(\mathbb E(g(X)|y)\)</span>.</p>
<p>As before <span class="math inline">\(\mathbb{E}(g(X)|y)\)</span> is defined as long as <span class="math inline">\(f(y) \neq 0\)</span>.</p>
<section id="caveats-interpretation" class="level3">
<h3 class="anchored" data-anchor-id="caveats-interpretation">Caveats, Interpretation</h3>
<p>When <span class="math inline">\(Y\)</span> is a discrete r.v. the conditional expectation is equivalent to conditioning on <span class="math inline">\(Y=y\)</span> as the notation suggests. However, if <span class="math inline">\(Y\)</span> is a continuous random variable, the interpretation is much more subtle since it requires measure theoretic considerations.</p>
</section>
</section>
<section id="conditional-distributions" class="level2">
<h2 class="anchored" data-anchor-id="conditional-distributions">Conditional Distributions</h2>
<p>Suppose <span class="math inline">\((X,Y)\)</span> is a random vector.</p>
<p>The conditional distribution of <span class="math inline">\(X\)</span> given that <span class="math inline">\(Y= y\)</span> is the probability measure <span class="math inline">\(Q\)</span> such that</p>
<p><span class="math display">\[
Q(A) = \mathbb{E}\left( \mathbb 1 (X \in A) \mid Y = y \right)
\]</span></p>
<p>for all measurable sets <span class="math inline">\(A\)</span>.</p>
<p>The conditional expectation <span class="math inline">\(\mathbb{E}(g(X) | Y = y)\)</span> can be thought of as the expected value of <span class="math inline">\(g(X)\)</span> under the conditional distribution of <span class="math inline">\(X\)</span> given <span class="math inline">\(Y = y\)</span>.</p>
<p>That is <span class="math display">\[
\mathbb{E}(g(X) | Y = y) = \mathbb{E}g(\tilde{X}) \text{ where } \tilde{X} \sim Q.
\]</span></p>
<p>This is a very useful way to think of conditional expectations.</p>
<section id="basic-properties" class="level3">
<h3 class="anchored" data-anchor-id="basic-properties">Basic Properties</h3>
<p>In particular,</p>
<ol type="1">
<li><span class="math inline">\(\mathbb{E}cg(X) | y) = c \mathbb{E}(g(X) | y)\)</span> for any <span class="math inline">\(c \in \mathbb{R}\)</span>.</li>
<li><span class="math inline">\(\mathbb{E}(g(X) + h(X) | y) = \mathbb{E}(g(X) | y) + \mathbb{E}(h(X)|y)\)</span></li>
<li><span class="math inline">\(g(x) \leq h(x) \to \mathbb{E}(g(X) | y) \leq \mathbb{E}(h(X)|y)\)</span>.</li>
</ol>
</section>
<section id="conditional-expectations-as-random-variables" class="level3">
<h3 class="anchored" data-anchor-id="conditional-expectations-as-random-variables">Conditional expectations as random variables</h3>
<p>It’s often useful to leave the condition as a random variable.</p>
<p>Consider the function <span class="math inline">\(h(y) = \mathbb{E}(X|Y = y)\)</span>. Then <span class="math inline">\(h(Y)\)</span> is a random variable, denoted <span class="math inline">\(\mathbb{E}(X|Y)\)</span>.</p>
<p>A key property of conditional expectations is that</p>
<p><span class="math display">\[
\mathbb{E}(g(Y) X | Y) = g(Y) \mathbb{E}(X|Y)
\]</span></p>
<p>The basic idea here is that if we were to condition on a particular value of <span class="math inline">\(Y\)</span>, then <span class="math inline">\(g(Y)\)</span> would just be constant on the left-hand-side of the expectation.</p>
</section>
<section id="law-of-total-expectation-law-of-total-variance" class="level3">
<h3 class="anchored" data-anchor-id="law-of-total-expectation-law-of-total-variance">Law of Total Expectation &amp; Law of Total Variance</h3>
<p>Conditional expectations follow two very useful properties:</p>
<p>The <span class="vocab">law of total expectation</span></p>
<p><span class="math display">\[
\mathbb{E}X = \mathbb{E}(\mathbb{E}(X \mid Y))\]</span></p>
<p>And the <span class="vocab">law of total variance</span>:</p>
<p><span class="math display">\[
\text{Var}X = \mathbb{E}( \text{Var}X \mid Y ) + \text{Var}( \mathbb{E}(X \mid Y ))\]</span></p>
<div class="cooltip">
<p>Exercise: Derive these two properties.</p>
<p>Note that <span class="math display">\[ \mathbb{E}(\mathbb{E}(X \mid Y)) = \mathbb{E}\left(\sum x f_{X|Y} (x|y)\right)
\]</span></p>
<p><span class="math display">\[ \mathbb{E}(\mathbb{E}(X \mid Y)) = \sum_{y \in \mathcal Y} \left(\sum_{x \in \mathcal X} x f_{X|Y} (x|y)\right) \cdot P(Y = y)
\]</span></p>
<p>If I recall correctly, <span class="math inline">\(P(X|Y) = P(X,Y)/P(Y)\)</span>.</p>
<p><span class="math display">\[ \mathbb{E}(\mathbb{E}(X \mid Y)) = \sum_{y \in \mathcal Y} \left(\sum_{x \in \mathcal X} x P(X = x, Y = y)\right)
\]</span></p>
<p>Interchange ordering of summation:</p>
<p><span class="math display">\[ \mathbb{E}(\mathbb{E}(X \mid Y)) = \sum_{x \in \mathcal X} \left(\sum_{y \in \mathcal Y} x P(X = x, Y = y)\right)
\]</span></p>
<p>Pull out the <span class="math inline">\(x\)</span> from the inner sum:</p>
<p><span class="math display">\[ \mathbb{E}(\mathbb{E}(X \mid Y)) = \sum_{x \in \mathcal X} x \left(\sum_{y \in \mathcal Y} P(X = x, Y = y)\right)
\]</span></p>
<p><span class="math display">\[ \mathbb{E}(\mathbb{E}(X \mid Y)) = \sum_{x \in \mathcal X} \left( x P(X = x) \right)
\]</span></p>
<p><span class="math display">\[ \mathbb{E}(\mathbb{E}(X \mid Y)) = \mathbb{E}X
\]</span></p>
<hr>
<p>Is there a way to apply</p>
<p><span class="math display">\[
\text{Var}X = \mathbb{E}X^2 - (\mathbb{E}X)^2 \quad ?
\]</span></p>
<p><span class="math display">\[
\text{Var}X = \mathbb{E}X^2 - (\mathbb{E}(\mathbb{E}(X \mid Y)))^2
\]</span></p>
</div>
<p><br></p>
<div class="chilltip">
<p>Answers. Recall our definition:</p>
<p><span class="math display">\[
\mathbb{E}(X \mid Y) = h(Y) \quad \text{where} \quad h(y) = \mathbb{E}(X | Y = y).
\]</span></p>
<p>Insert them into the problem stem:</p>
<p><span class="math display">\[
\begin{aligned}
\mathbb{E}( \mathbb{E}(X \mid Y)) &amp; = \mathbb{E}h(Y) = \sum_{y} h(y) p(y)  \\
&amp; = \sum_y \mathbb{E}(X \mid Y = y) p(y) = \sum_y \sum_x x p(x \mid y) p(y) \\
&amp; = \sum_y \sum_x x p(x,y) = \sum_x x \sum_y p(x,y) \\
&amp; = \sum_x x p(x) = \mathbb{E}X.
\end{aligned}
\]</span></p>
<p>Aside: How would one know what the expectation is with respect to in the outer expectations? The expectation is always with respect to whatever is random in the argument.</p>
<p>Aside #2: Isn’t the <span class="math inline">\(h(Y)\)</span> notation a little bit confusing? What’s the intuition? Well, one can call <span class="math inline">\(h(Y)\)</span> the “average value of <span class="math inline">\(X\)</span> at a given level of <span class="math inline">\(Y\)</span>”.</p>
<p>Maybe a more intuitive way to look at it is to think about the usual setting where we’re regressing <span class="math inline">\(Y\)</span> on <span class="math inline">\(X\)</span>.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="standalone_figures/conditional_expectation/conditional_expectation.svg" class="img-fluid figure-img" alt="conditional expectations given levels of X"></p>
</figure>
</div>
</div>
</section>
</section>
</section>
<section id="review-for-the-midterm" class="level1">
<h1>Review for the Midterm</h1>
<section id="sigma-algebras" class="level3">
<h3 class="anchored" data-anchor-id="sigma-algebras">Sigma Algebras</h3>
<p><span class="math inline">\(\mathcal B\)</span> is a sigma algebra if:</p>
<ol type="1">
<li><span class="math inline">\(\varnothing \in \mathcal B\)</span> (inclusion of the empty set)</li>
<li>if <span class="math inline">\(A \in \mathcal B \longrightarrow A^c \in \mathcal B\)</span> (closure under complements)</li>
<li>if <span class="math inline">\(A_1, A_2, ... \in \mathcal B \longrightarrow \cup_{i=1}^\infty A_i \in \mathcal B\)</span>.</li>
</ol>
<p>Consider the Borel sigma algebra:</p>
<p>If <span class="math inline">\((a,b) \in \mathcal B\)</span>, then <span class="math inline">\((a,b)^c = (-\infty, a] \cup [b, \infty) \in \mathcal B\)</span>.</p>
<p>Similarly, we could write <span class="math inline">\(\cup_{n=1}^\infty (a, n) = (a, \infty) \in \mathcal B\)</span>. Similarly, we could define the union on all of the complements of these sets to see that <span class="math inline">\((-\infty, a] \in \mathcal B\)</span>.</p>
<p>Every one of these sets is mapped to some value in <span class="math inline">\(\mathbb{R}\)</span>.</p>
</section>
<section id="inverse-probability-transform" class="level3">
<h3 class="anchored" data-anchor-id="inverse-probability-transform">Inverse Probability Transform</h3>
<p>Let <span class="math inline">\(F\)</span> be any cdf. If <span class="math inline">\(U \sim \text{Uniform}(0,1)\)</span>, then <span class="math inline">\(F^{-1}(U)\)</span> is a random variable with cdf <span class="math inline">\(F\)</span>.</p>
<p>The two transforms can be summarized as follows. Suppose <span class="math inline">\(U \sim \text{Uniform}(0,1)\)</span> and <span class="math inline">\(X\)</span> is a random variable with cdf <span class="math inline">\(F\)</span>. Then</p>
<ol type="1">
<li><span class="math inline">\(F^{-1}(U) \stackrel{d} = X\)</span></li>
<li><span class="math inline">\(F(X) \stackrel{d} = U\)</span> if <span class="math inline">\(F\)</span> is continuous, but not otherwise.</li>
</ol>
<p>Consider Homework #3 question 1.</p>
<p>Suppose <span class="math inline">\(X\)</span> is a random variable with pdf <span class="math inline">\(f(x) = bx^a \mathbb 1(0 &lt; x &lt; c)\)</span> where <span class="math inline">\(a &gt; 0, b &gt; 0\)</span> and <span class="math inline">\(c &gt; 0\)</span>.</p>
<p>Thus</p>
<p><span class="math display">\[
\begin{aligned}
u = F(x) &amp; = \int_{-\infty}^x f(t) dt = \int_0^x b t^a \mathbb 1 (t &lt; c) dt \\
&amp; = b \frac{t^{a-1}}{a-1} \biggr \lvert_0^x = \frac{b x^{a-1}}{a-1}.
\end{aligned}
\]</span></p>
<p>So <span class="math inline">\(F^{-1}(u) = \left( \frac{u(a-1)}{b} \right)^{\frac{1}{a-1}}\)</span>.</p>
<p>And <span class="math inline">\(F^{-1}(U) \stackrel d = X\)</span> where <span class="math inline">\(X\)</span> has pdf <span class="math inline">\(f(x)\)</span>.</p>
<p>Do note that we can use the inverse probability transform to map to discrete random variables from the uniform distribution, but the probability integral transform (direction 2) doesn’t work because <span class="math inline">\(F\)</span> won’t be continuous.</p>
<div class="chilltip">
<p>Consider <span class="math display">\[X \sim \text{Bernoulli}(1/2)\]</span></p>
<p><img src="standalone_figures/bernoulli_cdf/bernoulli_cdf.svg" class="img-fluid"></p>
<p>The generalized inverse is given as <span class="math display">\[F^{-1}(u) = \mathbb 1(u &gt; 1/2)\]</span></p>
<p><span class="math display">\[
F^{-1}(U) \sim \text{Bernoulli}(1/2)
\]</span></p>
</div>
</section>
<section id="location-scale-families" class="level3">
<h3 class="anchored" data-anchor-id="location-scale-families">Location Scale Families</h3>
<p>Generally the idea is that we start out with a standard pdf <span class="math inline">\(f(x)\)</span> and we look at all the things we can get by scaling and shifting.</p>
<!--
Let's look at the Laplace pdf. 

$$
p(x \mid \mu, s) = \frac{1}{2s} \exp \left( - \frac{1}{s} \lvert x - \mu \rvert \right)
$$

for all $x \in \R$. 
-->
<p>Recall and apply the probability transform formula:</p>
<p><span class="math display">\[Y = sX + m \quad m \in \mathbb{R}, s &gt; 0
\]</span></p>
<p><span class="math display">\[f_Y(y) = f_X(g^{-1}(y)) \lvert \frac{d}{dy} g^{-1}(y) \rvert\]</span></p>
<p><span class="math display">\[
y = g(x) = sx + m
\]</span></p>
<p><span class="math display">\[g^{-1}(y) = \frac{y-m}{s}
\]</span></p>
<p><span class="math display">\[
\frac{d}{dy} g^{-1}(y) = 1/s
\]</span></p>
<p>So <span class="math display">\[
f_Y(y) = \frac{1}{s} f(\frac{y-m}{s}).\]</span></p>
<p>Let’s do an example with the normal distribution.</p>
<p>The standard normal distribution is given as <span class="math display">\[
f(x) = \mathcal N(x \mid 0, 1) = \frac{1}{\sqrt{2\pi}} \exp \left( - \frac{1}{2} x^2 \right)
\]</span></p>
<p><span class="math display">\[
Y = sX + m \sim \mathcal N(m, s^2)
\]</span></p>
<p>What’s an example of something that is that is not a location-scale family? One could say the exponential distributions are not location-scale family because they do not allow for variation in the <span class="math inline">\(m\)</span> parameter.</p>
<p>Recall they’re defined <span class="math display">\[
\lambda e^{-\lambda x} \mathbb 1(x &gt; 0)
\]</span></p>
</section>
<section id="positive-and-negative-part-of-expectation" class="level3">
<h3 class="anchored" data-anchor-id="positive-and-negative-part-of-expectation">Positive and Negative Part of Expectation</h3>
<p>Remember that <span class="math inline">\(\mathbb{E}X\)</span> is well defined if $$ X^+ &lt; $ or <span class="math inline">\(\mathbb{E}X^- &lt; \infty\)</span>.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../week5/week5.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-title">Week 5</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
  </div>
</nav>
</div> <!-- /content -->



</body></html>