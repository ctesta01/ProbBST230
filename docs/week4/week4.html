<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.269">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Probability (BST 230) Notes - 5&nbsp; Week 4</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../week5/week5.html" rel="next">
<link href="../week3/week3.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

</head>

<body class="nav-sidebar floating">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-title">Week 4</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Probability (BST 230) Notes</a> 
    </div>
      </div>
      <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Methods (BST 232) Notes</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../week1/week1.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Week 1</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../week2/week2.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Week 2</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../week3/week3.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Week 3</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../week4/week4.html" class="sidebar-item-text sidebar-link active"><span class="chapter-title">Week 4</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../week5/week5.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Week 5</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../week6/week6.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Week 6</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../week7/week7.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Week 7</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../week8/week8.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Week 8</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../week9/week9.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Week 9</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../week10/week10.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Week 10</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../week11/week11.html" class="sidebar-item-text sidebar-link"><span class="chapter-title">Week 11</span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#recap-well-defined-vs.-undefined-expectations" id="toc-recap-well-defined-vs.-undefined-expectations" class="nav-link active" data-scroll-target="#recap-well-defined-vs.-undefined-expectations">Recap: Well-Defined vs.&nbsp;Undefined Expectations</a>
  <ul class="collapse">
  <li><a href="#cauchy-distribution-example" id="toc-cauchy-distribution-example" class="nav-link" data-scroll-target="#cauchy-distribution-example">Cauchy Distribution Example</a></li>
  </ul></li>
  <li><a href="#moments" id="toc-moments" class="nav-link" data-scroll-target="#moments">Moments</a></li>
  <li><a href="#moment-generating-functions" id="toc-moment-generating-functions" class="nav-link" data-scroll-target="#moment-generating-functions">Moment Generating Functions</a>
  <ul class="collapse">
  <li><a href="#exponential-example" id="toc-exponential-example" class="nav-link" data-scroll-target="#exponential-example">Exponential Example</a></li>
  <li><a href="#uniqueness-of-moments" id="toc-uniqueness-of-moments" class="nav-link" data-scroll-target="#uniqueness-of-moments">Uniqueness of Moments</a></li>
  </ul></li>
  <li><a href="#differentiating-under-the-integral-sign" id="toc-differentiating-under-the-integral-sign" class="nav-link" data-scroll-target="#differentiating-under-the-integral-sign">Differentiating under the integral sign</a></li>
  <li><a href="#families-of-distributions" id="toc-families-of-distributions" class="nav-link" data-scroll-target="#families-of-distributions">Families of Distributions</a>
  <ul class="collapse">
  <li><a href="#discrete-uniform-distribution" id="toc-discrete-uniform-distribution" class="nav-link" data-scroll-target="#discrete-uniform-distribution">Discrete Uniform Distribution</a></li>
  <li><a href="#hypergeometric-distribution" id="toc-hypergeometric-distribution" class="nav-link" data-scroll-target="#hypergeometric-distribution">Hypergeometric Distribution</a></li>
  <li><a href="#binomial-distribution" id="toc-binomial-distribution" class="nav-link" data-scroll-target="#binomial-distribution">Binomial Distribution</a></li>
  <li><a href="#independence-of-random-variables" id="toc-independence-of-random-variables" class="nav-link" data-scroll-target="#independence-of-random-variables">Independence of Random Variables</a>
  <ul class="collapse">
  <li><a href="#poisson-distribution" id="toc-poisson-distribution" class="nav-link" data-scroll-target="#poisson-distribution">Poisson Distribution</a></li>
  <li><a href="#geometric-distribution" id="toc-geometric-distribution" class="nav-link" data-scroll-target="#geometric-distribution">Geometric Distribution</a></li>
  </ul></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title d-none d-lg-block"><span class="chapter-title">Week 4</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  

</header>

<section id="recap-well-defined-vs.-undefined-expectations" class="level2">
<h2 class="anchored" data-anchor-id="recap-well-defined-vs.-undefined-expectations">Recap: Well-Defined vs.&nbsp;Undefined Expectations</h2>
<p>The positive part of <span class="math inline">\(X\)</span> is <span class="vocab">well-defined</span> if either <span class="math inline">\(E X^+ &lt; \infty\)</span> or <span class="math inline">\(E X^- &lt; \infty\)</span>.</p>
<p>A random variable <span class="math inline">\(X\)</span> has the Zeta(<span class="math inline">\(s\)</span>) distribution for <span class="math inline">\(s &gt; 1\)</span>, if it has pmf</p>
<p><span class="math display">\[f_X(k) = P(X = k) = \frac{1}{\zeta(s)k^s} \mathbb 1 (k \in \mathbb \{ 1, 2, ... \})\]</span></p>
<p>where <span class="math inline">\(\mathbb \zeta (s)\)</span> is the Riemann zeta function. The use of the Riemann zeta function may seem scary, but it’s really just acting as the normalizing constant here so that this is a proper pmf.</p>
<p>Since <span class="math inline">\(EX^- = E(-\min(X,0)) = 0\)</span>, <span class="math inline">\(EX\)</span> is well-defined.</p>
<p>Recall that <span class="math inline">\(\zeta(s) = \sum_{i=1}^\infty \frac{1}{k^s}\)</span>.</p>
<p>However, if <span class="math inline">\(s \leq 2\)</span> then the mean is infinite:</p>
<p><span class="math display">\[EX = \sum_{i=1}^\infty k f_X(k) = \sum_{i=1}^\infty
\frac{1}{\zeta (s) k^{s-1}} = \infty.\]</span></p>
<p>Now suppose that <span class="math inline">\(Y\)</span> is a discrete random variable with pmf</p>
<p><span class="math display">\[f_Y(k) = P(Y=k) = \frac{1}{2ck^2} \mathbb 1 (|k| \in \{ 1, 2, ... \})\]</span></p>
<p>where <span class="math inline">\(c = \zeta(2)\)</span>.</p>
<p>Then <span class="math inline">\(EY^+ = \infty\)</span> and <span class="math inline">\(EY^- = \infty\)</span>. For example,</p>
<p><span class="math display">\[EY^+ = \sum_{k=0}^\infty kP(Y^+ = k) = \sum_{k=1}^\infty \frac{1}{2ck} = \infty.\]</span></p>
<p>So the mean of <span class="math inline">\(Y\)</span> is not well-defined (or, undefined).</p>
<section id="cauchy-distribution-example" class="level3">
<h3 class="anchored" data-anchor-id="cauchy-distribution-example">Cauchy Distribution Example</h3>
<p>A random variable <span class="math inline">\(X\)</span> has the Cauchy(0,1) distribution if it has the pmf</p>
<p><span class="math display">\[f_X(x) = \frac{1}{p} \frac{1}{1+x^2}\]</span></p>
<p>for <span class="math inline">\(x \in \mathbb R\)</span>. If <span class="math inline">\(X \sim \text{Cauchy}(0,1)\)</span> then <span class="math inline">\(EX\)</span> is undefined.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="standalone_figures/cauchy_pdf/cauchy_pdf.svg" class="img-fluid figure-img" style="width:50.0%"></p>
</figure>
</div>
</div>
</div>
<p>The Cauchy distribution has heavy tails, meaning it can take very large values with non-negligible probability.</p>
</section>
</section>
<section id="moments" class="level2">
<h2 class="anchored" data-anchor-id="moments">Moments</h2>
<p>Let <span class="math inline">\(k\)</span> be a positive integer. The <span class="math inline">\(k\)</span>th moment of <span class="math inline">\(X\)</span> is <span class="math inline">\(E(X^k)\)</span>. The <span class="math inline">\(k\)</span>th central moment of <span class="math inline">\(X\)</span> is <span class="math inline">\(E((X-EX)^k)\)</span>.</p>
<p>The variance of a random variable is the 2nd central moment:</p>
<p><span class="math display">\[\text{Var}(X) \stackrel{def}{=} E((X-EX)^2).\]</span></p>
<p><span class="math inline">\(\text{Var}(X)\)</span> is sometimes denoted <span class="math inline">\(\sigma^2(X)\)</span> or simply <span class="math inline">\(\sigma^2\)</span>.</p>
<p>The standard deviation of <span class="math inline">\(X\)</span> is <span class="math inline">\(\sqrt{\text{Var}(X)}\)</span>.</p>
<p>Both the variance <span class="math inline">\(\sigma^2\)</span> and the standard deviation <span class="math inline">\(\sigma\)</span> quantify how spread out a distribution is. However, <span class="math inline">\(\sigma\)</span> is more interpretable since it is in the same units as <span class="math inline">\(X\)</span>.</p>
<div class="hottip">
<p>If the mean is undefined for a distribution, the variance and standard deviaion will also be undefined, as in the Cauchy distribution. How might we quantify the spread of the distribution? One might use quantiles. Median absolute deviation. A really simple approach might be the difference between the 95th and 5th percentiles.</p>
</div>
<section id="properties-of-variance" class="level4">
<h4 class="anchored" data-anchor-id="properties-of-variance">Properties of Variance</h4>
<ol type="1">
<li>If <span class="math inline">\(\text{Var}(X) &lt; \infty\)</span>, then for any <span class="math inline">\(a,b \in \mathbb R\)</span>,</li>
</ol>
<p><span class="math display">\[\text{Var}(aX + b) = a^2 \text{Var}(X).\]</span></p>
<ol start="2" type="1">
<li><p>A useful formula for the variance is <span class="math inline">\(\text{Var}(X) = EX^2 - (EX)^2\)</span>.</p></li>
<li><p>Suppose <span class="math inline">\(Y\)</span> is an estimator of some quantity <span class="math inline">\(y_0\)</span>. Then the mean squared error is <span class="math display">\[mse = E(|Y - y_0|^2) = (EY - y_0)^2 + E((Y-EY)^2).\]</span></p></li>
</ol>
<p><span class="math display">\[ = \text{bias}^2 + \text{variance}\]</span></p>
<div class="cooltip">
<p><strong>Proof of 2.</strong> <span class="math display">\[\text{Var}(X) = E((X - E X)^2)\]</span> <span class="math display">\[ = E(X X - X E X - E X X + (E X)^2)\]</span> <span class="math display">\[ = E X^2 - 2E XE X + (E X)^2\]</span> <span class="math display">\[ = E (X^2) - (E X)^2\]</span></p>
<p><strong>Proof of 1 using 2.</strong> Now apply the 2nd to the first question:</p>
<p><span class="math display">\[\text{Var}(aX+b) = E((aX+b - E(aX+b))^2)\]</span></p>
<p>or <span class="math display">\[\text{Var}(aX+b) = E((aX+b)^2) - E(aX+b)^2 \]</span> <span class="math display">\[ = E(a^2X^2+2abE X + b^2) - (aE X+b)^2 \]</span></p>
<p><span class="math display">\[ =  (a^2E(X^2)+\cancel{2abE X} + \cancel{b^2}) - (a^2(E X)^2+\cancel{2abE X}+\cancel{b^2})\]</span> <span class="math display">\[ = a^2((E X^2)^2 - E(X)^2) \]</span> <span class="math display">\[ = a^2\text{Var}(X)\]</span></p>
<p><strong>Proof of 1 using definitions.</strong> Using the 2nd central moment formula:</p>
<p><span class="math display">\[\text{Var}(aX+b) = E((aX+b - E(aX+b))^2)\]</span> <span class="math display">\[ = E((aX+b - (aE X+b))^2)\]</span> <span class="math display">\[ = E((aX - (aE X)^2)\]</span> <span class="math display">\[ = a^2E((X - (E X)^2)\]</span> <span class="math display">\[ = a^2\text{Var}(X)\]</span></p>
<p><strong>Proof of 3.</strong> Mean squared error is defined as <span class="math display">\[mse = E((Y - y_0)^2)\]</span></p>
<p>A nice trick is to add and subtract by the same thing.</p>
<p><span class="math display">\[ \text{mse} = E((Y - EY + EY - y_0)^2)\]</span> <span class="math display">\[ = E((Y-EY)^2 + 2(Y-EY)(EY-y_0) + (EY - y_0)^2)\]</span> <span class="math display">\[ = E((Y-EY)^2) + 2\underbrace{(EY-EY)}_{=0}(EY-y_0) + (EY - y_0)^2\]</span> <span class="math display">\[ = \underbrace{E((Y-EY)^2)}_{\text{variance}} + \underbrace{(EY - y_0)^2}_{\text{bias}^2}\]</span></p>
</div>
<div class="hottip">
<p>A good illustration of the bias-variance tradeoff is in estimating the sample variance of normally distributed values.</p>
<p>Suppose that <span class="math inline">\(X_1, X_2, ... \sim \mathcal N(\mu, \sigma^2)\)</span>.</p>
<p><span class="math display">\[\bar x = \frac{1}{n} \sum_{i=1}^n X_i\]</span></p>
<p><span class="math display">\[\hat \sigma^2 = \frac{1}{n-1} \sum_{i=1}^n (X_i-\bar X)^2\]</span></p>
<p>The above is unbiased, but it’s not the estimator with lowest mse. One can get a better estimator with lower mean-squared-error by using either <span class="math inline">\(1/n\)</span> or <span class="math inline">\(1/(n+1)\)</span>. For more details on the <span class="math inline">\(1/(n+1)\)</span> correction, look at page 351 in Casella and Berger.</p>
<p>The usual <span class="math inline">\(1/(n-1)\)</span> correction is known as <a href="https://en.wikipedia.org/wiki/Bessel%27s_correction">Bessel’s correction</a>.</p>
<p>Even further, suppose that <span class="math inline">\(X_1, X_2, ... \sim \mathcal N(\mu_i, \sigma^2)\)</span>.</p>
<p>Naively, one would think that the best estimates for <span class="math inline">\(\hat \mu_i\)</span> is just <span class="math inline">\(X_i\)</span>, but the <a href="https://en.wikipedia.org/wiki/James%E2%80%93Stein_estimator">James-Stein estimator/paradox</a> shows that by decreasing the variance we can come up with estimators that have lower mean-squared-error.</p>
</div>
<p>A common misperception is that bias is always bad. In fact, allowing some bias usually improves performance by reducing variance. This is especially important when building a prediction model. Less flexible models tend to have greater bias, since they cannot fit the distributions as closely. More flexible models tend to have greater variance, since they have more parameters to estimate. Since <span class="math inline">\(\text{mse} = \text{bias}^2 + variance\)</span>, there is a trade-off, and mse is minimized by setting the flexibility equal to some critical point.</p>
</section>
</section>
<section id="moment-generating-functions" class="level2">
<h2 class="anchored" data-anchor-id="moment-generating-functions">Moment Generating Functions</h2>
<p>The <span class="vocab">moment generating function</span> (mgf) of a random variable <span class="math inline">\(X\)</span> is</p>
<p><span class="math display">\[M_X(t) = E[e^{tX}]\]</span> for <span class="math inline">\(t \in \mathbb{R}\)</span>.</p>
<p>The mgf is said to exist if <span class="math inline">\(M_X(t)\)</span> is finite in a neighborhood of zero. In other words, if there is some <span class="math inline">\(h &gt; 0\)</span> such that <span class="math inline">\(M_X(t) &lt; \infty\)</span> whenever <span class="math inline">\(|t| &lt; h\)</span>.</p>
<p>This terminology is a little weird since the function always exists but might be infinite.</p>
<p>Why is it called the “moment generating function”?</p>
<p>For all <span class="math inline">\(k \in \{ 1, 2, 3, ... \}\)</span>,</p>
<p><span class="math display">\[EX^k = \frac{d^k}{dt^k} M_X(t) \lvert_{t=0.}\]</span></p>
<p>That is, the <span class="math inline">\(k\)</span>th moment of <span class="math inline">\(X\)</span> equals the <span class="math inline">\(k\)</span>th derivative of <span class="math inline">\(M_X(t)\)</span> evaluated at <span class="math inline">\(t=0\)</span>.</p>
<p>So <span class="math inline">\(M_X(t)\)</span> is a function from which one can “generate” the moments simply by differentiating and evaluating at <span class="math inline">\(t=0\)</span>.</p>
<section id="exponential-example" class="level3">
<h3 class="anchored" data-anchor-id="exponential-example">Exponential Example</h3>
<p>If <span class="math inline">\(X \sim \text{Exponential}(\lambda)\)</span>, then for <span class="math inline">\(|t| &lt; \lambda\)</span>,</p>
<p><span class="math display">\[M_X(t) = E[e^{tx}] = \int_0^\infty \exp(tx) \lambda \exp(-\lambda x) dx\]</span> <span class="math display">\[ = \lambda \int_0^\infty \exp(-(\lambda - t)x) dx\]</span></p>
<p><span class="math display">\[= \frac{\lambda}{\lambda - 1} \int_0^\infty (\lambda - t)\exp(-(\lambda - t)x)dx\]</span></p>
<p>In the last step, we multiplied and divided by <span class="math inline">\(\lambda - t\)</span> so that the inside is an exponential pdf with parameter <span class="math inline">\(\lambda - t)\)</span> (and thus has integral 1).</p>
<p><span class="math display">\[ = \frac{\lambda}{\lambda - 1} &lt; \infty\]</span></p>
<p>for <span class="math inline">\(|t| &lt; \lambda.\)</span></p>
<p>We can easily compute the moments of <span class="math inline">\(X\)</span> using the mgf.</p>
<p>Without using the mgf, we’d have to use integration by parts to solve:</p>
<p><span class="math display">\[EX^k = \int_0^\infty x^k \lambda e^{-\lambda x} dx,\]</span> which could be a bit painful for larger <span class="math inline">\(k\)</span>.</p>
<p>So instead, using the mgf, we get that the 1st and 2nd moments are:</p>
<p><span class="math display">\[E X = \frac{d}{dt} \frac{\lambda}{\lambda - t} \big\lvert_{t=0} = \frac{\lambda}{(\lambda - t)^2} \big\lvert_{t=0} = \frac{1}{\lambda}\]</span></p>
<p><span class="math display">\[EX^2 = \frac{d^2}{dt^2} \frac{\lambda}{\lambda-t} \big\lvert_{t=0} = \frac{d}{dt} \frac{\lambda}{(\lambda - t)^2} \big\lvert_{t=0} = \frac{2\lambda(\lambda- t)}{(\lambda-t)^4} \big\lvert_{t=0} = \frac{2}{\lambda^2}. \]</span></p>
<p>Thus the variance of <span class="math inline">\(X\)</span> is</p>
<p><span class="math display">\[\text{Var}(X) = EX^2 - (EX)^2 = \frac{2}{\lambda^2} - \frac{1}{\lambda^2} = \frac{1}{\lambda^2}.\]</span></p>
<div class="cooltip">
<p>Could it be that the moments still exist even if the mgf does not take on a finite value?</p>
<p>Recall that <span class="math display">\[e^{tX} = \sum_{k=0}^\infty \frac{(tX)^k}{k!} \geq \frac{t^kX^k}{k!} \quad (X \geq 0)\]</span></p>
<p>So it might be that the moment generating function doesn’t exist while the moments themselves do exist.</p>
<p>We’ll get to the characteristic function soon:</p>
<p><span class="math display">\[\phi_X(t) = E(e^{itX}).\]</span></p>
<p>And <span class="math inline">\(|e^{itX}| = 1\)</span>.</p>
</div>
</section>
<section id="uniqueness-of-moments" class="level3">
<h3 class="anchored" data-anchor-id="uniqueness-of-moments">Uniqueness of Moments</h3>
<p><span class="math inline">\(X\)</span> has <span class="vocab">bounded support</span> if <span class="math inline">\(P(|X| &lt; c) = 1\)</span> for some <span class="math inline">\(c \in \mathbb{R}\)</span>.</p>
<p>Suppose <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> have bounded support. Then <span class="math inline">\(X \stackrel{d}{=} Y\)</span> if and only if <span class="math inline">\(EX^k = EY^k\)</span> for all <span class="math inline">\(k \in \{ 1, 2, ... \}\)</span>.</p>
<p>If <span class="math inline">\(M_X(t)\)</span> and <span class="math inline">\(M_Y(t)\)</span> exist and are equal on a neighborhood of zero, then <span class="math inline">\(X \stackrel{d}{=} Y\)</span>.</p>
<div class="hottip">
<p>This does not hold in general for unbounded distributions. There’s such an example in Casella and Berger.</p>
</div>
</section>
</section>
<section id="differentiating-under-the-integral-sign" class="level2">
<h2 class="anchored" data-anchor-id="differentiating-under-the-integral-sign">Differentiating under the integral sign</h2>
<p>Often we want to interchange the order of differentiation and integration.</p>
<p>For example, for mgfs:</p>
<p><span class="math display">\[\frac{d^k}{dt^k} M_X(t) \lvert_{t=0} = \frac{d^k}{dt^k} E\big(\exp(tX)\big) \lvert_{t=0}\]</span> <span class="math display">\[ = E\big(\frac{d^k}{dt^k} \exp(tX)\lvert_{t=0}\big)\]</span> <span class="math display">\[ = E\big( X^k \exp(tX) \lvert_{t=0}\big)\]</span> <span class="math display">\[ = E(X^k).\]</span></p>
<p>This is using the fact that <span class="math inline">\(\frac{d}{dt} e^{tx} = x e^{tx}\)</span>, and hence <span class="math inline">\(\frac{d^k}{dt^x} e^{tx} = x^k e^{tx}\)</span>.</p>
<p>The step where we swap the order of <span class="math inline">\(\frac{d^k}{dt^k}\)</span> and <span class="math inline">\(E\)</span> is called <em>differentiating under the integral sign.</em></p>
<p>However, regularity conditions are needed for this to hold.</p>
<p>Suppose that <span class="math inline">\(f(x,t)\)</span> is differentiable with respect to <span class="math inline">\(t\)</span> for each <span class="math inline">\(x\)</span>, and there exists a function <span class="math inline">\(g(x,t)\)</span> such that</p>
<ol type="1">
<li><p>for all <span class="math inline">\(x\)</span> and all <span class="math inline">\(t'\)</span> in a neighborhood of <span class="math inline">\(t\)</span> <span class="math display">\[\left\lvert \frac{\partial }{\partial t} f(x,t) \lvert_{t=t'} \right\rvert \leq g(x,t)\]</span></p></li>
<li><p><span class="math inline">\(\int_{-\infty}^\infty g(x,t) dx &lt; \infty\)</span>. Then <span class="math display">\[\frac{d}{dt} \int_{-\infty}^\infty f(x,t) dx = \int_{-\infty}^\infty \frac{\partial}{\partial t} f(x,t) dx.\]</span></p></li>
</ol>
<p>See Casella &amp; Berger (Theorem 2.4.3) for a slightly more general version. This proof uses one of the most important results in measure theory: the dominated convergence theorem.</p>
<p>We present a non-measure theoretic version of the <span class="vocab">dominated convergence theorem</span> here, which is not fully general but gets the main idea across.</p>
<p>Suppose that <span class="math inline">\(f(x,t)\)</span> is continuous at <span class="math inline">\(t_0\)</span> for each <span class="math inline">\(x\)</span> and there exists <span class="math inline">\(g(x)\)</span> such that</p>
<ol type="1">
<li><span class="math inline">\(|f(x,t)| \leq g(x)\)</span> for all <span class="math inline">\(x\)</span> and all <span class="math inline">\(t\)</span>, and</li>
<li><span class="math inline">\(\int_{-\infty}^\infty g(x) dx &lt; \infty\)</span>.</li>
</ol>
<p>Then <span class="math display">\[\lim_{t\to t_0} \int_{-\infty}^\infty f(x,t) dx = \int_{-\infty}^{\infty} \lim_{t\to t_0} f(x,t) dx.\]</span></p>
<p>The dominated convergence theorem allows us to justify switching the order of limits and integrals.</p>
<p>We can think about the dominated convergence theorem as describing a situation where we have a sequence of functions:</p>
<p><span class="math display">\[f_1(x),\, f_2(x),\, f_3(x),\, \cdots\]</span></p>
<p>And what we’re saying is <span class="math display">\[\lim_{n \to \infty} \int f_n(x) dx = \int \left( \lim_{n \to \infty} f_n(x))\right) dx.\]</span></p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="standalone_figures/converging_normals/converging_normals.svg" class="img-fluid figure-img" style="width:75.0%"></p>
</figure>
</div>
</div>
</div>
<div class="hottip">
<p>A counter-example would be <span class="math inline">\(f_n(x) = 1/n\)</span>, so <span class="math inline">\(f_*(x) = 0\)</span>, but <span class="math inline">\(\int f_n(x) dx &gt; 0\)</span> for all <span class="math inline">\(n\)</span>.</p>
<p>Another counter-example is <span class="math inline">\(f_n(x) = \mathbb 1(n &lt; x &lt; n+1)\)</span>. The limit <span class="math inline">\(f_*(x) = 0\)</span> because for every <span class="math inline">\(x\)</span>, as <span class="math inline">\(n\to \infty\)</span>, there is an <span class="math inline">\(N \in \mathbb N\)</span> such that for all <span class="math inline">\(N' &gt; N\)</span> <span class="math inline">\(f_{N'}(x) = 0\)</span>.</p>
<p>This is what the requirements around the existence of such a function <span class="math inline">\(g(x)\)</span> are telling us (that <span class="math inline">\(g(x)\)</span> is an envelope for all <span class="math inline">\(f_n(x)\)</span> and <span class="math inline">\(\int_{-\infty}^\infty g(x) dx &lt; \infty\)</span>.</p>
</div>
</section>
<section id="families-of-distributions" class="level1">
<h1>Families of Distributions</h1>
<p>In statistics, families of distributions play a key role. Many statistical methods are based on assuming that the data are distributed according to some family of distributions. Estimation and inference then proceeds by finding the parameters of the family that could plausibly have generated the observed data.</p>
<p>For instance, if one assumes that data <span class="math inline">\(X_1, ..., X_n\)</span> are <span class="math inline">\(\mathcal N(\mu, \sigma^2)\)</span> distributed, then we could use maximum likelihood to estimate <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span> as:</p>
<p><span class="math display">\[\hat \mu = \frac{1}{n} \sigma_{i=1}^n X_i \quad \quad \hat \sigma^2 = \frac{1}{n} \sum_{i=1}^n (X_i - \bar X)^2.\]</span></p>
<p>Many commonly used distributions have special properties that make them well-justified in particular applications.</p>
<p>Examples:</p>
<ul>
<li>Gaussians have the central limit theorem.</li>
<li>Poissons have the law of small numbers.</li>
<li>Exponentials have memorylessness.</li>
<li>Pareto distributions have the power law.</li>
<li>Exponential families have maximum entropy.</li>
<li>The Poisson process is the limit of Bernoulli processes.</li>
</ul>
<p>Selecting, combining, and/or transforming distributions according to the “physics” of the data generating process is important for good statistical modeling.</p>
<p>We often write the name of the distribution itself to denote the pdf/pmf. For instance, <span class="math inline">\(\text{Uniform(x|a,b)\)</span> denotes the pdf of <span class="math inline">\(\text{Uniform(a,b)}\)</span>.</p>
<p>If the distribution <span class="math inline">\(X\)</span> has been defined, say as <span class="math inline">\(X \sim \text{Uniform}(a,b)\)</span>, another shorthand is to write <span class="math inline">\(p(x|a,b)\)</span> for the pdf/pmf.</p>
<p>We will often denote pdfs or pmfs as <span class="math inline">\(p(\cdot)\)</span> instead of <span class="math inline">\(f(\cdot)\)</span>.</p>
<p>When dealing with multiple r.v.s, say <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, it is common to simply write <span class="math inline">\(p(x)\)</span> and <span class="math inline">\(p(y)\)</span> for the pdf/pmf of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, respectively, instead of <span class="math inline">\(p_X(x)\)</span> and <span class="math inline">\(p_Y(y)\)</span>. In other words, the letters used (<span class="math inline">\(x\)</span> or <span class="math inline">\(y\)</span>) indicates which random variable we’re talking about.</p>
<section id="discrete-uniform-distribution" class="level3">
<h3 class="anchored" data-anchor-id="discrete-uniform-distribution">Discrete Uniform Distribution</h3>
<p>The <span class="math inline">\(\text{Uniform}(N)\)</span> distribution has pmf</p>
<p><span class="math display">\[p(x|N) = \frac{1}{N} \mathbb 1(x \in \{ 1, 2, ..., N \})\]</span></p>
<p><span class="math display">\[EX = (N+1)/2\]</span> <span class="math display">\[\text{Var}(X) = (N+1)(N+2)/12\]</span></p>
<p>More generally, for any set <span class="math inline">\(\mathcal X\)</span>, the uniform distribution on <span class="math inline">\(\mathcal X\)</span> is denoted <span class="math inline">\(\text{Uniform}(\mathcal X)\)</span>.</p>
<p><span class="math inline">\(\text{Uniform}(\mathcal X)\)</span> represents maximal uncertainty in the outcome of a quantity <span class="math inline">\(x \in \mathcal X\)</span>. It also maximizes the <span class="vocab">entropy</span>,</p>
<p><span class="math display">\[H = - \sum_{x \in \mathcal X} p(x) \log p(x),\]</span></p>
<p>over all distributions of <span class="math inline">\(\mathcal X\)</span>, when <span class="math inline">\(\mathcal X\)</span> is a finite set.</p>
<p>One characteristic of entropy is that it can be defined for <span class="math inline">\(\mathcal X\)</span> that is not a subset of <span class="math inline">\(\mathbb R\)</span>.</p>
<p>Boltzmann was the first to study entropy, and it was later studied by Claude Shannon in information theory (applied to communication across lossy channels and [en-]coding theory).</p>
</section>
<section id="hypergeometric-distribution" class="level3">
<h3 class="anchored" data-anchor-id="hypergeometric-distribution">Hypergeometric Distribution</h3>
<p>The <span class="math inline">\(\text{Hypergeometric}(N,M,K)\)</span> distribution has pmf</p>
<p><span class="math display">\[p(x|N,M,K) = \frac{{M \choose x}{N-M \choose K-x}}{N \choose K} \mathbb 1 (x \in \mathcal X),\]</span></p>
<p>where <span class="math inline">\(\mathcal X = \{ x \in \{ 0, 1, ..., K\} : K - N + M \leq x \leq M\}.\)</span></p>
<p>The parameters are <span class="math inline">\(N, M, K \in \{ 0, 1, 2, ... \}\)</span>.</p>
<p>The mean and variance of <span class="math inline">\(X \sim \text{Hypergeometric}(N,M,K)\)</span> are <span class="math display">\[EX = KM/N\]</span> <span class="math display">\[\text{Var}(X) = \frac{KM}{N} \frac{(N-M)(N-K)}{N(N-1)}.\]</span></p>
<p>Another example, besides drawing balls for a lottery, is the scenario of testing for defects in manufacturing. If a batch of <span class="math inline">\(N\)</span> items has <span class="math inline">\(M\)</span> defects and <span class="math inline">\(K\)</span> randomly selected items are tested, what is the probability that <span class="math inline">\(x\)</span> of the tested items are defective?</p>
<p>Approaches outlined: * Assume that <span class="math inline">\(\frac{x}{K} \approx \frac{M}{N} \to x \approx \frac{MK}{N} = \mathbb{E}X\)</span> and choose <span class="math inline">\(\hat M \approx \frac{xN}{K}\)</span>. This is called the <span class="vocab">method of moments</span>. * Look at the probability mass function <span class="math inline">\(p(x|N,M,K) = f(M)\)</span> and maximize with respect to <span class="math inline">\(0 \leq M \leq N\)</span>. Basically, let <span class="math inline">\(\hat M = \text{argmax}_{M} p(x|N,M,K)\)</span>. This is the <span class="vocab">maximum likelihood estimator (MLE)</span>. * <span class="vocab">Simulation methods</span> where if, say we didn’t know the analytic formula for the mean, we could empirically estimate the mean, and search for the distributions with simulated empirical mean close to <span class="math inline">\(x\)</span>. This comes up when models are analytically intractable: for example, a weather model. * The Bayesian approach would be to consider the probably that <span class="math inline">\(M\)</span> as a random variable is a particular <span class="math inline">\(m\)</span> given the parameters (the posterior): <span class="math display">\[p(M=m|N,K,X=x) = \frac{p(X=x | N,K,M=m) p(M=m)}{\sum_{m'=0}^\infty p(X=x|N,K,M=m')p(M=m')\]</span> If we’ve been doing testing for a long time, we may have a good prior for <span class="math inline">\(p(M=m')\)</span>. Two important quantities are the posterior mean: <span class="math display">\[\hat M = \mathbb{E}(M|N,K,X=x)\]</span> And the maximum a posteriori estimate, <span class="math display">\[\hat M = \text{argmax}_m p(M=m|N,K,X=x)\]</span></p>
<p>Probability is the forward process of going from the parameters to the outcome. Inference (statistics) is the inductive process of going from data to knowledge about the parameters (and generalized knowledge).</p>
</section>
<section id="binomial-distribution" class="level3">
<h3 class="anchored" data-anchor-id="binomial-distribution">Binomial Distribution</h3>
<p>The <span class="math inline">\(\text{Binomial}(N,q)\)</span> distribution has pmf</p>
<p><span class="math display">\[p(x|N,q) = {N \choose x} q^x(1-q)^{N-x} \mathbb 1(x \in \mathcal X)\]</span></p>
<p>where <span class="math inline">\(\mathcal X = \{ 0, 1, ..., N \}\)</span>. The parameters are <span class="math inline">\(N \in \{ 1, 2, ... \}\)</span> and <span class="math inline">\(q \in (0,1)\)</span>.</p>
<p>The mean and variance are</p>
<p><span class="math display">\[\mathbb{E}X = Nq\]</span></p>
<p><span class="math display">\[\text{Var}(X) = Nq(1-q)\]</span></p>
</section>
<section id="independence-of-random-variables" class="level2">
<h2 class="anchored" data-anchor-id="independence-of-random-variables">Independence of Random Variables</h2>
<p>Random variables <span class="math inline">\(X_1, ..., X_n\)</span> are independent if</p>
<p><span class="math display">\[P(X_1 \in A_1, ..., X_n \in A_n) = P(X_1 \in A_1) \cdots P(X_n \in A_n)\]</span> for all measurable subsets <span class="math inline">\(A_1, ..., A_n \subset \mathbb{R}\)</span>.</p>
<p>The <span class="math inline">\(\text{Bernoulli}(q)\)</span> distribution is the special case of <span class="math inline">\(\text{Binomial}(N,q)\)</span> when <span class="math inline">\(N=1\)</span>.</p>
<p>If <span class="math inline">\(X_1, ..., X_n \sim \text{Bernoulli}(q)\)</span> are independent, then</p>
<p><span class="math inline">\(\sum_{i=1}^n X_i \sim \text{Binomial}(N,q).\)</span></p>
<p>From this, it is easy to derive the mean of the Binomial distribution:</p>
<p><span class="math display">\[\mathbb{E}\left( \sum_{i=1}^n X_i \right) = \sum_{i=1}^n \mathbb{E}X_i = \sum_{i=1}^n q = nq.\]</span></p>
<section id="poisson-distribution" class="level3">
<h3 class="anchored" data-anchor-id="poisson-distribution">Poisson Distribution</h3>
<p>The <span class="math inline">\(\text{Poisson}(\lambda)\)</span> distribution has pmf</p>
<p><span class="math display">\[p(x|\lambda) = e^{-\lambda} \frac{\lambda^x}{x!} \mathbb 1(x \in \mathcal X)\]</span></p>
<p>where <span class="math inline">\(\mathcal X = \{ 0, 1, 2,...\}\)</span>. The parameter <span class="math inline">\(\lambda &gt; 0\)</span> is referred to as the <span class="vocab">rate</span>, for reasons that will become clear when we study Poisson processes.</p>
<p>The mean and variance of <span class="math inline">\(X \sim \text{Poisson}(\lambda)\)</span>.</p>
<p>The Poisson model is often a good model for counting the occurrences of independent rare events.</p>
<p>Examples:</p>
<ul>
<li>In genomics, the number of reads covering a given locus is well-modeled as Poisson.</li>
<li>In physics, the number of photons hitting a detector during a given period of time is Poisson distributed.</li>
<li>In ecology, the number of organisms in a given region is often well-modeled as Poisson.</li>
</ul>
<p>This is all due to a special property of the Poisson distribution jokingly referred to as the “law of small numbers”.</p>
<p>The Poisson is a limit of Binomials: if <span class="math inline">\(q_N \in (0,1)\)</span> is such that <span class="math display">\[N q_N \to \lambda\]</span> as <span class="math inline">\(N \to \infty\)</span> for some <span class="math inline">\(\lambda &gt; 0\)</span>, then for all <span class="math inline">\(x \in \{ 0, 1, 2, ...\}\)</span>,</p>
<p><span class="math display">\[\text{Binomial}(x|N,q_N) \longrightarrow \text{Poisson}(x|\lambda).\]</span></p>
</section>
<section id="geometric-distribution" class="level3">
<h3 class="anchored" data-anchor-id="geometric-distribution">Geometric Distribution</h3>
<p>The <span class="math inline">\(\text{Geometric}(q)\)</span> distribution has pmf</p>
<p><span class="math display">\[p(x|q) = (1-q)^{x-1}q \mathbb 1(x \in \mathcal X).\]</span></p>
<p><span class="math display">\[\mathbb{E}X = 1/q\]</span></p>
<p><span class="math display">\[\text{Var}(X) = \frac{1-q}{q^2}.\]</span></p>


</section>
</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../week3/week3.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-title">Week 3</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../week5/week5.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-title">Week 5</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->



</body></html>