[
  {
    "objectID": "week1/week1.html#il-famoso-smoking-ra-fisher",
    "href": "week1/week1.html#il-famoso-smoking-ra-fisher",
    "title": "2  Week 1",
    "section": "2.1 il famoso Smoking RA Fisher",
    "text": "2.1 il famoso Smoking RA Fisher\nWe’ll talk about a lot of the methods that Ronald A. Fisher developed. Already in the 1900s it was being observed that there was a strong association between smoking and lung cancer. However, Fisher was a smoker himself and posited that the association between lung cancer and smoking could be explained away by some genetic or biological difference between the smoking and non-smoking population (positing some genes that caused people to desire to smoke).\n\n\n\n\n\nRonald Fisher’s unsupported theory of genetics confounding the smoking-lung cancer relationship\n\n\n\n\nWe’re pretty sure that this was driven not by any substance matter expertise, but rather by Fisher’s love of smoking."
  },
  {
    "objectID": "week1/week1.html#hormone-replacement-therapy",
    "href": "week1/week1.html#hormone-replacement-therapy",
    "title": "2  Week 1",
    "section": "2.2 Hormone Replacement Therapy",
    "text": "2.2 Hormone Replacement Therapy\nIn the mid- to late- 20th century there were a ton of studies linking hormone replacement therapy for older women to better cardiovascular outcomes (lack of coronary heart disease).\nHowever, thankfully due to the Heart and Estrogen/Progestin Study (HERS, in the early 90s) we now know that a lot of those studies were not controlling for socioeconomic status. It turns out that socioeconomic status was highly associated with HRT usage, and associated at least in the US with a lot of better health outcomes across the board.\nIt turned out that HRT when applied at certain times for some people can actually be harmful — but the point is the picture is much muddier than was initially thought and recommendations were rolled back. Later randomized studies were performed that produced reliable bodies of evidence demonstrating either no effect or in some cases harmful effects.\nWe’ll use the baseline data from HERS (not so much interested in the HRT treatment effect), but to investigate the research question:\n\nHow is systolic blood pressure related to age, independently of other well-known cardiovascular risk factors? (Age, diabetes, smoking, etc.)"
  },
  {
    "objectID": "week1/week1.html#prediction-studies",
    "href": "week1/week1.html#prediction-studies",
    "title": "2  Week 1",
    "section": "2.3 Prediction Studies",
    "text": "2.3 Prediction Studies\nTypically in prediction settings, there’s no single exposure of particular interest; mechanisms and confounding is treated as less of a concern (if at all), and the main challenge is that we need to take care to not overfit the data.\nA major theme of this class will be that different tasks require different analysis strategies and diffrent statistical tools."
  },
  {
    "objectID": "week1/week1.html#quantifying-uncertainty",
    "href": "week1/week1.html#quantifying-uncertainty",
    "title": "2  Week 1",
    "section": "2.4 Quantifying Uncertainty",
    "text": "2.4 Quantifying Uncertainty\nTypically standard statistical models have nice theoretical properties because years-and-years ago, we didn’t have much data so people spent their time studying theory instead of data. As a result, we have a lot of nice theories about the uncertainty represented in statistical models.\nAn example of the kind of uncertainty we might be interested in is shown in this figure relating Alzheimer’s disease rates and exposure to PM2.5.\n\n\n\n\n\n\n\n\n\nThis figure is taken from the article Long-term effects of PM2·5 on neurological disorders in the American Medicare population: a longitudinal cohort study by Shi et al, Lancet Planetary Health (2020)."
  },
  {
    "objectID": "week1/week1.html#why-learn-methods-before-study-design",
    "href": "week1/week1.html#why-learn-methods-before-study-design",
    "title": "2  Week 1",
    "section": "2.5 Why Learn Methods Before Study Design",
    "text": "2.5 Why Learn Methods Before Study Design\nAn interesting point made is that it’s important to understand the limitations, strengths of methods, what they can and can’t do, and how to use them before designing a study."
  },
  {
    "objectID": "week1/week1.html#recommended-reading",
    "href": "week1/week1.html#recommended-reading",
    "title": "2  Week 1",
    "section": "2.6 Recommended Reading",
    "text": "2.6 Recommended Reading\nKutner M, Nachtsheim C, Neter J, Li W. Applied Linear Statistical Model. 5th edition. chapters 1-3\nShmueli, G. (2010). To explain or to predict? Statistical Science. https://www.stat.berkeley.edu/~aldous/157/Papers/shmueli.pdf"
  },
  {
    "objectID": "week1/week1.html",
    "href": "week1/week1.html",
    "title": "Week 1",
    "section": "",
    "text": "Introductory Overview\nJeffrey Miller’s office hours: Noon-1pm on Thursdays\nTA: Cathy Xue Office hours: 5:30-6:30pm on Tuesdays in 2-434 (Building 2, Room 434)"
  },
  {
    "objectID": "week1/week1.html#course-description",
    "href": "week1/week1.html#course-description",
    "title": "Week 1",
    "section": "Course description:",
    "text": "Course description:\n\nAxiomatic foundations of probability, independence, conditional probability, joint distributions, transformations, moment generating functions, characteristic functions, moment inequalities, sampling distributions, modes of convergence and their interrelationships, laws of large numbers, central limit theorem, and stochastic processes"
  },
  {
    "objectID": "week1/week1.html#course-readings",
    "href": "week1/week1.html#course-readings",
    "title": "Week 1",
    "section": "Course Readings:",
    "text": "Course Readings:\n\nStatistical Inference (Second Edition), by George Casella and Roger L. Berger. Cengage Learning, 2021.\nProbability: Theory and Examples (Fourth Edition), by Richard Durrett. Cambridge University Press, 2010. (https://services.math.duke.edu/~rtd/PTE/PTE5_011119.pdf)\nIntroduction to Stochastic Processes (Second Edition), by Gregory F. Lawler. Chapman & Hall/CRC, 2006.\n\nThe Durrett book is more measure-theoretic, but covers some things better (according to Miller) than Casella and Berger. Lawler’s book is a gentle introduction to stochastic processes."
  },
  {
    "objectID": "week1/week1.html#labs",
    "href": "week1/week1.html#labs",
    "title": "Week 1",
    "section": "Labs",
    "text": "Labs\nWeekly Tuesdays at 3:45-5:15 in FXB G10"
  },
  {
    "objectID": "week1/week1.html#outline-of-topics",
    "href": "week1/week1.html#outline-of-topics",
    "title": "Week 1",
    "section": "Outline of Topics",
    "text": "Outline of Topics\n\nFundamentals (CB 1.1 - 1.2.2)\n\nSet theory basics, Measure theory basics, Properties of probability measures\n\nProbability basics (CB 1.2.3 - 1.6)\n\nCombinatorics, Conditional probability and Independence, Random variables\n\nTransformations of random variables (CB 2.1)\n\nChange of variable formula for r.v.s, Probability integral transform\n\nExpectations of random variables CB 2.2 - 2.4)\n\nMean and variance, Moments, MGFs, Differentiation and limits of integrals\n\nFamilies of distributions (CB 3.1 - 3.5)\n\nDiscrete and continuous families, exponential families, location-scale families\n\nInequalities (CB 3.6, 3.8, 4.7)\n\nMarkov, Chebyshev, Gauss, Hölder, Cauchy-Schwarz, Minkowski, Jensen\n\nMultiple random variables (CB 4.1 - 4.6)\n\nRandom vectors, conditional distributions, independence, mixtures, covariance and correlation\n\nGaussian distributions (Bishop pp. 78-93, in Files/Reading on Canvas site)\n\nMultivariate normal, marginals and conditionals, linear-Gaussian model\n\nStatistics of a random sample (CB 5.1 - 5.4)\n\nSampling distributions, Sums of random variables, Student’s t and Snedecor’s F distribution, Order statistics and friends\n\nAsymptotics (CB 5.5)\n\nModes of convergence, Limit theorems, Delta method, Borel-Cantelli lemma\n\nLaws of large numbers (CB 5.5, D 2.2 - 2.4)\n\nWeak laws of large numbers, Strong laws of large numbers, Generalizations\n\nCentral limit theorems (CB 5.5, D 3.1 - 3.4)\n\nWeak convergence, characteristic functions, central limit theorems\n\nGenerating random samples (CB 5.6)\n\nInverse cdf method, accept/reject method, Markov chain Monte Carlo\n\nStochastics processes (L 1 - 3)\n\nMarkov chains, Random walks, Branching processes, Poisson processes"
  },
  {
    "objectID": "week1/week1.html#introduction",
    "href": "week1/week1.html#introduction",
    "title": "Week 1",
    "section": "Introduction",
    "text": "Introduction\nHow could we tell if either of the two sequences were faked.\n\nstr1 <- \"1 0 0 0 0 1 0 0 1 1 0 0 1 1 0 0 0 1 1 1 1 1 0 0 0 1 1 1 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 0 0 0 1 1 0 1 0 0 0 1 1 0 1 1 0 0 0 0 0 1 1 0 1 0 0 1 0 1 1 0 0 0 1 0 1 1 0 1 0 0 1 0 1 0 1 1 0 0 1 0 1\"\n\nstr2 <- \"1 1 0 0 1 0 0 0 1 1 1 0 1 0 1 0 0 0 1 0 1 1 1 0 1 0 0 0 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 0 0 0 1 0 1 1 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0 1 1 0 1 0 0 1 0 1 0 0 1 1 1 0 1 1 1 0 0 1 1 1 0 1 0 1 0 1 0 1 0\"\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nstr_count(str1, \"0|1\")\n\n[1] 100\n\nstr_count(str1, \"1\")\n\n[1] 43\n\nstr_count(str1, \"0\")\n\n[1] 57\n\nstr_count(str2, \"0|1\")\n\n[1] 100\n\nstr_count(str2, \"1\")\n\n[1] 53\n\nstr_count(str2, \"0\")\n\n[1] 47\n\nstr1_num <- as.numeric(unlist(stringr::str_split(str1, \" \")))\nstr2_num <- as.numeric(unlist(stringr::str_split(str2, \" \")))\n\n# the first way I proposed was to look at the probability of \n# the coin being \"fair\" given the beta distribution parameterized \n# by the observed coinflips \nx <- seq(0,1,0.01)\ncurve(dbeta(x, str_count(str1, \"0\")+1, str_count(str1, \"1\")+1))\n\n\n\ncurve(dbeta(x, str_count(str2, \"0\")+1, str_count(str2, \"1\")+1))\n\n\n\n# then we tried looking at the running mean\nplot(1:100, cummean(str1_num), type='l')\n\n\n\nplot(1:100, cummean(str2_num), type='l')\n\n\n\n# another classmate suggested that the human-generated \n# sequence may have more anti-correlation than the\n# real sequence because more anti-correlation \"looks\" more\n# random \ncor(lag(str1_num), str1_num, use = 'pairwise.complete.obs')\n\n[1] 0.007518797\n\ncor(lag(str2_num), str2_num, use = 'pairwise.complete.obs')\n\n[1] -0.2367884\n\n\nMiller suggests we could also look at it as a sequence of random variables.\n\nstr1_as_geometric_series <- sapply(unlist(stringr::str_split(stringr::str_remove_all(str1, \" \"), \"1\")), nchar)\nunname(str1_as_geometric_series)\n\n [1] 0 4 2 0 2 0 3 0 0 0 0 3 0 0 0 3 3 8 0 0 3 0 1 3 0 1 0 5 0 1 2 1 0 3 1 0 1 2\n[39] 1 1 0 2 1 0\n\nstr2_as_geometric_series <- sapply(unlist(stringr::str_split(stringr::str_remove_all(str2, \" \"), \"1\")), nchar)\nunname(str2_as_geometric_series)\n\n [1] 0 0 2 3 0 0 1 1 3 1 0 0 1 3 0 0 0 3 0 1 1 0 0 3 1 0 1 1 2 0 0 0 1 1 1 2 0 1\n[39] 2 1 2 0 0 1 0 0 2 0 0 1 1 1 1 1\n\ncurve(dgeom(x, prob = .5), from = 0, to = 10, n = 11)"
  },
  {
    "objectID": "week1/week1.html#history-of-probability",
    "href": "week1/week1.html#history-of-probability",
    "title": "Week 1",
    "section": "History of Probability",
    "text": "History of Probability\nGames of chance have been played for millenia. Early dice games were played with “astragali”, or “knucklebones”, from the ankle of a sheep or goat.\n\n\n\n\n\n\n\n\n\nEgyptian tomb paintings from 3500 BC show games played with astragali, and ancient Greek vases show young men tossing the bones into a circle. Gambling in these games was common, so it would have been advantageous to have some understanding of probability.\nInterest in gambling led mathematicians in the 1500-1600s to begin to formalize the rules of probability.\nTwo players put equal money in a pot. The first player to win 8 rounds of a game gets all the money. If they have to stop before finishing, how should the money be divided between them based on how much they would have won, on average?\nAround 1654, Blaise Pascal and Pierre de Fermat developed the concept of expected value to solve this problem.\nChristiaan Huuygens built upon this in his 1657 textbook on probability, “De Ratiociniis in Ludo Aleae” (“The Value of all Chances in Games of Fortune,”)\nIn the early 1700s, Jacob Bernoulli and Abraham De Moivre wrote foundational books on probability.\nThey systematically developed the mathematics of probability, focusing primarily on discrete problems.\nCombinatorial approaches were developed to handle difficult probability calculations.\nBernoulli proved the first version of the law of large numbers.\nIn 1812, Pierre-Simon Laplace published his book “Théorie analytique des probabilités”.\nLaplace developed or advanced many key methods and results in modern probability and statistics.\nGenerating functions, characteristic functions, linear regression, density functions, Bayesian inference, and hypothesis testing.\nHe employed advanced calculus and real/complex analysis,\ntaking probability calculations to a whole new level.\nLaplace proved the first general version of the central limit theorem.\nIn the 1930s, Andrey Kolmogorov introduced the measure theoretic foundations of modern probability.\nMeasure theory had recently been developed to resolve certain paradoxes that arose in defining volume and integration.\nKolmogorov applied measure theory to put probability on solid theoretical footing.\nThis is particularly important for limits and derivatives of integrals, conditional distributions, and stochastic processes."
  },
  {
    "objectID": "week1/week1.html#set-theory-basics",
    "href": "week1/week1.html#set-theory-basics",
    "title": "Week 1",
    "section": "Set Theory Basics",
    "text": "Set Theory Basics\nThe sample space denoted \\(S\\) is the set of possible outcomes of an experiment.\nExamples include \\(S = \\{ H, T \\}\\) for a coin toss, or math SAT scores \\(S = \\{ 200, 201, ..., 799, 800 \\}\\), or time-to-events: \\(S = (0, \\infty)\\).\nWe say that an event \\(E\\) is a subset of \\(S\\) that is \\(E \\subset S\\).\nA set \\(A\\) is a collection of elements. We say that \\[A \\cup B = \\{ x \\colon x \\in A \\text{ or } x \\in B \\}.\\]\n\\[A \\cap B = \\{ x \\colon x \\in A \\text{ and } x \\in B \\}.\\]\n\\[A^c = \\{ x \\in S \\colon x \\not \\in A \\}\\]\n\\[A \\backslash B = \\{ x \\colon x \\in A \\text{ and } x \\not \\in B \\}.\\]\nThe empty set is denoted \\(\\varnothing = \\{\\}\\).\n\\(A \\subset B\\) means that if \\(x \\in A\\) then \\(x \\in B\\).\n\nProperties of Set Operations\nCommutativity:\n\\[A \\cup B = B \\cup A, \\quad A \\cap B = B \\cap A\\]\nAssociativity:\n\\[\nA \\cup (B \\cup C) = (A \\cup B) \\cup C \\quad\nA \\cap (B \\cap C) = (A \\cap B) \\cap C\n\\]\nDistributive Laws:\n\\[ A \\cup (B \\cup C) = (A \\cap B) \\cup (A \\cap C)\n\\quad\nA \\cup (B \\cap C) = (A \\cup B) \\cap (A \\cup C)\\]\nDeMorgan’s Laws:\n\\[ (A \\cup B)^c = A^c \\cap B^c \\quad\n(A \\cap B)^c = A^c \\cup B^c \\]\n\n\nSigma Algebras\nDefinition. Suppose that \\(\\mathcal B\\) is a set of subsets of a sample space \\(S\\). Then \\(\\mathcal B\\) is a sigma-algebra if:\n\n\\(\\varnothing \\in \\mathcal B\\).\nif \\(A \\in \\mathcal B\\) then \\(A^c \\in \\mathcal B\\).\nif \\(A_1, A_2, ... \\in \\mathcal B\\) then \\(\\bigcup_{i=1}^\\infty A_i \\in \\mathcal B\\)\n\n\nThe powerset, denoted \\(2^S\\) is a specific example of a sigma algebra.\n\n\n\nWe have to be very careful that \\(\\varnothing\\) must be an element of \\(\\mathcal B\\) in order for \\(\\mathcal B\\) to be a sigma algebra. \\(\\varnothing\\) is certainly a subset of any set, but \\(\\varnothing\\) needs to be an element of \\(\\mathcal B\\) as a collection of sets.\n\nThe smallest sigma algebra, called the trivial sigma algebra, is \\(\\{ \\varnothing, S \\}\\) for a sample space \\(S\\). When \\(S\\) is uncountable, we usually don’t use the power set as a sigma algebra. Instead, we typically opt for using the Borel sigma algebra.\nFor a topological space \\(S\\), the Borel sigma algebra, denoted \\(\\mathcal B(S)\\) is the smallest sigma algebra containing all open sets.\nDefinition. Let \\(X\\) be a set and \\(\\tau\\) be a family of subsets of \\(X\\). Then \\(\\tau\\) is a topology and \\((X, \\tau)\\) is a topological space if\n\nBoth the empty set and \\(X\\) are elements in \\(\\tau\\).\nAny union of elements of \\(\\tau\\) is an element of \\(\\tau\\).\nAny intersection of finitely many elements of \\(\\tau\\) is an element of \\(\\tau\\).\n\nThe members of \\(\\tau\\) are called open sets in \\(X\\).\nWhen \\(A \\in \\mathcal B\\), we say that \\(A\\) is a measurable set.\n\n\nProbability Measures\nDefinition. If \\((S, \\mathcal B)\\) is a measurable space, then \\(P: \\mathcal B \\to \\mathbb R\\) is a probability measure if:\n\n\\(P(A) \\geq 0\\) for all \\(A \\in \\mathcal B\\). (non-negativity)\n\\(P(S) = 1\\) (unitarity)\nif \\(A_1, A_2, ... \\in \\mathcal B\\) are pairwise disjoint, then \\[P\\left(\\bigcup_{i=1}^\\infty A_i \\right) =\n  \\sum_{i=1}^\\infty P(A_i).\\] (countable additivity)\n\nThese properties are called the axioms of probability, or sometimes Kolmogorov’s axioms.\nIf \\(A \\in \\mathcal B\\) we call \\(A\\) a measurable set.\nIn this course, we may assume that the sets we are working with are measurable. Almost exclusively we will be working with the Borel sigma algebra. While non-measurable sets do exist in this setting, they do not often arise in practice.\n\nTJ asks: “I know it’s possible to demonstrate non-measureable sets non-constructively, but is it possible to demonstrate them constructively?”\nMiller: “I think you have to use infinite series/sets [and the axiom of choice].”"
  },
  {
    "objectID": "week1/week1.html#probability-measure-on-a-countable-set",
    "href": "week1/week1.html#probability-measure-on-a-countable-set",
    "title": "Week 1",
    "section": "Probability measure on a countable set",
    "text": "Probability measure on a countable set\nSuppose that \\(S = \\{ s_1, s_2, ... \\}\\) is a countable set.\nLet \\(p_1, p_2, ... \\geq 0\\) such that \\(\\sum_{i=1}^\\infty p_i = 1\\).\nFor \\(A \\subset S\\), define \\[P(A) = \\sum_{i=1}^\\infty p_i \\mathbb 1(i \\in A).\\]\nWe will write that \\(\\mathbb 1(\\cdot)\\) to denote the indicator function, where\n\\[\\mathbb 1(C) = \\left\\{ \\begin{array}{ll}\n1 & \\text{ if condition } C \\text{ is true} \\\\\n0 & \\text{ if condition } C \\text{ is false}\n\\end{array}\n\\right.\\]\nSuppose you toss a fair coin until you get heads. Then \\(p_k\\), the probability that you toss the coin \\(k\\) times, is \\((1/2)^k\\). This defines a probability measure on \\(S = \\{ 1, 2, ... \\}\\).\nWe could imagine measuring how long a lightbulb lasts until it dies after being left on. It could die at any non-negative time \\(t \\geq 0\\). The probability \\(P([0,t))\\) be the probability the lightbulb dies before time \\(t\\). This defines a probability measure on \\(S = [0,\\infty).\\) (Of course, \\([0,\\infty)\\) is a continuous example and not a countable set).\nFor any probability measure, we have that:\n\n\\(P(A^c) = 1 - P(A)\\)\n\\(P(\\varnothing) = 0\\)\n\\(P(A) \\leq 1\\)\nif \\(A \\subset B\\) then \\(P(A) \\leq P(B)\\)\n\\(P(A) = P(A \\cap B) + P(A \\cap B^c)\\)\n\\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\).\n\nProofs:\n\n\\(P(S) = 1\\), and \\(A, A^c\\) are pairwise disjoint and a partition of \\(S\\), so \\(1 = P(S) = P(A^c \\cup A) = P(A) + P(A^c)\\). Subtracting from both sides, \\(P(A) = 1 - P(A^c)\\).\n\\(\\varnothing\\) and \\(S\\) are disjoint, so \\(1 = P(S) = P(S) + P(\\varnothing)\\). Subtracting from both sides, we have that \\(1 - 1 = P(\\varnothing)\\)\nWe have from 1 that \\(1 - P(A^c) = P(A)\\), and \\(P(A^c) \\geq 0\\), so then \\(P(A) \\leq 1\\)\nWe can write that \\(B \\backslash A\\) and \\(A\\) as disjoint sets since \\(A \\subset B\\). Then \\(P(B) = P(A \\cup B \\backslash A) = P(A) + P(B \\backslash A)\\). Since \\(P(B \\backslash A)\\) we have that \\(P(B) \\geq P(A)\\).\nWe need to show that \\(A = (A \\cap B) \\cup (A \\cup B^c)\\). If \\(a \\in A\\) then either \\(a \\in B\\) or \\(a \\in B^c\\), but not both by the definition of complement. Therefore \\(A \\cap B\\) and \\(A \\cap B^c\\) are disjoint and their union is equal to \\(A\\). Hence \\(P(A) = P(A \\cap B \\bigcup A \\cap B^c) = P(A \\cap B) + P(A \\cap B^c)\\).\nFirst, note that \\(A \\cup B = A \\cap (B \\backslash A)\\). Thus \\(P(A \\cup B) = P(A \\cup (B \\backslash A))\\). Since the latter are disjoint, we establish that \\(P(A \\cup B) = P(A) + P(B \\backslash A)\\). Now if we consider that \\(B = (B \\backslash A) \\cup (A \\cap B)\\), and that these are disjoint sets, we have that \\(P(B) = P(B \\backslash A) + P(A \\cap B)\\). Rearranging, we have that \\(P(B \\backslash A) = P(B) - P(A \\cap B)\\). Substituting, now we have that \\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\) as desired."
  },
  {
    "objectID": "week1/week1.html#properties-of-probability-measures",
    "href": "week1/week1.html#properties-of-probability-measures",
    "title": "Week 1",
    "section": "Properties of Probability Measures",
    "text": "Properties of Probability Measures\nLaw of total probability: for any partition \\(B_1, B_2, ...\\) of \\(S\\), we have that \\[P(A) = \\sum P(A \\cap B_i)\\]\nBoole’s inequality (aka union bound): For \\(A_1, A_2, ...\\),\n\\[P(\\bigcup_{i=1}^\\infty A_i) \\leq \\sum_{i=1}^\\infty P(A_i).\\]\nBonferroni’s inequality: For any \\(A_1, A_2,...\\),\n\\[P\\left(\\bigcap_{i=1}^\\infty A_i \\right) \\geq 1 - \\sum_{i=1}^\\infty P(A_i^c).\\]\nBoole’s inequality is often useful when we want to show that some event has probability near zero. For example, \\(P(E) = P(A_1 \\cup A_2 \\cup A_3) \\leq P(A_1) + P(A_2) + P(A_3) \\leq 3\\epsilon\\)."
  },
  {
    "objectID": "week1/week1.html#selecting-k-items-from-n-options",
    "href": "week1/week1.html#selecting-k-items-from-n-options",
    "title": "Week 1",
    "section": "Selecting \\(k\\) items from \\(n\\) options",
    "text": "Selecting \\(k\\) items from \\(n\\) options\n\n\n\n\nwithout replacement\nwith replacement\n\n\n\n\nordered\n\\(\\frac{n!}{(n-k)!}\\)\n\\(n^k\\)\n\n\nunordered\n\\({n \\choose k}\\)\n\\({ n + k - 1 \\choose k }\\)"
  },
  {
    "objectID": "week1/week1.html#determining-the-leading-factor-in-stirlings-formula",
    "href": "week1/week1.html#determining-the-leading-factor-in-stirlings-formula",
    "title": "Week 1",
    "section": "Determining the Leading Factor in Stirling’s Formula",
    "text": "Determining the Leading Factor in Stirling’s Formula\nStirling’s formula is that for large values of \\(n\\), the following is a good approximation for the factorial function:\n\\[ n! \\approx \\frac{n^n}{e^n} \\sqrt{2 \\pi n}. \\]\nI’ll concern myself with, as an exercise, showing the leading factor \\(n^n e^{-n}\\) is correct.\nFirst, observe the relationship between \\(n!\\) and \\(n^n\\):\n\\[n! = \\underbrace{n \\cdot (n-1) \\cdots 1}_{n \\text{ terms}} < \\underbrace{n \\cdot n \\cdots n}_{n \\text{ times}} = n^n\\]"
  },
  {
    "objectID": "week2/week2.html",
    "href": "week2/week2.html",
    "title": "Week 2",
    "section": "",
    "text": "Conditional Probability and Independence\nIf we go back to the formula for sampling with replacement, the \\(-1\\) term comes from the fact that when converting from the number of bins we could put balls into to the number of dividers between the bins, there’s one less divider than there are bins.\nThat formula was \\[ {n + k - 1 \\choose k} \\] so we could think about how to put \\(k\\) balls into \\(n\\) bins, or instead \\(k\\) balls into \\(n-1\\) dividers, or we could think about there being \\(n-1\\) blue balls and \\(k\\) red balls and the blue balls represent the dividers, so now we’re describing choosing \\(k\\) balls to be red out of \\(n + k - 1\\) balls.\nWe have only been so far considering events and outcomes in the sample space \\(S\\). Often it’s most useful to work with functions of the outcome.\nFor instance, suppose a coin is tossed \\(N\\) times.\nA natural definition of the sample space would be \\(S = \\{ 0, 1 \\}^N\\), that is all sequences of \\(N\\) zeroes or ones.\nDefine \\(X\\) to be the number of times that heads comes up.\nIf we only want to evaluate whether the coin is biased, we may as well work with \\(X\\) rather than the whole sequence.\n\\(X\\) can be thought of as a function from the sample space \\(S\\) to the set of integers.\nDefinition. A random variable \\(X\\) is a function from the sample space equipped with sigma algebra \\(\\Omega\\) to the real numbers \\(X: S \\to \\mathbb R\\). Technically it must be a measurable function, that is \\(X^{-1}(A)\\) must be a measurable set for all measurable sets \\(A \\in \\mathcal B(\\mathbb R)\\). But we won’t worry about this so much in this course.\nIn other words, when the outcome is \\(s \\in S\\), the random variable takes the value \\(X(s)\\) which is some real number.\nThe probability that \\(X\\) takes value \\(x\\), denoted \\(P(X=x)\\), is \\[P(X=x) = P(\\{s \\in S \\colon X(s) = x \\})\\]\nIn the coin tossing example, if \\(s = (s_1, ..., s_N) \\in S = \\{0,1\\}^N\\), then the number of heads \\(X(s) = \\sum_{i=1}^N s_i\\).\nIf the probability of heads is \\(q \\in (0,1)\\), then\n\\[P(\\{s\\}) = \\prod_{i=1}^N q^{s_i}(1-q)^{1-s_i} = q^x (1-q)^{N-x}\\]\nwhere \\(x = \\sum_{i=1}^N s_i\\). Let \\(X(s) = \\sum_{i=1}^N s_i\\).\nThe probability of getting heads \\(x\\) times in \\(N\\) coin tosses is\n\\[P(X=x) = P(\\{ s \\in S \\colon X(s) = x \\}) = \\sum_{s \\in S} P(\\{ s\\})\n\\mathbb 1(X(s) = x)\\] \\[ = \\sum_{s \\in S} q^x (1-q)^{N-x} \\mathbb 1 (X(s)=x)\\] \\[ {N \\choose x} q^x (1-q)^{N-x}\\]\n\\(X\\) is said to follow the binomial distribution with parameters \\(N\\) and \\(q\\). This is denoted by writing \\(X \\sim \\text{Binomial}(N,q)\\).\nWe often think of \\(X\\) as a random quantity, but formally it is a function.\nExercise 1. Prove a generalized Bonferroni inequality:\n\\[ P \\left( \\cap_{i=1}^n A_i \\right) \\geq \\sum_{i=1}^n P(A_i) - (n-1) \\quad \\text{ for any events } A_1, ..., A_n.\\]\nProof 1. We proceed by induction. The base case is clear: \\(P(A_1) \\geq P(A_1) - (1-1)\\).\nAssume an inductive hypothesis: \\[P(\\cap_{i=1}^n A_i) \\geq \\sum_{i=1}^n P(A_i) - (n-1).\\]\nNow we can write that \\[P\\left(\\bigcap_{i=1}^{n+1} A_i \\right) = P\\left(A_{n+1} \\cap \\bigcap_{i=1}^n A_i \\right).\\]\nRecall Boole’s inequality: \\(P(B \\cap A^c) = P(B) - P(A \\cap B)\\), and then let \\(B = \\cap_{i=1}^n A_i\\) and \\(A^c = A_{n+1}\\). Then \\[\\begin{aligned}\nP\\left(A_{n+1} \\cap \\bigcap_{i=1}^n A_i \\right) & =\nP\\left(\\bigcap_{i=1}^n A_i\\right) - P\\left(A^c_{n+1} \\cap \\bigcap_{i=1}^n A_i\\right) \\\\\n& \\geq \\sum_{i=1}^n P(A_i) - (n-1) - P\\left(A^c_{n+1} \\cap \\bigcap_{i=1}^n A_i \\right). \\quad (\\star)\n\\end{aligned}\\]\nNow let’s apply Boole’s inequality again to the right-most probability, this time letting \\(B = A^c_{n+1}\\) and \\(A^c = \\bigcap_{i=1}^n A_i\\).\n\\[\nP\\left(A^c_{n+1} \\cap \\bigcap_{i=1}^n A_i \\right) =\nP(A^c_{n+1}) - P\\left(A^c_{n+1} \\cap \\left( \\bigcap_{i=1}^n A_i\\right)^c \\right).\n\\]\nApplying the fact that \\(P(A) = 1-P(A^c)\\) for any set \\(A\\), we then have that \\[P(A^c_{n+1}) = 1 - P(A_{n+1}) \\quad (\\star) \\]\nSubstituting that back in, we have that:\n\\[\nP\\left(A^c_{n+1} \\cap \\bigcap_{i=1}^n A_i \\right) =\n1 - P(A_{n+1}) - P\\left(A^c_{n+1} \\cap \\left( \\bigcap_{i=1}^n A_i\\right)^c \\right).\n\\]\nAnd then substituting that back into \\((\\star)\\):\n\\[P\\left( \\bigcap_{i=1}^{n+1} A_i \\right) \\geq\n\\sum_{i=1}^n P(A_i) - (n-1) -\n\\left[ 1 - P(A_{n+1}) - P\\left(A^c_{n+1} \\cap \\left( \\bigcap_{i=1}^n A_i\\right)^c \\right) \\right]\n\\] \\[\n= \\sum_{i=1}^n P(A_i) - (n-1) -\n1 + P(A_{n+1}) + P\\left(A^c_{n+1} \\cap \\left( \\bigcap_{i=1}^n A_i\\right)^c \\right)\n\\] \\[\n= \\sum_{i=1}^{n+1} P(A_i) - (n+1-1) + \\underbrace{P\\left(A^c_{n+1} \\cap \\left( \\bigcap_{i=1}^n A_i\\right)^c \\right)}_{\\geq 0 \\, \\text{ by the axioms of probability}}\n\\] \\[\n\\geq \\sum_{i=1}^{n+1} P(A_i) - (n+1-1)\n\\]\nThis concludes the proof.\nProof 2. We proceed by proof by contradiction: Assume that \\[P\\left( \\bigcap_{i=1}^n A_i \\right) < \\sum_{i=1}^n P(A_i) - (n-1)\\] \\[P\\left( \\bigcap_{i=1}^n A_i \\right) < 1 - n + \\sum_{i=1}^n P(A_i)\\] \\[P\\left( \\bigcap_{i=1}^n A_i \\right) < 1 - n + \\sum_{i=1}^n 1 - P(A_i^c)\\] \\[P\\left( \\bigcap_{i=1}^n A_i \\right) < 1 - n + n - \\sum_{i=1}^n P(A_i^c)\\] \\[P\\left( \\bigcap_{i=1}^n A_i \\right) < 1 - \\sum_{i=1}^n P(A_i^c)\\]\nBut this contradicts (the un-generalized version of) Bonferroni’s inequality. \\(\\rightarrow \\leftarrow\\). This concludes the proof.\nExercise 2. Given \\(B \\subset C\\) and \\(P(B) > 0\\), prove that \\[P(A | C) > P(A | B) \\Longleftrightarrow P(A | C \\cap B^c) > P(A | B).\\]\nExercise 3. Prove or disprove the following statement: For events \\(A\\) and \\(B\\) such that \\(0 < P(A) < 1\\) and \\(0 < P(B) < 1\\), \\[P(A|B) > P(A) \\Longleftrightarrow P(B | A) > P(B|A^c).\\]\n1-2pm Office Hours for Miller, Building 1 Room 245"
  },
  {
    "objectID": "week2/week2.html#jackpot-scenario",
    "href": "week2/week2.html#jackpot-scenario",
    "title": "Week 2",
    "section": "Jackpot Scenario",
    "text": "Jackpot Scenario\nWhen the pot reached $5M, a “rolldown” occurred in which the prizes for matching 3, 4, or 5 numbers were 10x higher.\n\n\n\nMatch Number\n6\n5\n4\n\n\n\n\nProbability\n1/13,983,815\n1/54,201\n1/1032\n\n\nAll prizes (in 2 years)\n15\n2158\n117685\n\n\nPrize (normal)\nJackpot\n$2,500\n$100\n\n\nPrize in Fall\nJackpot not hit\n$25,000\n$1,000\n\n\n\nIf there were a rolldown in the WINfall lottery, we’d have an expected return on a single ticket as:\n\n(1/54201)*25000 + (1/1032)*1000 + (1/57)*(50)\n\n[1] 2.307431"
  },
  {
    "objectID": "week2/week2.html#hypergeometric-distribution",
    "href": "week2/week2.html#hypergeometric-distribution",
    "title": "Week 2",
    "section": "Hypergeometric Distribution",
    "text": "Hypergeometric Distribution\nThe Hypergeometric\\((N,K,n)\\) distribution gives the probability of matching \\(k\\) numbers from a set of \\(K\\) winning numbers when selecting \\(n\\) from a set of \\(N\\) total.\nWe often say distribution instead of probability measure.\nIn the Winfall lottery, \\(N = 49\\), \\(K=6\\), and \\(n=6\\), and the outcome is the number of matches \\(k\\).\nLetting \\(P\\) denote the Hypergeometric\\((N,K,n)\\) distribution, \\[P(\\{k\\}) = \\frac{{K \\choose k}{N - K \\choose n - k}}{N \\choose n}.\\]\nPlugging \\(k=3, k=4, k=5, k=6\\) into this formula yields the probabilities in the above table."
  },
  {
    "objectID": "week2/week2.html#playing-cards",
    "href": "week2/week2.html#playing-cards",
    "title": "Week 2",
    "section": "Playing Cards",
    "text": "Playing Cards\nWhat’s the probability of drawing 4 cards from a deck and getting 4 aces?\nThe number of possible hands of 4 cards is \\({52 \\choose 4}\\).\nThus the probability of getting all 4 aces is\n\\[\\frac{1}{52 \\choose 4} = \\frac{4!48!}{52!}.\\]\nOr we could think about it sequentially: The probability of drawing an ace on the first card is \\(4/52\\), the next is \\(3/51\\), then \\(2/50\\), and \\(1/49\\) each conditioned on assuming we previously drew an ace card.\n\n\\[ \\frac{4}{52} \\times \\frac{3}{51} \\times \\frac{2}{50} \\times \\frac{1}{49} = \\frac{4!48!}{52!} = \\frac{4! \\cancel{48 \\cdot 47 \\cdots 1}}{52 \\cdot 51 \\cdot 50 \\cdot 49 \\cdot \\cancel{48 \\cdot 47 \\cdots 1}} \\]\nDefinition. The conditional probability of \\(A\\) given \\(B\\) denoted \\(P(A|B)\\) is\n\\[ P(A|B) = \\frac{P(A \\cap B)}{P(B)}\\]\nWe could think of \\(B\\) as the event where 1 ace has already been drawn (and thus represents the scenario where \\(B\\) consists of all the cards except 1 ace). Then,\n\\[P(A | B) =\n\\frac{P(\\{ \\text{Ace} \\clubsuit, \\text{Ace} \\diamondsuit, \\text{Ace} \\spadesuit, \\text{Ace} \\heartsuit\\} \\cap B\\})}{P(B)} = \\frac{3}{51} \\]"
  },
  {
    "objectID": "week2/week2.html#bayes-rule",
    "href": "week2/week2.html#bayes-rule",
    "title": "Week 2",
    "section": "Bayes’ Rule",
    "text": "Bayes’ Rule\n\\[P(A|B) = \\frac{P(A \\cap B)}{P(B)}.\\]\nMultiplying by \\(P(B)\\) yields:\n\\[P(A \\cap B) = P(A|B) P(B).\\]\nBy symmetry, \\[P(A \\cap B) = P(B|A) P(A).\\]\nIf \\(P(A) > 0\\) and \\(P(B) > 0\\), then \\[P(A|B) = \\frac{P(B|A) P(A)}{P(B)}.\\]\nIf \\(A_1, A_2, ...\\) form a partition of the sample space then\n\\[P(A_i |B) = \\frac{P(B|A_i) P(A_i)}{\\sum_{j=1}^\\infty P(B|A_j) P(A_j)}.\\]"
  },
  {
    "objectID": "week2/week2.html#the-monty-hall-problem",
    "href": "week2/week2.html#the-monty-hall-problem",
    "title": "Week 2",
    "section": "The Monty Hall Problem",
    "text": "The Monty Hall Problem\nOn a game show, there are 3 doors. Behind one door is a car, and behind the other two doors are goats. You win whatever is behind the door you select. First you pick door #1. The game show host then reveals that there is a goat behind door #3. The host then asks “Do you want to stay with #1 or switch to #2?” What would you do and why?\nIt turns out you should always switch. Let \\(D_1, D_2, D_3\\) denote the events that the car is behind door 1, 2, or 3. Let \\(M_1, M_2, M_3\\) denote the event that Monty opens door 1, 2, or 3, respectively.\nWe will assume that there’s equal probability of the car being behind 1, 2, or 3. Additionally, we will assume that he always opens a door that is not the one you picked and which does not have the car behind it.\nBy assumption \\[P(D_1) = P(D_2) = P(D_3) = 1/3.\\]\n\\[P(M_j|D_1) = (1/2) \\mathbb 1 (j \\in \\{2,3\\})\\] \\[P(M_j|D_2) = \\mathbb 1 (j = 3)\\] \\[P(M_j|D_3) = \\mathbb 1 (j = 2)\\]\nHere’s a table of the probability of each possible combination \\(i,j\\) of door that the car is behind \\((i)\\) and the door opened by Monty \\((j)\\):\n\n\n\n\nOpen #1\nOpen #2\nOpen #3\n\n\n\n\nCar in #1\n0\n1/6\n1/6\n\n\nCar in #2\n0\n0\n1/3\n\n\nCar in #3\n0\n1/3\n0\n\n\n\nBy the law of total probability, the probability that Monty opens door #3 is\n\\[P(M_3) = \\sum_{i=1}^3 P(M_3|D_i) P(D_i) = \\frac{1}{2} \\times \\frac{1}{3} +\n1 \\times \\frac{1}{3} + 0 \\times \\frac{1}{3} = \\frac{1}{2}.\\]\nSo by Bayes’ rule the conditional probability of the car being behind door #1, given Monty opened door #3, is\n\\[P(D_1|M_3) = \\frac{P(M_3|D_1) P(D_1)}{P(M_3)} = \\frac{(1/2) \\times (1/3)}{1/2} = \\frac{1}{3}\\]\nMeanwhile, the conditional probability of the car being behind door #2 given that Monty opened door #3 is\n\\[P(D_2 | M_3) = \\frac{P(M_3|D_2) P(D_2)}{P(M_2)} = \\frac{1 \\times (1/3)}{(1/2)} = \\frac{2}{3}\\]\n\nWhat’s wrong with the following reasoning?\nWe can think of the sample space as \\(\\{1,2,3\\}\\) and the outcome as the number that the door is behind, so \\(D_i = \\{i\\}.\\)\nBy conditioning on Monty opening door #3, we are conditioning on the event that the car is behind #1 or #2, that is \\(M_3 = D_3^c = \\{1,2\\}\\). Therefore\n\\[ P(D_1 | M_3) = \\frac{P(D_1 \\cap M_3)}{P(M_3)} \\] \\[ = \\frac{P(\\{1\\} \\cap \\{1,2\\})}{P(\\{1,2\\})} \\] \\[ = \\frac{1/3}{2/3} = \\frac{1}{2}.\\]\nFurther, \\(P(D_2|M_3) = 1 - P(D_1 | M_3) = 1/2\\) so we gain nothing by switching to door #2.\nThe problem is that this doesn’t condition on the fact that Monty will never choose the door with the car. In other words, the door Monty picks depends on your choice: when your door contains a goat, Monty only has one choice: the remaining door with a goat.\nIn essence, we need to define the joint-distribution of \\(D_i\\) and \\(M_j\\).\n\nThe intuition is that the probability gets squished into the other doors, so you get probabilities of \\(1/3\\) and \\(2/3\\)."
  },
  {
    "objectID": "week2/week2.html#independence",
    "href": "week2/week2.html#independence",
    "title": "Week 2",
    "section": "Independence",
    "text": "Independence\nIf we had a coin and we flipped it multiple times, where \\(A\\) represents the event where it comes up heads the first time and \\(B\\) is the event where it comes up heads the second time.\nWe would assume that \\(P(B|A) = P(B)\\).\nBy Bayes’ theorem, \\(P(B|A) = P(A \\cap B)/P(A)\\), that implies that \\(P(A \\cap B) = P(A)P(B)\\). When this holds, we say that \\(A\\) and \\(B\\) are independent.\nOne difference is that the second statement doesn’t require that \\(P(A)\\) is nonzero, but the definition using \\(P(B|A)\\) does.\nIf \\(A\\) and \\(B\\) are independent, then so are \\(A\\) and \\(B^c\\), \\(A^c\\) and \\(B\\), and \\(A^c\\) and \\(B^c\\).\nEvents \\(A_1, ..., A_n\\) are mutually independent if \\[P(\\cap_{i \\in I} A_i) = \\prod_{i \\in I} P(A_i)\\] for every subset \\(I \\subset \\{ 1, ..., n \\}.\\)\nYou might think that \\(P(A_1 \\cap \\cdots \\cap A_n) = P(A_1) \\cdots P(A_n)\\) would be a simpler definition of independence of multiple events, but this is not correct. See Casella & Berger (1.3.10)."
  },
  {
    "objectID": "week2/week2.html#continuing-on-random-variables",
    "href": "week2/week2.html#continuing-on-random-variables",
    "title": "Week 2",
    "section": "Continuing on Random Variables",
    "text": "Continuing on Random Variables\nIf the range of a random variable \\(\\{X(s) : s \\in S \\}\\) is countable then \\(X\\) is a discrete random variable.\nFor a discreet random variable, the function \\(f(x) = P(X = x)\\) is called a probability mass function.\nRecall how we define probability on random variables: \\[P(X = x)  = P(\\{ s \\in S : X(s) = x \\})\\]"
  },
  {
    "objectID": "week2/week2.html#cumulative-distribution-functions",
    "href": "week2/week2.html#cumulative-distribution-functions",
    "title": "Week 2",
    "section": "Cumulative Distribution Functions",
    "text": "Cumulative Distribution Functions\nThe cumulative distribution function (cdf) of \\(X\\) is defined to be \\[F(x) = P(X \\leq x).\\]\nDefinition. A function \\(F \\colon \\mathbb R \\to \\mathbb R\\) is a cdf if and only if\n\n\\(\\lim_{x \\to -\\infty} F(x) = 0\\) and \\(\\lim_{x \\to \\infty} F(x) = 1\\)\n\\(F(x)\\) is non-decreasing.\n\\(F(x)\\) is right-continuous, that is for all \\(x' \\in \\mathbb R\\) \\[ \\lim_{x \\downarrow x'} F(x) = F(x')\\]\n\nHere’s an example for a binomial distribution:\n\n\n\nBinomial distribution CDF\n\n\nFor any discrete probability distribution, the cdf will necessarily be discontinuous.\nAn example of a continuous cdf is \\[F(x) = (1- e^{-\\lambda x}) \\mathbb 1 (x > 0),\\] where \\(\\lambda\\) is a positive fixed number.\n\n\n\nExponential distribution CDF\n\n\n\nContinuous Random Variables\nA random variable is continuous if its cdf is a continuous function.\nIn other words, if \\(\\lim_{x \\to x'} F(x) = F(x')\\) for every \\(x' \\in \\mathbb R\\).\nA probability density function of a continuous random variable function \\(f: \\mathbb R \\to [0,\\infty)\\) such that for all \\(x \\in \\mathbb R\\), \\[F(x) = \\int_{-\\infty}^\\infty f(t) dt.\\]\nTherefore we need to integrate to get probabilities:\n\\[P(X \\in A) = \\int_A f(x) dx\\]"
  },
  {
    "objectID": "week2/week2.html#relationship-between-cdf-and-pdf",
    "href": "week2/week2.html#relationship-between-cdf-and-pdf",
    "title": "Week 2",
    "section": "Relationship between CDF and PDF",
    "text": "Relationship between CDF and PDF\n“Randavble” = “Random Variable” (Nice abbreviation, but a joke)\nSuppose we had a cdf \\(F\\), we could derive a pdf \\(f\\) by differentiating (usually).\nSimilarly, we can obtain \\(F\\) from \\(f\\).\nIf \\(f(x)\\) is continuous at \\(x\\), then \\[f(x) = \\frac{d}{dx} F(x),\\] by the fundamental theorem of calculus.\nA function \\(f: \\mathbb R \\to [0,\\infty)\\) is a probability density function if and only if \\(\\int f(x) dx = 1\\).\nIn fact any integrable non-negative function can be normalized to a pdf, by performing \\(f(x) = \\tilde{f}(x)/C\\) for a constant \\(C\\).\n\n\n\nNormalizing a pdf to have integral 1\n\n\nThere is not a unique pdf for a given distribution since the pdf can change arbitrarily on sets with Lebesgue measure 0. However, any two pdfs for the same distribution will agree “almost everywhere.”\nNot every continuous distribution has a pdf, since there are continuous cdfs that are nondifferentiable at uncountably many points, such as the Cantor function.\n\n\n\n\n\n\n\n\n\nRecall that the Exponential(\\(\\lambda\\)) distribution is defined by the cdf: \\[F(x) = (1-e^{-\\lambda x}) \\mathbb 1\\{x > 0\\}.\\]\nThe pdf can be obtained by differentiating:\n\\[f(x) = \\frac{d}{dx} F(x)= \\lambda e^{-\\lambda x} \\mathbb 1 \\{x > 0\\}.\\]\n\\(F(x)\\) is not differentiable at 0, but it doesn’t matter since \\(f(x)\\) can be defined arbitrarily on any countable set.\n\n\n\nThe exponential pdf\n\n\nFor measurable sets \\(A \\subset \\mathbb R\\), we define \\[X^{-1}(A) = \\{ s \\in S \\colon X(s) = x \\}\\]\nFor any random variable \\(X\\) not necessarily discrete or continuous, we define \\[P(X \\in A) = P(X^{-1}(A)).\\]"
  },
  {
    "objectID": "week2/week2.html#identically-distributed-random-variables",
    "href": "week2/week2.html#identically-distributed-random-variables",
    "title": "Week 2",
    "section": "Identically distributed random variables",
    "text": "Identically distributed random variables\nRandom variables \\(X\\) and \\(Y\\) are identically distributed if for every measurable set \\(A\\), \\[P(X \\in A) = P(Y \\in A).\\]\nThis is denoted by \\(P \\stackrel{d}{=} Y\\).\nIt turns out that \\(X \\stackrel{d}{=} Y\\) if and only if \\(F_X(x) = F_Y(x)\\) for all \\(x \\in \\mathbb R\\)."
  },
  {
    "objectID": "week2/week2.html#transformations-of-random-variables",
    "href": "week2/week2.html#transformations-of-random-variables",
    "title": "Week 2",
    "section": "Transformations of Random Variables",
    "text": "Transformations of Random Variables\nOften we are interested in functions \\(g(X)\\) of a random variable \\(X\\).\nIf \\(g\\) is measurable, then \\(g(X)\\) is a random variable.\nLetting \\(Y = g(X)\\), the distribution of \\(Y\\) is characterized by \\[P(Y \\in A) = P(g(X) \\in A) = P(X \\in g^{-1}(A)).\\]\nRecall that \\[g^{-1}(A) = \\{ x \\colon g(x) \\in A \\},\\] so this does not require that \\(g\\) be invertible.\nIn the discrete case,\n\\[f_Y(y) = P(Y = y) = P(g(X) = y) = \\sum_{x \\colon g(x) = y} f_X(x).\\]"
  },
  {
    "objectID": "week2/week2.html#binomial-variable-transformation-example",
    "href": "week2/week2.html#binomial-variable-transformation-example",
    "title": "Week 2",
    "section": "Binomial Variable Transformation Example",
    "text": "Binomial Variable Transformation Example\nSuppose \\(X \\sim \\text{Binomial}(N,q)\\) and \\(Y = N - X\\). Then \\[P(Y = k) = P(g(X) = k) = P(N - X = k) = P(X = N-k).\\]\nThus plugging in the pdf of \\(X\\):\n\\[ P(Y=k) = {N \\choose N-k} q^{N-k} (1-q)^{N-(N-k)}\\] \\[ = { N \\choose k} (1-q)^k q^{N-k}.\\]\nTherefore \\(Y \\sim \\text{Binomial}(N,1-q)\\)."
  },
  {
    "objectID": "week2/week2.html#transformations-of-continuous-random-variables",
    "href": "week2/week2.html#transformations-of-continuous-random-variables",
    "title": "Week 2",
    "section": "Transformations of continuous random variables",
    "text": "Transformations of continuous random variables\nFor a continuous r.v. \\(X\\) we have to take more care.\n\nIf \\(g\\) is invertible, then you might mistakenly think the pdf of \\(Y = g(X)\\) equals \\(f_X(g^{-1}(y))\\), but this is not true in general. The reason is that the pdf \\(f_Y\\) is a density, not a probability.\nThe correct formula accounts for the derivative.\nSuppose \\(X\\) is a continuous r.v. and let \\(\\mathcal X = \\{ x \\colon f_X(x) > 0 \\}\\). If \\(Y = g(X)\\) where \\(g \\colon \\mathcal X \\to \\mathbb R\\) is a strictly monotone function such that the inverse \\(g^{-1}(y)\\) has a continuous derivative then \\[f_Y(y) = f_X(g^{-1}(y)) \\left \\lvert \\frac{d}{dy}g^{-1}(y) \\right\\rvert\\]\nfor \\(y \\in \\mathcal Y \\coloneqq \\{ g(x) \\colon x \\in \\mathcal X \\}\\) and \\(f_Y(y) = 0\\) elsewhere.\n“This is a great source for exam problems!”\n\n\nMonotonicity\nA function is monotone increasing if \\[x < x' \\Longrightarrow g(x) \\leq g(x').\\] We also call this non-decreasing.\nSimilarly, a function is monotone decreasing (or non-increasing) if \\[ x < x' \\Longrightarrow g(x) \\geq g(x').\\]\nNote that Casella and Berger use “monotone” to imply strictly monotone, though the definitions above are the more conventional meaning."
  },
  {
    "objectID": "week2/week2.html#example-square-root-of-an-exponential-r.v.",
    "href": "week2/week2.html#example-square-root-of-an-exponential-r.v.",
    "title": "Week 2",
    "section": "Example: Square root of an Exponential r.v.",
    "text": "Example: Square root of an Exponential r.v.\nSuppose \\(X \\sim \\text{Exponential}(\\lambda)\\) and \\(Y=\\sqrt{X}\\).\nThen \\(f_X(x) = \\lambda \\exp(-\\lambda x) \\mathbb 1 \\{ x > 0 \\}\\), so \\(\\mathcal X = (0,\\infty).\\)\nWe write \\(Y = g(X)\\) where \\(g(x) = \\sqrt{x}\\) for \\(x \\in \\mathcal X\\).\nFor \\(y \\in \\mathcal Y = (0,\\infty)\\), the inverse of \\(g\\) is \\[g^{-1}(y) = y^2.\\]\n\\[\\frac{d}{dx} g^{-1}(y) = 2y.\\]\nThus by the change of variables formula above,\n\\[f_Y(y) = f_X(g^{-1}(y)) \\left\\lvert \\frac{d}{dx} g^{-1}(y) \\right\\rvert\\] \\[ = 2\\lambda y \\exp (-\\lambda y^2) \\mathbb 1 \\{ y > 0 \\}.\\]"
  },
  {
    "objectID": "week3/week3.html",
    "href": "week3/week3.html",
    "title": "Week 3",
    "section": "",
    "text": "Probability Distributions of Transformed Random Variables\nThe uniform distribution has a particularly important role in probability. We define the uniform distribution on \\((a,b)\\) where \\(a < b\\) as\n\\[f_U(u) = \\frac{1}{b-a} \\mathbb 1 ( a < u < b) \\]\nfor \\(u \\in \\mathbb R\\). This is denoted \\(U \\sim \\text{Uniform}(a,b)\\).\nThe cdf of \\(U \\sim \\text{Uniform}(a,b)\\) is\n\\[F_U(u) = \\left\\{ \\begin{array}{ll} 0 \\quad \\quad & \\text{ if } u \\leq a \\\\ (u-a)/(b-a) & \\text{ if } u \\in (a,b) \\\\ 1 & \\text{ if } u \\geq b \\end{array}\\right.\\]\nThe standard uniform distribution Uniform(0,1) has the special property that \\(CDF(u) = u\\).\nThe expected value denoted \\(\\mathbb E(X)\\) or \\(\\mathbb EX\\) is the average over all values that the random variable takes weighted according to their probability or probability density. It is also referred to as the expectation or mean of \\(X\\).\nLet \\(X\\) be a random variable and let \\(g(x)\\) be a measurable function. If \\(X\\) is discrete then the expected value is\n\\[\\mathbb Eg(X) = \\sum_{x \\in \\mathcal X} g(x) f(x)\\]\nWhere \\(\\mathcal X = \\{ X(s) : s \\in S \\}\\) is the range of \\(X\\) and \\(f(x)\\) is the pmf of \\(X\\).\nIf \\(X\\) is continuous then the expected value of \\(g(X)\\) is \\[\\mathbb Eg(X) = \\int_{-\\infty}^\\infty g(x) f(x) \\, dx\\] where \\(f(x)\\) is the pdf of \\(X\\)."
  },
  {
    "objectID": "week3/week3.html#change-of-variables-piecewise",
    "href": "week3/week3.html#change-of-variables-piecewise",
    "title": "Week 3",
    "section": "Change of variables piecewise",
    "text": "Change of variables piecewise\nOften \\(g(x)\\) is not strictly monotone, but is piecewise strictly monotone, e.g., \\(g(x) = x^2, \\, \\forall x \\in \\mathbb R\\).\nSuppose that \\(X\\) is a continuous random variable and let \\(\\mathcal X = \\{ x \\colon f_X(x) > 0 \\}\\).\nSuppose \\(g \\colon \\mathcal X \\to \\mathbb R\\) and \\(A_0, A_1, ..., A_k\\) is a partition of \\(\\mathcal X\\) such that\n\n\\(P(X \\in A_0) = 0\\),\n\\(g\\) is strictly monotone on \\(A_i\\) for \\(i = 1,...,k\\),\nthe inverse of \\(g\\) on \\(A_i\\), say \\(g_i^{-1}\\), has a continuous derivative on \\(g(A_i) = \\{ g(x) \\colon x \\in A_i \\}\\),\n\nthen\n\\[f_Y(y) = \\sum_{i = 1}^k f_X(g^{-1}_i(y)) \\left| \\frac{d}{dy} g_i^{-1}(y) \\right| \\mathbb 1(y \\in g(A_i)).\\]\n\n\n\n\n\nExample of partitioning a function into monotone sections\n\n\n\n\nWe were discussing in class whether or not the partitioning needs to be finite, and we think that one could use infinitely many partitions in some cases (say, for example, a distribution convolved with the \\(\\sin\\) function).\n\nExample: Normal to Chi-squared Transformation\nA random variable \\(X\\) has the standard normal distribution if\n\\[f_X(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp(-\\frac{1}{2} x^2)\\] for \\(x \\in \\mathbb R\\). This is denoted \\(X \\sim \\mathcal N(0,1)\\).\nIf \\(X \\sim \\mathcal N(0,1)\\), then the random variable \\(Y = X^2\\) has the chi-squared distribution, denoted \\(Y \\sim \\chi^2\\).\n\\(g(x) = x^2\\) is strictly monotone on \\(A_1 = (-\\infty, 0)\\) and \\(A_2 = (0,\\infty)\\), wiith inverses \\(g_1^{-1}(y) = -\\sqrt{x}\\) and \\(g_2^{-1}(y) = \\sqrt{x}\\).\nBy the change of variables formula (in the piecewise case), for \\(y > 0\\) (remember we have to check when \\(y \\in g^{-1}(A_i)\\)),\n\\[f_Y(y) = \\frac{1}{\\sqrt{2\\pi}} \\exp(-\\frac{1}{2}(-\\sqrt{y})^2)\\left| \\frac{-1}{2\\sqrt{y}} \\right| +\n\\frac{1}{\\sqrt{2\\pi}} \\exp(-\\frac{1}{2}(\\sqrt{y})^2)\\left| \\frac{1}{2\\sqrt{y}} \\right| \\] \\[ = \\frac{1}{\\sqrt{2\\pi}} \\frac{1}{\\sqrt{y}} \\exp(-\\frac{1}{2} y).\\]\nAlong the way, we had to evaluate \\(\\frac{d}{dy} (-\\sqrt{y}) = \\frac{d}{dy} (-y^{1/2}) = \\frac{-1}{2}y^{-1/2}.\\)\nWe could write this ever-so-slightly more precisely:\n\\[ = \\left\\{ \\begin{array}{ll} \\frac{1}{\\sqrt{2\\pi}} \\frac{1}{\\sqrt{y}} \\exp(-\\frac{1}{2} y) \\quad & \\text{ if } y > 0 \\\\\n0 & \\text{ otherwise } \\end{array} \\right.\\]\n\nThese kinds of problems are great for exams!"
  },
  {
    "objectID": "week3/week3.html#probability-integral-transform",
    "href": "week3/week3.html#probability-integral-transform",
    "title": "Week 3",
    "section": "Probability Integral Transform",
    "text": "Probability Integral Transform\nLet \\(X\\) be a random variable with cdf \\(F\\). If \\(F\\) is a continuous function, then \\(F(X) \\sim \\text{Uniform}(0,1)\\).\nThis is called the probability integral transform.\nExample: Let \\(X \\sim \\text{Exponential}(\\lambda)\\). The cdf of \\(X\\) is \\[F(x) = (1-\\exp(-\\lambda x)) \\mathbb 1(x > 0),\\] which is a continuous function. Therefore, \\[1 - \\exp(-\\lambda X) \\sim \\text{Uniform}(0,1).\\]\nWhy can we drop the \\(\\mathbb 1 (X > 0)\\) factor? Because \\(X\\) is an exponential variable, so \\(X > 0\\) with probability 1. This is like asking \\[(1-e^{-\\lambda x}) \\mathbb 1 (X > 0) \\stackrel{d}{=} (1-e^{-\\lambda x})?\\]\nCan we simplify further? Yes! \\(\\text{Uniform}(0,1) = 1 - \\text{Uniform}(0,1)\\).\nSo we can rewrite this:\n\\[\\exp{-\\lambda x} = \\text{Uniform}(0,1).\\]"
  },
  {
    "objectID": "week3/week3.html#generalized-inverse-of-a-cdf",
    "href": "week3/week3.html#generalized-inverse-of-a-cdf",
    "title": "Week 3",
    "section": "Generalized Inverse of a cdf",
    "text": "Generalized Inverse of a cdf\nA cdf \\(F\\) can fail to be invertible in two ways:\n\n\\(F(x) = F(y)\\) for some \\(x \\neq y\\) (it is flat in some region), or\n\\(F\\) is discontinuous at some \\(x\\) (it has a jump at some point).\n\nThe generalized inverse of a cdf \\(F\\) is the function\n\\[G(u) = \\inf\\{x \\in \\mathbb R\\colon F(x) \\geq u \\}\\] for \\(u \\in (0,1).\\) We write \\(F_{-1}\\) to denote this function.\nWhen \\(F\\) is invertible, the generalized inverse equals the inverse, so there is no conflict in notation."
  },
  {
    "objectID": "week3/week3.html#inverse-probability-integral-transform",
    "href": "week3/week3.html#inverse-probability-integral-transform",
    "title": "Week 3",
    "section": "Inverse Probability Integral Transform",
    "text": "Inverse Probability Integral Transform\nLet \\(F\\) be any cdf. If \\(U \\sim \\text{Uniform}(0,1)\\) then \\(F^{-1}(U)\\) is a random variable with cdf \\(F\\).\nThis is called the inverse probability integral transform or the Smirnov transform.\nThe two transforms can be summarized as follows. Suppose \\(U \\sim \\text{Uniform}(0,1)\\) and \\(X\\) is a random variable with cdf \\(F\\). Then\n\n\\(F^{-1}(U) \\stackrel{d}{=} X.\\)\n\\(F(X) \\stackrel{d}{=} U\\) if \\(F\\) is continuous, but not otherwise.\n\n\nActivity: What is the distribution of \\(F(X)\\) in this example?\nMy thoughts:\nIf \\[F(x) = \\left\\{ \\begin{array}{ll}\n0 \\quad & \\text{ if } x \\leq 0 \\\\\nx \\quad & \\text{ if } 0 < x < .5 \\\\\n1 & \\text{ if } .5 \\leq x\n\\end{array}\\right.\\]\nSo if we take the derivative, \\[\\frac{d}{dx} F(x) = f_X(x) = \\left\\{\n\\begin{array}{ll}\n0 \\quad & \\text{ if } x \\leq 0 \\\\\n1 \\quad & \\text{ if } 0 < x < .5 \\\\\n0 & \\text{ if } .5 \\leq x\n\\end{array}\n\\right. \\]\nAnother student pointed out that looking at the height of the jump, there’s \\(Pr(X = .5) = .5\\),\nSo is it \\(f_X(x) = \\mathbb 1(0 < x < 0.5) + .5 \\times \\delta_{x = .5}(x)\\)?\nThere is a small mistake. The delta-distribution should be at 1, because when \\(x=1\\), \\(F(x) = 1\\).\nSo \\[F(x) = \\left\\{ \\begin{array}{ll}\n1 & \\text{ with probability }\\frac{1}{2} \\\\\n\\text{Uniform}(0,\\frac{1}{2}) & \\text{ with probability } \\frac{1}{2}\n\\end{array} \\right.\\]\n\nWhy should we care about this? Well it’s really useful computationally for generating random numbers that have different probability distributions from the uniform distribution. The Mersenne-Twister algorithm is the state of the art for generating uniform distribution random samples, and then often to get random numbers from other distributions one employs the above type of transformations under-the-hood."
  },
  {
    "objectID": "week3/week3.html#properties-of-expectation",
    "href": "week3/week3.html#properties-of-expectation",
    "title": "Week 3",
    "section": "Properties of Expectation",
    "text": "Properties of Expectation\nLet \\(X\\) be a random variable with range \\(\\mathcal X\\) and let \\(g,h \\colon \\mathcal X \\to \\mathbb R\\) be measurable functions such that \\(\\mathbb Eg(X)\\) and \\(\\mathbb Eh(X)\\) exist and are finite.\nThe following properties hold:\n\n\\(\\mathbb E(c\\, g(X)) = c \\mathbb Eg(X)\\) for any \\(c \\in \\mathbb R\\).\n\\(\\mathbb E(g(X) + h(X)) = \\mathbb Eg(X) + \\mathbb Eh(X)\\)\nIf \\(g(x) \\leq h(x)\\) for all \\(x \\in \\mathcal X\\), then \\(\\mathbb Eg(X) \\leq \\mathbb Eh(X)\\).\n\n\nExpectation minimizes the mean squared error\nSuppose we want to choose one point \\(a\\) that predicts \\(Y\\) as closely as possible (for instance, as in regression).\nIf we define “close” in terms of mean squared error, \\(\\mathbb E(|Y - a|^2)\\), then \\(\\mathbb EY\\) is the optimal choice of \\(a\\).\nTo see this, observe that \\[\\mathbb E(|Y-a|^2) = \\mathbb E(Y^2 - 2aY + a^2)\\] \\[ = \\mathbb EY^2 - 2a\\mathbb EY + a^2.\\]\nTo minimize, set the derivative equal to zero:\n\\[ 0 = \\frac{d}{da} \\mathbb E(|Y-a|^2) = -2\\mathbb EY + 2a\\]\nand solve for \\(a\\) to obtain \\(a = \\mathbb EY\\).\n\n\nExistence of Expected Values\nThe positive part of \\(X\\), denoted \\(X^+\\), is \\(X^+ = \\max(X,0)\\).\nThe negative part of \\(X\\), denoted \\(X^-\\), is \\(X^- = -\\min(X,0)\\).\nNote that \\(X = X^+ - X^-\\) and \\(|X| = X^+ + X^-\\).\n\n \n\n\nExistence vs. Finiteness\nWe say that the expected value of \\(X\\) exists if either \\(\\mathbb EX^+ < \\infty\\) or \\(\\mathbb EX^- < \\infty\\) or both. This definition differs from Casella & Berger’s definition.\nIf \\(\\mathbb E|X| < \\infty\\) then \\(\\mathbb EX\\) exists.\n\nThis is a sufficient (but not necessary) condition for existence.\nRecall that \\(|X| = X^+ + X^-\\), so \\(\\mathbb E|X| = \\mathbb EX^+ + \\mathbb EX^-\\).\nTherefore if \\(\\mathbb E|X| < \\infty\\), then \\(\\mathbb EX^+ < \\infty\\) and \\(\\mathbb EX^- < \\infty\\), since \\(X^+ \\geq 0\\) and \\(X^- \\geq 0\\).\n\nIn fact, if \\(\\mathbb E|X| < \\infty\\), then \\(\\mathbb EX\\) is finite. That is, \\(\\mathbb EX \\in \\mathbb R\\). This is because \\(\\mathbb EX = \\mathbb EX^+ - \\mathbb EX^-\\).\n\nWhy does \\(\\mathbb E|X|\\) always exist? Because \\(|X|^- = 0\\) and hence \\(\\mathbb E|X|^- < \\infty\\), and therefore we satisfy the definition of existence for an expectation that either the expectation of the positive part or the negative part are finite.\nThe only time the expectation doesn’t exist is when the expectation of the positive part and negative part are both infinite (resulting in \\(\\mathbb EX = \\mathbb EX^+ - \\mathbb EX^- = \\infty - \\infty = \\text{undefined}\\))."
  },
  {
    "objectID": "week4/week4.html#well-defined-vs.-undefined-expectations",
    "href": "week4/week4.html#well-defined-vs.-undefined-expectations",
    "title": "Week 4",
    "section": "Well-Defined vs. Undefined Expectations",
    "text": "Well-Defined vs. Undefined Expectations\nThe positive part of \\(X\\) is well-defined if either \\(E X^+ < \\infty\\) or \\(E X^- < \\infty\\).\nA random variable \\(X\\) has the Zeta(\\(s\\)) distribution for \\(s > 1\\), if it has pmf\n\\[f_X(k) = P(X = k) = \\frac{1}{\\zeta(s)k^s} \\mathbb 1 (k \\in \\mathbb \\{ 1, 2, ... \\})\\]\nwhere \\(\\mathbb \\zeta (s)\\) is the Riemann zeta function.\nSince \\(EX^- = E(-\\min(X,0)) = 0\\), \\(EX\\) is well-defined.\nRecall that \\(\\zeta(s) = \\sum_{i=1}^\\infty \\frac{1}{k^2}\\).\nHowever, if \\(s \\leq 2\\) then the mean is infinite:\n\\[EX = \\sum_{i=1}^\\infty k f_X(k) = \\sum_{i=1}^\\infty\n\\frac{1}{\\zeta (s) k^{s-1}} = \\infty.\\]"
  },
  {
    "objectID": "week4/week4.html#recap-well-defined-vs.-undefined-expectations",
    "href": "week4/week4.html#recap-well-defined-vs.-undefined-expectations",
    "title": "Week 4",
    "section": "Recap: Well-Defined vs. Undefined Expectations",
    "text": "Recap: Well-Defined vs. Undefined Expectations\nThe positive part of \\(X\\) is well-defined if either \\(E X^+ < \\infty\\) or \\(E X^- < \\infty\\).\nA random variable \\(X\\) has the Zeta(\\(s\\)) distribution for \\(s > 1\\), if it has pmf\n\\[f_X(k) = P(X = k) = \\frac{1}{\\zeta(s)k^s} \\mathbb 1 (k \\in \\mathbb \\{ 1, 2, ... \\})\\]\nwhere \\(\\mathbb \\zeta (s)\\) is the Riemann zeta function. The use of the Riemann zeta function may seem scary, but it’s really just acting as the normalizing constant here so that this is a proper pmf.\nSince \\(EX^- = E(-\\min(X,0)) = 0\\), \\(EX\\) is well-defined.\nRecall that \\(\\zeta(s) = \\sum_{i=1}^\\infty \\frac{1}{k^s}\\).\nHowever, if \\(s \\leq 2\\) then the mean is infinite:\n\\[EX = \\sum_{i=1}^\\infty k f_X(k) = \\sum_{i=1}^\\infty\n\\frac{1}{\\zeta (s) k^{s-1}} = \\infty.\\]\nNow suppose that \\(Y\\) is a discrete random variable with pmf\n\\[f_Y(k) = P(Y=k) = \\frac{1}{2ck^2} \\mathbb 1 (|k| \\in \\{ 1, 2, ... \\})\\]\nwhere \\(c = \\zeta(2)\\).\nThen \\(EY^+ = \\infty\\) and \\(EY^- = \\infty\\). For example,\n\\[EY^+ = \\sum_{k=0}^\\infty kP(Y^+ = k) = \\sum_{k=1}^\\infty \\frac{1}{2ck} = \\infty.\\]\nSo the mean of \\(Y\\) is not well-defined (or, undefined).\n\nCauchy Distribution Example\nA random variable \\(X\\) has the Cauchy(0,1) distribution if it has the pmf\n\\[f_X(x) = \\frac{1}{p} \\frac{1}{1+x^2}\\]\nfor \\(x \\in \\mathbb R\\). If \\(X \\sim \\text{Cauchy}(0,1)\\) then \\(EX\\) is undefined.\n\n\n\n\n\n\n\n\n\nThe Cauchy distribution has heavy tails, meaning it can take very large values with non-negligible probability."
  },
  {
    "objectID": "week4/week4.html#moments",
    "href": "week4/week4.html#moments",
    "title": "Week 4",
    "section": "Moments",
    "text": "Moments\nLet \\(k\\) be a positive integer. The \\(k\\)th moment of \\(X\\) is \\(E(X^k)\\). The \\(k\\)th central moment of \\(X\\) is \\(E((X-EX)^k)\\).\nThe variance of a random variable is the 2nd central moment:\n\\[\\text{Var}(X) \\stackrel{def}{=} E((X-EX)^2).\\]\n\\(\\text{Var}(X)\\) is sometimes denoted \\(\\sigma^2(X)\\) or simply \\(\\sigma^2\\).\nThe standard deviation of \\(X\\) is \\(\\sqrt{\\text{Var}(X)}\\).\nBoth the variance \\(\\sigma^2\\) and the standard deviation \\(\\sigma\\) quantify how spread out a distribution is. However, \\(\\sigma\\) is more interpretable since it is in the same units as \\(X\\).\n\nIf the mean is undefined for a distribution, the variance and standard deviaion will also be undefined, as in the Cauchy distribution. How might we quantify the spread of the distribution? One might use quantiles. Median absolute deviation. A really simple approach might be the difference between the 95th and 5th percentiles.\n\n\nProperties of Variance\n\nIf \\(\\text{Var}(X) < \\infty\\), then for any \\(a,b \\in \\mathbb R\\),\n\n\\[\\text{Var}(aX + b) = a^2 \\text{Var}(X).\\]\n\nA useful formula for the variance is \\(\\text{Var}(X) = EX^2 - (EX)^2\\).\nSuppose \\(Y\\) is an estimator of some quantity \\(y_0\\). Then the mean squared error is \\[mse = E(|Y - y_0|^2) = (EY - y_0)^2 + E((Y-EY)^2).\\]\n\n\\[ = \\text{bias}^2 + \\text{variance}\\]\n\nProof of 2. \\[\\text{Var}(X) = E((X - E X)^2)\\] \\[ = E(X X - X E X - E X X + (E X)^2)\\] \\[ = E X^2 - 2E XE X + (E X)^2\\] \\[ = E (X^2) - (E X)^2\\]\nProof of 1 using 2. Now apply the 2nd to the first question:\n\\[\\text{Var}(aX+b) = E((aX+b - E(aX+b))^2)\\]\nor \\[\\text{Var}(aX+b) = E((aX+b)^2) - E(aX+b)^2 \\] \\[ = E(a^2X^2+2abE X + b^2) - (aE X+b)^2 \\]\n\\[ =  (a^2E(X^2)+\\cancel{2abE X} + \\cancel{b^2}) - (a^2(E X)^2+\\cancel{2abE X}+\\cancel{b^2})\\] \\[ = a^2((E X^2)^2 - E(X)^2) \\] \\[ = a^2\\text{Var}(X)\\]\nProof of 1 using definitions. Using the 2nd central moment formula:\n\\[\\text{Var}(aX+b) = E((aX+b - E(aX+b))^2)\\] \\[ = E((aX+b - (aE X+b))^2)\\] \\[ = E((aX - (aE X)^2)\\] \\[ = a^2E((X - (E X)^2)\\] \\[ = a^2\\text{Var}(X)\\]\nProof of 3. Mean squared error is defined as \\[mse = E((Y - y_0)^2)\\]\nA nice trick is to add and subtract by the same thing.\n\\[ \\text{mse} = E((Y - EY + EY - y_0)^2)\\] \\[ = E((Y-EY)^2 + 2(Y-EY)(EY-y_0) + (EY - y_0)^2)\\] \\[ = E((Y-EY)^2) + 2\\underbrace{(EY-EY)}_{=0}(EY-y_0) + (EY - y_0)^2\\] \\[ = \\underbrace{E((Y-EY)^2)}_{\\text{variance}} + \\underbrace{(EY - y_0)^2}_{\\text{bias}^2}\\]\n\n\nA good illustration of the bias-variance tradeoff is in estimating the sample variance of normally distributed values.\nSuppose that \\(X_1, X_2, ... \\sim \\mathcal N(\\mu, \\sigma^2)\\).\n\\[\\bar x = \\frac{1}{n} \\sum_{i=1}^n X_i\\]\n\\[\\hat \\sigma^2 = \\frac{1}{n-1} \\sum_{i=1}^n (X_i-\\bar X)^2\\]\nThe above is unbiased, but it’s not the estimator with lowest mse. One can get a better estimator with lower mean-squared-error by using either \\(1/n\\) or \\(1/(n+1)\\). For more details on the \\(1/(n+1)\\) correction, look at page 351 in Casella and Berger.\nThe usual \\(1/(n-1)\\) correction is known as Bessel’s correction.\nEven further, suppose that \\(X_1, X_2, ... \\sim \\mathcal N(\\mu_i, \\sigma^2)\\).\nNaively, one would think that the best estimates for \\(\\hat \\mu_i\\) is just \\(X_i\\), but the James-Stein estimator/paradox shows that by decreasing the variance we can come up with estimators that have lower mean-squared-error.\n\nA common misperception is that bias is always bad. In fact, allowing some bias usually improves performance by reducing variance. This is especially important when building a prediction model. Less flexible models tend to have greater bias, since they cannot fit the distributions as closely. More flexible models tend to have greater variance, since they have more parameters to estimate. Since \\(\\text{mse} = \\text{bias}^2 + variance\\), there is a trade-off, and mse is minimized by setting the flexibility equal to some critical point."
  },
  {
    "objectID": "week4/week4.html#moment-generating-functions",
    "href": "week4/week4.html#moment-generating-functions",
    "title": "Week 4",
    "section": "Moment Generating Functions",
    "text": "Moment Generating Functions\nThe moment generating function (mgf) of a random variable \\(X\\) is\n\\[M_X(t) = E[e^{tX}]\\] for \\(t \\in \\mathbb{R}\\).\nThe mgf is said to exist if \\(M_X(t)\\) is finite in a neighborhood of zero. In other words, if there is some \\(h > 0\\) such that \\(M_X(t) < \\infty\\) whenever \\(|t| < h\\).\nThis terminology is a little weird since the function always exists but might be infinite.\nWhy is it called the “moment generating function”?\nFor all \\(k \\in \\{ 1, 2, 3, ... \\}\\),\n\\[EX^k = \\frac{d^k}{dt^k} M_X(t) \\lvert_{t=0.}\\]\nThat is, the \\(k\\)th moment of \\(X\\) equals the \\(k\\)th derivative of \\(M_X(t)\\) evaluated at \\(t=0\\).\nSo \\(M_X(t)\\) is a function from which one can “generate” the moments simply by differentiating and evaluating at \\(t=0\\).\n\nExponential Example\nIf \\(X \\sim \\text{Exponential}(\\lambda)\\), then for \\(|t| < \\lambda\\),\n\\[M_X(t) = E[e^{tx}] = \\int_0^\\infty \\exp(tx) \\lambda \\exp(-\\lambda x) dx\\] \\[ = \\lambda \\int_0^\\infty \\exp(-(\\lambda - t)x) dx\\]\n\\[= \\frac{\\lambda}{\\lambda - 1} \\int_0^\\infty (\\lambda - t)\\exp(-(\\lambda - t)x)dx\\]\nIn the last step, we multiplied and divided by \\(\\lambda - t\\) so that the inside is an exponential pdf with parameter \\(\\lambda - t)\\) (and thus has integral 1).\n\\[ = \\frac{\\lambda}{\\lambda - 1} < \\infty\\]\nfor \\(|t| < \\lambda.\\)\nWe can easily compute the moments of \\(X\\) using the mgf.\nWithout using the mgf, we’d have to use integration by parts to solve:\n\\[EX^k = \\int_0^\\infty x^k \\lambda e^{-\\lambda x} dx,\\] which could be a bit painful for larger \\(k\\).\nSo instead, using the mgf, we get that the 1st and 2nd moments are:\n\\[E X = \\frac{d}{dt} \\frac{\\lambda}{\\lambda - t} \\big\\lvert_{t=0} = \\frac{\\lambda}{(\\lambda - t)^2} \\big\\lvert_{t=0} = \\frac{1}{\\lambda}\\]\n\\[EX^2 = \\frac{d^2}{dt^2} \\frac{\\lambda}{\\lambda-t} \\big\\lvert_{t=0} = \\frac{d}{dt} \\frac{\\lambda}{(\\lambda - t)^2} \\big\\lvert_{t=0} = \\frac{2\\lambda(\\lambda- t)}{(\\lambda-t)^4} \\big\\lvert_{t=0} = \\frac{2}{\\lambda^2}. \\]\nThus the variance of \\(X\\) is\n\\[\\text{Var}(X) = EX^2 - (EX)^2 = \\frac{2}{\\lambda^2} - \\frac{1}{\\lambda^2} = \\frac{1}{\\lambda^2}.\\]\n\nCould it be that the moments still exist even if the mgf does not take on a finite value?\nRecall that \\[e^{tX} = \\sum_{k=0}^\\infty \\frac{(tX)^k}{k!} \\geq \\frac{t^kX^k}{k!} \\quad (X \\geq 0)\\]\nSo it might be that the moment generating function doesn’t exist while the moments themselves do exist.\nWe’ll get to the characteristic function soon:\n\\[\\phi_X(t) = E(e^{itX}).\\]\nAnd \\(|e^{itX}| = 1\\).\n\n\n\nUniqueness of Moments\n\\(X\\) has bounded support if \\(P(|X| < c) = 1\\) for some \\(c \\in \\mathbb{R}\\).\nSuppose \\(X\\) and \\(Y\\) have bounded support. Then \\(X \\stackrel{d}{=} Y\\) if and only if \\(EX^k = EY^k\\) for all \\(k \\in \\{ 1, 2, ... \\}\\).\nIf \\(M_X(t)\\) and \\(M_Y(t)\\) exist and are equal on a neighborhood of zero, then \\(X \\stackrel{d}{=} Y\\).\n\nThis does not hold in general for unbounded distributions. There’s such an example in Casella and Berger."
  },
  {
    "objectID": "week4/week4.html",
    "href": "week4/week4.html",
    "title": "Week 4",
    "section": "",
    "text": "Families of Distributions\nIn statistics, families of distributions play a key role. Many statistical methods are based on assuming that the data are distributed according to some family of distributions. Estimation and inference then proceeds by finding the parameters of the family that could plausibly have generated the observed data.\nFor instance, if one assumes that data \\(X_1, ..., X_n\\) are \\(\\mathcal N(\\mu, \\sigma^2)\\) distributed, then we could use maximum likelihood to estimate \\(\\mu\\) and \\(\\sigma^2\\) as:\n\\[\\hat \\mu = \\frac{1}{n} \\sigma_{i=1}^n X_i \\quad \\quad \\hat \\sigma^2 = \\frac{1}{n} \\sum_{i=1}^n (X_i - \\bar X)^2.\\]\nMany commonly used distributions have special properties that make them well-justified in particular applications.\nExamples:\nSelecting, combining, and/or transforming distributions according to the “physics” of the data generating process is important for good statistical modeling.\nWe often write the name of the distribution itself to denote the pdf/pmf. For instance, \\(\\text{Uniform(x|a,b)\\) denotes the pdf of \\(\\text{Uniform(a,b)}\\).\nIf the distribution \\(X\\) has been defined, say as \\(X \\sim \\text{Uniform}(a,b)\\), another shorthand is to write \\(p(x|a,b)\\) for the pdf/pmf.\nWe will often denote pdfs or pmfs as \\(p(\\cdot)\\) instead of \\(f(\\cdot)\\).\nWhen dealing with multiple r.v.s, say \\(X\\) and \\(Y\\), it is common to simply write \\(p(x)\\) and \\(p(y)\\) for the pdf/pmf of \\(X\\) and \\(Y\\), respectively, instead of \\(p_X(x)\\) and \\(p_Y(y)\\). In other words, the letters used (\\(x\\) or \\(y\\)) indicates which random variable we’re talking about."
  },
  {
    "objectID": "week4/week4.html#differentiating-under-the-integral-sign",
    "href": "week4/week4.html#differentiating-under-the-integral-sign",
    "title": "Week 4",
    "section": "Differentiating under the integral sign",
    "text": "Differentiating under the integral sign\nOften we want to interchange the order of differentiation and integration.\nFor example, for mgfs:\n\\[\\frac{d^k}{dt^k} M_X(t) \\lvert_{t=0} = \\frac{d^k}{dt^k} E\\big(\\exp(tX)\\big) \\lvert_{t=0}\\] \\[ = E\\big(\\frac{d^k}{dt^k} \\exp(tX)\\lvert_{t=0}\\big)\\] \\[ = E\\big( X^k \\exp(tX) \\lvert_{t=0}\\big)\\] \\[ = E(X^k).\\]\nThis is using the fact that \\(\\frac{d}{dt} e^{tx} = x e^{tx}\\), and hence \\(\\frac{d^k}{dt^x} e^{tx} = x^k e^{tx}\\).\nThe step where we swap the order of \\(\\frac{d^k}{dt^k}\\) and \\(E\\) is called differentiating under the integral sign.\nHowever, regularity conditions are needed for this to hold.\nSuppose that \\(f(x,t)\\) is differentiable with respect to \\(t\\) for each \\(x\\), and there exists a function \\(g(x,t)\\) such that\n\nfor all \\(x\\) and all \\(t'\\) in a neighborhood of \\(t\\) \\[\\left\\lvert \\frac{\\partial }{\\partial t} f(x,t) \\lvert_{t=t'} \\right\\rvert \\leq g(x,t)\\]\n\\(\\int_{-\\infty}^\\infty g(x,t) dx < \\infty\\). Then \\[\\frac{d}{dt} \\int_{-\\infty}^\\infty f(x,t) dx = \\int_{-\\infty}^\\infty \\frac{\\partial}{\\partial t} f(x,t) dx.\\]\n\nSee Casella & Berger (Theorem 2.4.3) for a slightly more general version. This proof uses one of the most important results in measure theory: the dominated convergence theorem.\nWe present a non-measure theoretic version of the dominated convergence theorem here, which is not fully general but gets the main idea across.\nSuppose that \\(f(x,t)\\) is continuous at \\(t_0\\) for each \\(x\\) and there exists \\(g(x)\\) such that\n\n\\(|f(x,t)| \\leq g(x)\\) for all \\(x\\) and all \\(t\\), and\n\\(\\int_{-\\infty}^\\infty g(x) dx < \\infty\\).\n\nThen \\[\\lim_{t\\to t_0} \\int_{-\\infty}^\\infty f(x,t) dx = \\int_{-\\infty}^{\\infty} \\lim_{t\\to t_0} f(x,t) dx.\\]\nThe dominated convergence theorem allows us to justify switching the order of limits and integrals.\nWe can think about the dominated convergence theorem as describing a situation where we have a sequence of functions:\n\\[f_1(x),\\, f_2(x),\\, f_3(x),\\, \\cdots\\]\nAnd what we’re saying is \\[\\lim_{n \\to \\infty} \\int f_n(x) dx = \\int \\left( \\lim_{n \\to \\infty} f_n(x))\\right) dx.\\]\n\n\n\n\n\n\n\n\n\n\nA counter-example would be \\(f_n(x) = 1/n\\), so \\(f_*(x) = 0\\), but \\(\\int f_n(x) dx > 0\\) for all \\(n\\).\nAnother counter-example is \\(f_n(x) = \\mathbb 1(n < x < n+1)\\). The limit \\(f_*(x) = 0\\) because for every \\(x\\), as \\(n\\to \\infty\\), there is an \\(N \\in \\mathbb N\\) such that for all \\(N' > N\\) \\(f_{N'}(x) = 0\\).\nThis is what the requirements around the existence of such a function \\(g(x)\\) are telling us (that \\(g(x)\\) is an envelope for all \\(f_n(x)\\) and \\(\\int_{-\\infty}^\\infty g(x) dx < \\infty\\)."
  },
  {
    "objectID": "week4/week4.html#independence-of-random-variables",
    "href": "week4/week4.html#independence-of-random-variables",
    "title": "Week 4",
    "section": "Independence of Random Variables",
    "text": "Independence of Random Variables\nRandom variables \\(X_1, ..., X_n\\) are independent if\n\\[P(X_1 \\in A_1, ..., X_n \\in A_n) = P(X_1 \\in A_1) \\cdots P(X_n \\in A_n)\\] for all measurable subsets \\(A_1, ..., A_n \\subset \\mathbb{R}\\).\nThe \\(\\text{Bernoulli}(q)\\) distribution is the special case of \\(\\text{Binomial}(N,q)\\) when \\(N=1\\).\nIf \\(X_1, ..., X_n \\sim \\text{Bernoulli}(q)\\) are independent, then\n\\(\\sum_{i=1}^n X_i \\sim \\text{Binomial}(N,q).\\)\nFrom this, it is easy to derive the mean of the Binomial distribution:\n\\[\\mathbb{E}\\left( \\sum_{i=1}^n X_i \\right) = \\sum_{i=1}^n \\mathbb{E}X_i = \\sum_{i=1}^n q = nq.\\]\n\nPoisson Distribution\nThe \\(\\text{Poisson}(\\lambda)\\) distribution has pmf\n\\[p(x|\\lambda) = e^{-\\lambda} \\frac{\\lambda^x}{x!} \\mathbb 1(x \\in \\mathcal X)\\]\nwhere \\(\\mathcal X = \\{ 0, 1, 2,...\\}\\). The parameter \\(\\lambda > 0\\) is referred to as the rate, for reasons that will become clear when we study Poisson processes.\nThe mean and variance of \\(X \\sim \\text{Poisson}(\\lambda)\\).\nThe Poisson model is often a good model for counting the occurrences of independent rare events.\nExamples:\n\nIn genomics, the number of reads covering a given locus is well-modeled as Poisson.\nIn physics, the number of photons hitting a detector during a given period of time is Poisson distributed.\nIn ecology, the number of organisms in a given region is often well-modeled as Poisson.\n\nThis is all due to a special property of the Poisson distribution jokingly referred to as the “law of small numbers”.\nThe Poisson is a limit of Binomials: if \\(q_N \\in (0,1)\\) is such that \\[N q_N \\to \\lambda\\] as \\(N \\to \\infty\\) for some \\(\\lambda > 0\\), then for all \\(x \\in \\{ 0, 1, 2, ...\\}\\),\n\\[\\text{Binomial}(x|N,q_N) \\longrightarrow \\text{Poisson}(x|\\lambda).\\]\n\n\nGeometric Distribution\nThe \\(\\text{Geometric}(q)\\) distribution has pmf\n\\[p(x|q) = (1-q)^{x-1}q \\mathbb 1(x \\in \\mathcal X).\\]\n\\[\\mathbb{E}X = 1/q\\]\n\\[\\text{Var}(X) = \\frac{1-q}{q^2}.\\]"
  }
]