[
  {
    "objectID": "week1/week1.html#il-famoso-smoking-ra-fisher",
    "href": "week1/week1.html#il-famoso-smoking-ra-fisher",
    "title": "2  Week 1",
    "section": "2.1 il famoso Smoking RA Fisher",
    "text": "2.1 il famoso Smoking RA Fisher\nWe’ll talk about a lot of the methods that Ronald A. Fisher developed. Already in the 1900s it was being observed that there was a strong association between smoking and lung cancer. However, Fisher was a smoker himself and posited that the association between lung cancer and smoking could be explained away by some genetic or biological difference between the smoking and non-smoking population (positing some genes that caused people to desire to smoke).\n\n\n\n\n\nRonald Fisher’s unsupported theory of genetics confounding the smoking-lung cancer relationship\n\n\n\n\nWe’re pretty sure that this was driven not by any substance matter expertise, but rather by Fisher’s love of smoking."
  },
  {
    "objectID": "week1/week1.html#hormone-replacement-therapy",
    "href": "week1/week1.html#hormone-replacement-therapy",
    "title": "2  Week 1",
    "section": "2.2 Hormone Replacement Therapy",
    "text": "2.2 Hormone Replacement Therapy\nIn the mid- to late- 20th century there were a ton of studies linking hormone replacement therapy for older women to better cardiovascular outcomes (lack of coronary heart disease).\nHowever, thankfully due to the Heart and Estrogen/Progestin Study (HERS, in the early 90s) we now know that a lot of those studies were not controlling for socioeconomic status. It turns out that socioeconomic status was highly associated with HRT usage, and associated at least in the US with a lot of better health outcomes across the board.\nIt turned out that HRT when applied at certain times for some people can actually be harmful — but the point is the picture is much muddier than was initially thought and recommendations were rolled back. Later randomized studies were performed that produced reliable bodies of evidence demonstrating either no effect or in some cases harmful effects.\nWe’ll use the baseline data from HERS (not so much interested in the HRT treatment effect), but to investigate the research question:\n\nHow is systolic blood pressure related to age, independently of other well-known cardiovascular risk factors? (Age, diabetes, smoking, etc.)"
  },
  {
    "objectID": "week1/week1.html#prediction-studies",
    "href": "week1/week1.html#prediction-studies",
    "title": "2  Week 1",
    "section": "2.3 Prediction Studies",
    "text": "2.3 Prediction Studies\nTypically in prediction settings, there’s no single exposure of particular interest; mechanisms and confounding is treated as less of a concern (if at all), and the main challenge is that we need to take care to not overfit the data.\nA major theme of this class will be that different tasks require different analysis strategies and diffrent statistical tools."
  },
  {
    "objectID": "week1/week1.html#quantifying-uncertainty",
    "href": "week1/week1.html#quantifying-uncertainty",
    "title": "2  Week 1",
    "section": "2.4 Quantifying Uncertainty",
    "text": "2.4 Quantifying Uncertainty\nTypically standard statistical models have nice theoretical properties because years-and-years ago, we didn’t have much data so people spent their time studying theory instead of data. As a result, we have a lot of nice theories about the uncertainty represented in statistical models.\nAn example of the kind of uncertainty we might be interested in is shown in this figure relating Alzheimer’s disease rates and exposure to PM2.5.\n\n\n\n\n\n\n\n\n\nThis figure is taken from the article Long-term effects of PM2·5 on neurological disorders in the American Medicare population: a longitudinal cohort study by Shi et al, Lancet Planetary Health (2020)."
  },
  {
    "objectID": "week1/week1.html#why-learn-methods-before-study-design",
    "href": "week1/week1.html#why-learn-methods-before-study-design",
    "title": "2  Week 1",
    "section": "2.5 Why Learn Methods Before Study Design",
    "text": "2.5 Why Learn Methods Before Study Design\nAn interesting point made is that it’s important to understand the limitations, strengths of methods, what they can and can’t do, and how to use them before designing a study."
  },
  {
    "objectID": "week1/week1.html#recommended-reading",
    "href": "week1/week1.html#recommended-reading",
    "title": "2  Week 1",
    "section": "2.6 Recommended Reading",
    "text": "2.6 Recommended Reading\nKutner M, Nachtsheim C, Neter J, Li W. Applied Linear Statistical Model. 5th edition. chapters 1-3\nShmueli, G. (2010). To explain or to predict? Statistical Science. https://www.stat.berkeley.edu/~aldous/157/Papers/shmueli.pdf"
  },
  {
    "objectID": "week1/week1.html",
    "href": "week1/week1.html",
    "title": "Week 1",
    "section": "",
    "text": "Introductory Overview\nJeffrey Miller’s office hours: Noon-1pm on Thursdays\nTA: Cathy Xue Office hours: 5:30-6:30pm on Tuesdays in 2-434 (Building 2, Room 434)"
  },
  {
    "objectID": "week1/week1.html#course-description",
    "href": "week1/week1.html#course-description",
    "title": "Week 1",
    "section": "Course description:",
    "text": "Course description:\n\nAxiomatic foundations of probability, independence, conditional probability, joint distributions, transformations, moment generating functions, characteristic functions, moment inequalities, sampling distributions, modes of convergence and their interrelationships, laws of large numbers, central limit theorem, and stochastic processes"
  },
  {
    "objectID": "week1/week1.html#course-readings",
    "href": "week1/week1.html#course-readings",
    "title": "Week 1",
    "section": "Course Readings:",
    "text": "Course Readings:\n\nStatistical Inference (Second Edition), by George Casella and Roger L. Berger. Cengage Learning, 2021.\nProbability: Theory and Examples (Fourth Edition), by Richard Durrett. Cambridge University Press, 2010. (https://services.math.duke.edu/~rtd/PTE/PTE5_011119.pdf)\nIntroduction to Stochastic Processes (Second Edition), by Gregory F. Lawler. Chapman & Hall/CRC, 2006.\n\nThe Durrett book is more measure-theoretic, but covers some things better (according to Miller) than Casella and Berger. Lawler’s book is a gentle introduction to stochastic processes."
  },
  {
    "objectID": "week1/week1.html#labs",
    "href": "week1/week1.html#labs",
    "title": "Week 1",
    "section": "Labs",
    "text": "Labs\nWeekly Tuesdays at 3:45-5:15 in FXB G10"
  },
  {
    "objectID": "week1/week1.html#outline-of-topics",
    "href": "week1/week1.html#outline-of-topics",
    "title": "Week 1",
    "section": "Outline of Topics",
    "text": "Outline of Topics\n\nFundamentals (CB 1.1 - 1.2.2)\n\nSet theory basics, Measure theory basics, Properties of probability measures\n\nProbability basics (CB 1.2.3 - 1.6)\n\nCombinatorics, Conditional probability and Independence, Random variables\n\nTransformations of random variables (CB 2.1)\n\nChange of variable formula for r.v.s, Probability integral transform\n\nExpectations of random variables CB 2.2 - 2.4)\n\nMean and variance, Moments, MGFs, Differentiation and limits of integrals\n\nFamilies of distributions (CB 3.1 - 3.5)\n\nDiscrete and continuous families, exponential families, location-scale families\n\nInequalities (CB 3.6, 3.8, 4.7)\n\nMarkov, Chebyshev, Gauss, Hölder, Cauchy-Schwarz, Minkowski, Jensen\n\nMultiple random variables (CB 4.1 - 4.6)\n\nRandom vectors, conditional distributions, independence, mixtures, covariance and correlation\n\nGaussian distributions (Bishop pp. 78-93, in Files/Reading on Canvas site)\n\nMultivariate normal, marginals and conditionals, linear-Gaussian model\n\nStatistics of a random sample (CB 5.1 - 5.4)\n\nSampling distributions, Sums of random variables, Student’s t and Snedecor’s F distribution, Order statistics and friends\n\nAsymptotics (CB 5.5)\n\nModes of convergence, Limit theorems, Delta method, Borel-Cantelli lemma\n\nLaws of large numbers (CB 5.5, D 2.2 - 2.4)\n\nWeak laws of large numbers, Strong laws of large numbers, Generalizations\n\nCentral limit theorems (CB 5.5, D 3.1 - 3.4)\n\nWeak convergence, characteristic functions, central limit theorems\n\nGenerating random samples (CB 5.6)\n\nInverse cdf method, accept/reject method, Markov chain Monte Carlo\n\nStochastics processes (L 1 - 3)\n\nMarkov chains, Random walks, Branching processes, Poisson processes"
  },
  {
    "objectID": "week1/week1.html#introduction",
    "href": "week1/week1.html#introduction",
    "title": "Week 1",
    "section": "Introduction",
    "text": "Introduction\nHow could we tell if either of the two sequences were faked.\n\nstr1 <- \"1 0 0 0 0 1 0 0 1 1 0 0 1 1 0 0 0 1 1 1 1 1 0 0 0 1 1 1 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 0 0 0 1 1 0 1 0 0 0 1 1 0 1 1 0 0 0 0 0 1 1 0 1 0 0 1 0 1 1 0 0 0 1 0 1 1 0 1 0 0 1 0 1 0 1 1 0 0 1 0 1\"\n\nstr2 <- \"1 1 0 0 1 0 0 0 1 1 1 0 1 0 1 0 0 0 1 0 1 1 1 0 1 0 0 0 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 0 0 0 1 0 1 1 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0 1 1 0 1 0 0 1 0 1 0 0 1 1 1 0 1 1 1 0 0 1 1 1 0 1 0 1 0 1 0 1 0\"\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nstr_count(str1, \"0|1\")\n\n[1] 100\n\nstr_count(str1, \"1\")\n\n[1] 43\n\nstr_count(str1, \"0\")\n\n[1] 57\n\nstr_count(str2, \"0|1\")\n\n[1] 100\n\nstr_count(str2, \"1\")\n\n[1] 53\n\nstr_count(str2, \"0\")\n\n[1] 47\n\nstr1_num <- as.numeric(unlist(stringr::str_split(str1, \" \")))\nstr2_num <- as.numeric(unlist(stringr::str_split(str2, \" \")))\n\n# the first way I proposed was to look at the probability of \n# the coin being \"fair\" given the beta distribution parameterized \n# by the observed coinflips \nx <- seq(0,1,0.01)\ncurve(dbeta(x, str_count(str1, \"0\")+1, str_count(str1, \"1\")+1))\n\n\n\ncurve(dbeta(x, str_count(str2, \"0\")+1, str_count(str2, \"1\")+1))\n\n\n\n# then we tried looking at the running mean\nplot(1:100, cummean(str1_num), type='l')\n\n\n\nplot(1:100, cummean(str2_num), type='l')\n\n\n\n# another classmate suggested that the human-generated \n# sequence may have more anti-correlation than the\n# real sequence because more anti-correlation \"looks\" more\n# random \ncor(lag(str1_num), str1_num, use = 'pairwise.complete.obs')\n\n[1] 0.007518797\n\ncor(lag(str2_num), str2_num, use = 'pairwise.complete.obs')\n\n[1] -0.2367884\n\n\nMiller suggests we could also look at it as a sequence of random variables.\n\nstr1_as_geometric_series <- sapply(unlist(stringr::str_split(stringr::str_remove_all(str1, \" \"), \"1\")), nchar)\nunname(str1_as_geometric_series)\n\n [1] 0 4 2 0 2 0 3 0 0 0 0 3 0 0 0 3 3 8 0 0 3 0 1 3 0 1 0 5 0 1 2 1 0 3 1 0 1 2\n[39] 1 1 0 2 1 0\n\nstr2_as_geometric_series <- sapply(unlist(stringr::str_split(stringr::str_remove_all(str2, \" \"), \"1\")), nchar)\nunname(str2_as_geometric_series)\n\n [1] 0 0 2 3 0 0 1 1 3 1 0 0 1 3 0 0 0 3 0 1 1 0 0 3 1 0 1 1 2 0 0 0 1 1 1 2 0 1\n[39] 2 1 2 0 0 1 0 0 2 0 0 1 1 1 1 1\n\ncurve(dgeom(x, prob = .5), from = 0, to = 10, n = 11)"
  },
  {
    "objectID": "week1/week1.html#history-of-probability",
    "href": "week1/week1.html#history-of-probability",
    "title": "Week 1",
    "section": "History of Probability",
    "text": "History of Probability\nGames of chance have been played for millenia. Early dice games were played with “astragali”, or “knucklebones”, from the ankle of a sheep or goat.\n\n\n\n\n\n\n\n\n\nEgyptian tomb paintings from 3500 BC show games played with astragali, and ancient Greek vases show young men tossing the bones into a circle. Gambling in these games was common, so it would have been advantageous to have some understanding of probability.\nInterest in gambling led mathematicians in the 1500-1600s to begin to formalize the rules of probability.\nTwo players put equal money in a pot. The first player to win 8 rounds of a game gets all the money. If they have to stop before finishing, how should the money be divided between them based on how much they would have won, on average?\nAround 1654, Blaise Pascal and Pierre de Fermat developed the concept of expected value to solve this problem.\nChristiaan Huuygens built upon this in his 1657 textbook on probability, “De Ratiociniis in Ludo Aleae” (“The Value of all Chances in Games of Fortune,”)\nIn the early 1700s, Jacob Bernoulli and Abraham De Moivre wrote foundational books on probability.\nThey systematically developed the mathematics of probability, focusing primarily on discrete problems.\nCombinatorial approaches were developed to handle difficult probability calculations.\nBernoulli proved the first version of the law of large numbers.\nIn 1812, Pierre-Simon Laplace published his book “Théorie analytique des probabilités”.\nLaplace developed or advanced many key methods and results in modern probability and statistics.\nGenerating functions, characteristic functions, linear regression, density functions, Bayesian inference, and hypothesis testing.\nHe employed advanced calculus and real/complex analysis,\ntaking probability calculations to a whole new level.\nLaplace proved the first general version of the central limit theorem.\nIn the 1930s, Andrey Kolmogorov introduced the measure theoretic foundations of modern probability.\nMeasure theory had recently been developed to resolve certain paradoxes that arose in defining volume and integration.\nKolmogorov applied measure theory to put probability on solid theoretical footing.\nThis is particularly important for limits and derivatives of integrals, conditional distributions, and stochastic processes."
  },
  {
    "objectID": "week1/week1.html#set-theory-basics",
    "href": "week1/week1.html#set-theory-basics",
    "title": "Week 1",
    "section": "Set Theory Basics",
    "text": "Set Theory Basics\nThe sample space denoted \\(S\\) is the set of possible outcomes of an experiment.\nExamples include \\(S = \\{ H, T \\}\\) for a coin toss, or math SAT scores \\(S = \\{ 200, 201, ..., 799, 800 \\}\\), or time-to-events: \\(S = (0, \\infty)\\).\nWe say that an event \\(E\\) is a subset of \\(S\\) that is \\(E \\subset S\\).\nA set \\(A\\) is a collection of elements. We say that \\[A \\cup B = \\{ x \\colon x \\in A \\text{ or } x \\in B \\}.\\]\n\\[A \\cap B = \\{ x \\colon x \\in A \\text{ and } x \\in B \\}.\\]\n\\[A^c = \\{ x \\in S \\colon x \\not \\in A \\}\\]\n\\[A \\backslash B = \\{ x \\colon x \\in A \\text{ and } x \\not \\in B \\}.\\]\nThe empty set is denoted \\(\\varnothing = \\{\\}\\).\n\\(A \\subset B\\) means that if \\(x \\in A\\) then \\(x \\in B\\).\n\nProperties of Set Operations\nCommutativity:\n\\[A \\cup B = B \\cup A, \\quad A \\cap B = B \\cap A\\]\nAssociativity:\n\\[\nA \\cup (B \\cup C) = (A \\cup B) \\cup C \\quad\nA \\cap (B \\cap C) = (A \\cap B) \\cap C\n\\]\nDistributive Laws:\n\\[ A \\cup (B \\cup C) = (A \\cap B) \\cup (A \\cap C)\n\\quad\nA \\cup (B \\cap C) = (A \\cup B) \\cap (A \\cup C)\\]\nDeMorgan’s Laws:\n\\[ (A \\cup B)^c = A^c \\cap B^c \\quad\n(A \\cap B)^c = A^c \\cup B^c \\]\n\n\nSigma Algebras\nDefinition. Suppose that \\(\\mathcal B\\) is a set of subsets of a sample space \\(S\\). Then \\(\\mathcal B\\) is a sigma-algebra if:\n\n\\(\\varnothing \\in \\mathcal B\\).\nif \\(A \\in \\mathcal B\\) then \\(A^c \\in \\mathcal B\\).\nif \\(A_1, A_2, ... \\in \\mathcal B\\) then \\(\\bigcup_{i=1}^\\infty A_i \\in \\mathcal B\\)\n\n\nThe powerset, denoted \\(2^S\\) is a specific example of a sigma algebra.\n\n\n\nWe have to be very careful that \\(\\varnothing\\) must be an element of \\(\\mathcal B\\) in order for \\(\\mathcal B\\) to be a sigma algebra. \\(\\varnothing\\) is certainly a subset of any set, but \\(\\varnothing\\) needs to be an element of \\(\\mathcal B\\) as a collection of sets.\n\nThe smallest sigma algebra, called the trivial sigma algebra, is \\(\\{ \\varnothing, S \\}\\) for a sample space \\(S\\). When \\(S\\) is uncountable, we usually don’t use the power set as a sigma algebra. Instead, we typically opt for using the Borel sigma algebra.\nFor a topological space \\(S\\), the Borel sigma algebra, denoted \\(\\mathcal B(S)\\) is the smallest sigma algebra containing all open sets.\nDefinition. Let \\(X\\) be a set and \\(\\tau\\) be a family of subsets of \\(X\\). Then \\(\\tau\\) is a topology and \\((X, \\tau)\\) is a topological space if\n\nBoth the empty set and \\(X\\) are elements in \\(\\tau\\).\nAny union of elements of \\(\\tau\\) is an element of \\(\\tau\\).\nAny intersection of finitely many elements of \\(\\tau\\) is an element of \\(\\tau\\).\n\nThe members of \\(\\tau\\) are called open sets in \\(X\\).\nWhen \\(A \\in \\mathcal B\\), we say that \\(A\\) is a measurable set.\n\n\nProbability Measures\nDefinition. If \\((S, \\mathcal B)\\) is a measurable space, then \\(P: \\mathcal B \\to \\mathbb R\\) is a probability measure if:\n\n\\(P(A) \\geq 0\\) for all \\(A \\in \\mathcal B\\). (non-negativity)\n\\(P(S) = 1\\) (unitarity)\nif \\(A_1, A_2, ... \\in \\mathcal B\\) are pairwise disjoint, then \\[P\\left(\\bigcup_{i=1}^\\infty A_i \\right) =\n  \\sum_{i=1}^\\infty P(A_i).\\] (countable additivity)\n\nThese properties are called the axioms of probability, or sometimes Kolmogorov’s axioms.\nIf \\(A \\in \\mathcal B\\) we call \\(A\\) a measurable set.\nIn this course, we may assume that the sets we are working with are measurable. Almost exclusively we will be working with the Borel sigma algebra. While non-measurable sets do exist in this setting, they do not often arise in practice.\n\nTJ asks: “I know it’s possible to demonstrate non-measureable sets non-constructively, but is it possible to demonstrate them constructively?”\nMiller: “I think you have to use infinite series/sets [and the axiom of choice].”"
  },
  {
    "objectID": "week1/week1.html#probability-measure-on-a-countable-set",
    "href": "week1/week1.html#probability-measure-on-a-countable-set",
    "title": "Week 1",
    "section": "Probability measure on a countable set",
    "text": "Probability measure on a countable set\nSuppose that \\(S = \\{ s_1, s_2, ... \\}\\) is a countable set.\nLet \\(p_1, p_2, ... \\geq 0\\) such that \\(\\sum_{i=1}^\\infty p_i = 1\\).\nFor \\(A \\subset S\\), define \\[P(A) = \\sum_{i=1}^\\infty p_i \\mathbb 1(i \\in A).\\]\nWe will write that \\(\\mathbb 1(\\cdot)\\) to denote the indicator function, where\n\\[\\mathbb 1(C) = \\left\\{ \\begin{array}{ll}\n1 & \\text{ if condition } C \\text{ is true} \\\\\n0 & \\text{ if condition } C \\text{ is false}\n\\end{array}\n\\right.\\]\nSuppose you toss a fair coin until you get heads. Then \\(p_k\\), the probability that you toss the coin \\(k\\) times, is \\((1/2)^k\\). This defines a probability measure on \\(S = \\{ 1, 2, ... \\}\\).\nWe could imagine measuring how long a lightbulb lasts until it dies after being left on. It could die at any non-negative time \\(t \\geq 0\\). The probability \\(P([0,t))\\) be the probability the lightbulb dies before time \\(t\\). This defines a probability measure on \\(S = [0,\\infty).\\) (Of course, \\([0,\\infty)\\) is a continuous example and not a countable set).\nFor any probability measure, we have that:\n\n\\(P(A^c) = 1 - P(A)\\)\n\\(P(\\varnothing) = 0\\)\n\\(P(A) \\leq 1\\)\nif \\(A \\subset B\\) then \\(P(A) \\leq P(B)\\)\n\\(P(A) = P(A \\cap B) + P(A \\cap B^c)\\)\n\\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\).\n\nProofs:\n\n\\(P(S) = 1\\), and \\(A, A^c\\) are pairwise disjoint and a partition of \\(S\\), so \\(1 = P(S) = P(A^c \\cup A) = P(A) + P(A^c)\\). Subtracting from both sides, \\(P(A) = 1 - P(A^c)\\).\n\\(\\varnothing\\) and \\(S\\) are disjoint, so \\(1 = P(S) = P(S) + P(\\varnothing)\\). Subtracting from both sides, we have that \\(1 - 1 = P(\\varnothing)\\)\nWe have from 1 that \\(1 - P(A^c) = P(A)\\), and \\(P(A^c) \\geq 0\\), so then \\(P(A) \\leq 1\\)\nWe can write that \\(B \\backslash A\\) and \\(A\\) as disjoint sets since \\(A \\subset B\\). Then \\(P(B) = P(A \\cup B \\backslash A) = P(A) + P(B \\backslash A)\\). Since \\(P(B \\backslash A)\\) we have that \\(P(B) \\geq P(A)\\).\nWe need to show that \\(A = (A \\cap B) \\cup (A \\cup B^c)\\). If \\(a \\in A\\) then either \\(a \\in B\\) or \\(a \\in B^c\\), but not both by the definition of complement. Therefore \\(A \\cap B\\) and \\(A \\cap B^c\\) are disjoint and their union is equal to \\(A\\). Hence \\(P(A) = P(A \\cap B \\bigcup A \\cap B^c) = P(A \\cap B) + P(A \\cap B^c)\\).\nFirst, note that \\(A \\cup B = A \\cap (B \\backslash A)\\). Thus \\(P(A \\cup B) = P(A \\cup (B \\backslash A))\\). Since the latter are disjoint, we establish that \\(P(A \\cup B) = P(A) + P(B \\backslash A)\\). Now if we consider that \\(B = (B \\backslash A) \\cup (A \\cap B)\\), and that these are disjoint sets, we have that \\(P(B) = P(B \\backslash A) + P(A \\cap B)\\). Rearranging, we have that \\(P(B \\backslash A) = P(B) - P(A \\cap B)\\). Substituting, now we have that \\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\) as desired."
  },
  {
    "objectID": "week1/week1.html#properties-of-probability-measures",
    "href": "week1/week1.html#properties-of-probability-measures",
    "title": "Week 1",
    "section": "Properties of Probability Measures",
    "text": "Properties of Probability Measures\nLaw of total probability: for any partition \\(B_1, B_2, ...\\) of \\(S\\), we have that \\[P(A) = \\sum P(A \\cap B_i)\\]\nBoole’s inequality (aka union bound): For \\(A_1, A_2, ...\\),\n\\[P(\\bigcup_{i=1}^\\infty A_i) \\leq \\sum_{i=1}^\\infty P(A_i).\\]\nBonferroni’s inequality: For any \\(A_1, A_2,...\\),\n\\[P\\left(\\bigcap_{i=1}^\\infty A_i \\right) \\geq 1 - \\sum_{i=1}^\\infty P(A_i^c).\\]\nBoole’s inequality is often useful when we want to show that some event has probability near zero. For example, \\(P(E) = P(A_1 \\cup A_2 \\cup A_3) \\leq P(A_1) + P(A_2) + P(A_3) \\leq 3\\epsilon\\)."
  },
  {
    "objectID": "week1/week1.html#selecting-k-items-from-n-options",
    "href": "week1/week1.html#selecting-k-items-from-n-options",
    "title": "Week 1",
    "section": "Selecting \\(k\\) items from \\(n\\) options",
    "text": "Selecting \\(k\\) items from \\(n\\) options\n\n\n\n\nwithout replacement\nwith replacement\n\n\n\n\nordered\n\\(\\frac{n!}{(n-k)!}\\)\n\\(n^k\\)\n\n\nunordered\n\\({n \\choose k}\\)\n\\({ n + k - 1 \\choose k }\\)"
  },
  {
    "objectID": "week1/week1.html#determining-the-leading-factor-in-stirlings-formula",
    "href": "week1/week1.html#determining-the-leading-factor-in-stirlings-formula",
    "title": "Week 1",
    "section": "Determining the Leading Factor in Stirling’s Formula",
    "text": "Determining the Leading Factor in Stirling’s Formula\nStirling’s formula is that for large values of \\(n\\), the following is a good approximation for the factorial function:\n\\[ n! \\approx \\frac{n^n}{e^n} \\sqrt{2 \\pi n}. \\]\nI’ll concern myself with, as an exercise, showing the leading factor \\(n^n e^{-n}\\) is correct.\nFirst, observe the relationship between \\(n!\\) and \\(n^n\\):\n\\[n! = \\underbrace{n \\cdot (n-1) \\cdots 1}_{n \\text{ terms}} < \\underbrace{n \\cdot n \\cdots n}_{n \\text{ times}} = n^n\\]"
  },
  {
    "objectID": "week2/week2.html",
    "href": "week2/week2.html",
    "title": "Week 2",
    "section": "",
    "text": "Conditional Probability and Independence\nIf we go back to the formula for sampling with replacement, the \\(-1\\) term comes from the fact that when converting from the number of bins we could put balls into to the number of dividers between the bins, there’s one less divider than there are bins.\nThat formula was \\[ {n + k - 1 \\choose k} \\] so we could think about how to put \\(k\\) balls into \\(n\\) bins, or instead \\(k\\) balls into \\(n-1\\) dividers, or we could think about there being \\(n-1\\) blue balls and \\(k\\) red balls and the blue balls represent the dividers, so now we’re describing choosing \\(k\\) balls to be red out of \\(n + k - 1\\) balls.\nWe have only been so far considering events and outcomes in the sample space \\(S\\). Often it’s most useful to work with functions of the outcome.\nFor instance, suppose a coin is tossed \\(N\\) times.\nA natural definition of the sample space would be \\(S = \\{ 0, 1 \\}^N\\), that is all sequences of \\(N\\) zeroes or ones.\nDefine \\(X\\) to be the number of times that heads comes up.\nIf we only want to evaluate whether the coin is biased, we may as well work with \\(X\\) rather than the whole sequence.\n\\(X\\) can be thought of as a function from the sample space \\(S\\) to the set of integers.\nDefinition. A random variable \\(X\\) is a function from the sample space equipped with sigma algebra \\(\\Omega\\) to the real numbers \\(X: S \\to \\mathbb R\\). Technically it must be a measurable function, that is \\(X^{-1}(A)\\) must be a measurable set for all measurable sets \\(A \\in \\mathcal B(\\mathbb R)\\). But we won’t worry about this so much in this course.\nIn other words, when the outcome is \\(s \\in S\\), the random variable takes the value \\(X(s)\\) which is some real number.\nThe probability that \\(X\\) takes value \\(x\\), denoted \\(P(X=x)\\), is \\[P(X=x) = P(\\{s \\in S \\colon X(s) = x \\})\\]\nIn the coin tossing example, if \\(s = (s_1, ..., s_N) \\in S = \\{0,1\\}^N\\), then the number of heads \\(X(s) = \\sum_{i=1}^N s_i\\).\nIf the probability of heads is \\(q \\in (0,1)\\), then\n\\[P(\\{s\\}) = \\prod_{i=1}^N q^{s_i}(1-q)^{1-s_i} = q^x (1-q)^{N-x}\\]\nwhere \\(x = \\sum_{i=1}^N s_i\\). Let \\(X(s) = \\sum_{i=1}^N s_i\\).\nThe probability of getting heads \\(x\\) times in \\(N\\) coin tosses is\n\\[P(X=x) = P(\\{ s \\in S \\colon X(s) = x \\}) = \\sum_{s \\in S} P(\\{ s\\})\n\\mathbb 1(X(s) = x)\\] \\[ = \\sum_{s \\in S} q^x (1-q)^{N-x} \\mathbb 1 (X(s)=x)\\] \\[ {N \\choose x} q^x (1-q)^{N-x}\\]\n\\(X\\) is said to follow the binomial distribution with parameters \\(N\\) and \\(q\\). This is denoted by writing \\(X \\sim \\text{Binomial}(N,q)\\).\nWe often think of \\(X\\) as a random quantity, but formally it is a function.\nExercise 1. Prove a generalized Bonferroni inequality:\n\\[ P \\left( \\cap_{i=1}^n A_i \\right) \\geq \\sum_{i=1}^n P(A_i) - (n-1) \\quad \\text{ for any events } A_1, ..., A_n.\\]\nProof 1. We proceed by induction. The base case is clear: \\(P(A_1) \\geq P(A_1) - (1-1)\\).\nAssume an inductive hypothesis: \\[P(\\cap_{i=1}^n A_i) \\geq \\sum_{i=1}^n P(A_i) - (n-1).\\]\nNow we can write that \\[P\\left(\\bigcap_{i=1}^{n+1} A_i \\right) = P\\left(A_{n+1} \\cap \\bigcap_{i=1}^n A_i \\right).\\]\nRecall Boole’s inequality: \\(P(B \\cap A^c) = P(B) - P(A \\cap B)\\), and then let \\(B = \\cap_{i=1}^n A_i\\) and \\(A^c = A_{n+1}\\). Then \\[\\begin{aligned}\nP\\left(A_{n+1} \\cap \\bigcap_{i=1}^n A_i \\right) & =\nP\\left(\\bigcap_{i=1}^n A_i\\right) - P\\left(A^c_{n+1} \\cap \\bigcap_{i=1}^n A_i\\right) \\\\\n& \\geq \\sum_{i=1}^n P(A_i) - (n-1) - P\\left(A^c_{n+1} \\cap \\bigcap_{i=1}^n A_i \\right). \\quad (\\star)\n\\end{aligned}\\]\nNow let’s apply Boole’s inequality again to the right-most probability, this time letting \\(B = A^c_{n+1}\\) and \\(A^c = \\bigcap_{i=1}^n A_i\\).\n\\[\nP\\left(A^c_{n+1} \\cap \\bigcap_{i=1}^n A_i \\right) =\nP(A^c_{n+1}) - P\\left(A^c_{n+1} \\cap \\left( \\bigcap_{i=1}^n A_i\\right)^c \\right).\n\\]\nApplying the fact that \\(P(A) = 1-P(A^c)\\) for any set \\(A\\), we then have that \\[P(A^c_{n+1}) = 1 - P(A_{n+1}) \\quad (\\star) \\]\nSubstituting that back in, we have that:\n\\[\nP\\left(A^c_{n+1} \\cap \\bigcap_{i=1}^n A_i \\right) =\n1 - P(A_{n+1}) - P\\left(A^c_{n+1} \\cap \\left( \\bigcap_{i=1}^n A_i\\right)^c \\right).\n\\]\nAnd then substituting that back into \\((\\star)\\):\n\\[P\\left( \\bigcap_{i=1}^{n+1} A_i \\right) \\geq\n\\sum_{i=1}^n P(A_i) - (n-1) -\n\\left[ 1 - P(A_{n+1}) - P\\left(A^c_{n+1} \\cap \\left( \\bigcap_{i=1}^n A_i\\right)^c \\right) \\right]\n\\] \\[\n= \\sum_{i=1}^n P(A_i) - (n-1) -\n1 + P(A_{n+1}) + P\\left(A^c_{n+1} \\cap \\left( \\bigcap_{i=1}^n A_i\\right)^c \\right)\n\\] \\[\n= \\sum_{i=1}^{n+1} P(A_i) - (n+1-1) + \\underbrace{P\\left(A^c_{n+1} \\cap \\left( \\bigcap_{i=1}^n A_i\\right)^c \\right)}_{\\geq 0 \\, \\text{ by the axioms of probability}}\n\\] \\[\n\\geq \\sum_{i=1}^{n+1} P(A_i) - (n+1-1)\n\\]\nThis concludes the proof.\nProof 2. We proceed by proof by contradiction: Assume that \\[P\\left( \\bigcap_{i=1}^n A_i \\right) < \\sum_{i=1}^n P(A_i) - (n-1)\\] \\[P\\left( \\bigcap_{i=1}^n A_i \\right) < 1 - n + \\sum_{i=1}^n P(A_i)\\] \\[P\\left( \\bigcap_{i=1}^n A_i \\right) < 1 - n + \\sum_{i=1}^n 1 - P(A_i^c)\\] \\[P\\left( \\bigcap_{i=1}^n A_i \\right) < 1 - n + n - \\sum_{i=1}^n P(A_i^c)\\] \\[P\\left( \\bigcap_{i=1}^n A_i \\right) < 1 - \\sum_{i=1}^n P(A_i^c)\\]\nBut this contradicts (the un-generalized version of) Bonferroni’s inequality. \\(\\rightarrow \\leftarrow\\). This concludes the proof.\nExercise 2. Given \\(B \\subset C\\) and \\(P(B) > 0\\), prove that \\[P(A | C) > P(A | B) \\Longleftrightarrow P(A | C \\cap B^c) > P(A | B).\\]\nExercise 3. Prove or disprove the following statement: For events \\(A\\) and \\(B\\) such that \\(0 < P(A) < 1\\) and \\(0 < P(B) < 1\\), \\[P(A|B) > P(A) \\Longleftrightarrow P(B | A) > P(B|A^c).\\]\n1-2pm Office Hours for Miller, Building 1 Room 245"
  },
  {
    "objectID": "week2/week2.html#jackpot-scenario",
    "href": "week2/week2.html#jackpot-scenario",
    "title": "Week 2",
    "section": "Jackpot Scenario",
    "text": "Jackpot Scenario\nWhen the pot reached $5M, a “rolldown” occurred in which the prizes for matching 3, 4, or 5 numbers were 10x higher.\n\n\n\nMatch Number\n6\n5\n4\n\n\n\n\nProbability\n1/13,983,815\n1/54,201\n1/1032\n\n\nAll prizes (in 2 years)\n15\n2158\n117685\n\n\nPrize (normal)\nJackpot\n$2,500\n$100\n\n\nPrize in Fall\nJackpot not hit\n$25,000\n$1,000\n\n\n\nIf there were a rolldown in the WINfall lottery, we’d have an expected return on a single ticket as:\n\n(1/54201)*25000 + (1/1032)*1000 + (1/57)*(50)\n\n[1] 2.307431"
  },
  {
    "objectID": "week2/week2.html#hypergeometric-distribution",
    "href": "week2/week2.html#hypergeometric-distribution",
    "title": "Week 2",
    "section": "Hypergeometric Distribution",
    "text": "Hypergeometric Distribution\nThe Hypergeometric\\((N,K,n)\\) distribution gives the probability of matching \\(k\\) numbers from a set of \\(K\\) winning numbers when selecting \\(n\\) from a set of \\(N\\) total.\nWe often say distribution instead of probability measure.\nIn the Winfall lottery, \\(N = 49\\), \\(K=6\\), and \\(n=6\\), and the outcome is the number of matches \\(k\\).\nLetting \\(P\\) denote the Hypergeometric\\((N,K,n)\\) distribution, \\[P(\\{k\\}) = \\frac{{K \\choose k}{N - K \\choose n - k}}{N \\choose n}.\\]\nPlugging \\(k=3, k=4, k=5, k=6\\) into this formula yields the probabilities in the above table."
  },
  {
    "objectID": "week2/week2.html#playing-cards",
    "href": "week2/week2.html#playing-cards",
    "title": "Week 2",
    "section": "Playing Cards",
    "text": "Playing Cards\nWhat’s the probability of drawing 4 cards from a deck and getting 4 aces?\nThe number of possible hands of 4 cards is \\({52 \\choose 4}\\).\nThus the probability of getting all 4 aces is\n\\[\\frac{1}{52 \\choose 4} = \\frac{4!48!}{52!}.\\]\nOr we could think about it sequentially: The probability of drawing an ace on the first card is \\(4/52\\), the next is \\(3/51\\), then \\(2/50\\), and \\(1/49\\) each conditioned on assuming we previously drew an ace card.\n\n\\[ \\frac{4}{52} \\times \\frac{3}{51} \\times \\frac{2}{50} \\times \\frac{1}{49} = \\frac{4!48!}{52!} = \\frac{4! \\cancel{48 \\cdot 47 \\cdots 1}}{52 \\cdot 51 \\cdot 50 \\cdot 49 \\cdot \\cancel{48 \\cdot 47 \\cdots 1}} \\]\nDefinition. The conditional probability of \\(A\\) given \\(B\\) denoted \\(P(A|B)\\) is\n\\[ P(A|B) = \\frac{P(A \\cap B)}{P(B)}\\]\nWe could think of \\(B\\) as the event where 1 ace has already been drawn (and thus represents the scenario where \\(B\\) consists of all the cards except 1 ace). Then,\n\\[P(A | B) =\n\\frac{P(\\{ \\text{Ace} \\clubsuit, \\text{Ace} \\diamondsuit, \\text{Ace} \\spadesuit, \\text{Ace} \\heartsuit\\} \\cap B\\})}{P(B)} = \\frac{3}{51} \\]"
  },
  {
    "objectID": "week2/week2.html#bayes-rule",
    "href": "week2/week2.html#bayes-rule",
    "title": "Week 2",
    "section": "Bayes’ Rule",
    "text": "Bayes’ Rule\n\\[P(A|B) = \\frac{P(A \\cap B)}{P(B)}.\\]\nMultiplying by \\(P(B)\\) yields:\n\\[P(A \\cap B) = P(A|B) P(B).\\]\nBy symmetry, \\[P(A \\cap B) = P(B|A) P(A).\\]\nIf \\(P(A) > 0\\) and \\(P(B) > 0\\), then \\[P(A|B) = \\frac{P(B|A) P(A)}{P(B)}.\\]\nIf \\(A_1, A_2, ...\\) form a partition of the sample space then\n\\[P(A_i |B) = \\frac{P(B|A_i) P(A_i)}{\\sum_{j=1}^\\infty P(B|A_j) P(A_j)}.\\]"
  },
  {
    "objectID": "week2/week2.html#the-monty-hall-problem",
    "href": "week2/week2.html#the-monty-hall-problem",
    "title": "Week 2",
    "section": "The Monty Hall Problem",
    "text": "The Monty Hall Problem\nOn a game show, there are 3 doors. Behind one door is a car, and behind the other two doors are goats. You win whatever is behind the door you select. First you pick door #1. The game show host then reveals that there is a goat behind door #3. The host then asks “Do you want to stay with #1 or switch to #2?” What would you do and why?\nIt turns out you should always switch. Let \\(D_1, D_2, D_3\\) denote the events that the car is behind door 1, 2, or 3. Let \\(M_1, M_2, M_3\\) denote the event that Monty opens door 1, 2, or 3, respectively.\nWe will assume that there’s equal probability of the car being behind 1, 2, or 3. Additionally, we will assume that he always opens a door that is not the one you picked and which does not have the car behind it.\nBy assumption \\[P(D_1) = P(D_2) = P(D_3) = 1/3.\\]\n\\[P(M_j|D_1) = (1/2) \\mathbb 1 (j \\in \\{2,3\\})\\] \\[P(M_j|D_2) = \\mathbb 1 (j = 3)\\] \\[P(M_j|D_3) = \\mathbb 1 (j = 2)\\]\nHere’s a table of the probability of each possible combination \\(i,j\\) of door that the car is behind \\((i)\\) and the door opened by Monty \\((j)\\):\n\n\n\n\nOpen #1\nOpen #2\nOpen #3\n\n\n\n\nCar in #1\n0\n1/6\n1/6\n\n\nCar in #2\n0\n0\n1/3\n\n\nCar in #3\n0\n1/3\n0\n\n\n\nBy the law of total probability, the probability that Monty opens door #3 is\n\\[P(M_3) = \\sum_{i=1}^3 P(M_3|D_i) P(D_i) = \\frac{1}{2} \\times \\frac{1}{3} +\n1 \\times \\frac{1}{3} + 0 \\times \\frac{1}{3} = \\frac{1}{2}.\\]\nSo by Bayes’ rule the conditional probability of the car being behind door #1, given Monty opened door #3, is\n\\[P(D_1|M_3) = \\frac{P(M_3|D_1) P(D_1)}{P(M_3)} = \\frac{(1/2) \\times (1/3)}{1/2} = \\frac{1}{3}\\]\nMeanwhile, the conditional probability of the car being behind door #2 given that Monty opened door #3 is\n\\[P(D_2 | M_3) = \\frac{P(M_3|D_2) P(D_2)}{P(M_2)} = \\frac{1 \\times (1/3)}{(1/2)} = \\frac{2}{3}\\]\n\nWhat’s wrong with the following reasoning?\nWe can think of the sample space as \\(\\{1,2,3\\}\\) and the outcome as the number that the door is behind, so \\(D_i = \\{i\\}.\\)\nBy conditioning on Monty opening door #3, we are conditioning on the event that the car is behind #1 or #2, that is \\(M_3 = D_3^c = \\{1,2\\}\\). Therefore\n\\[ P(D_1 | M_3) = \\frac{P(D_1 \\cap M_3)}{P(M_3)} \\] \\[ = \\frac{P(\\{1\\} \\cap \\{1,2\\})}{P(\\{1,2\\})} \\] \\[ = \\frac{1/3}{2/3} = \\frac{1}{2}.\\]\nFurther, \\(P(D_2|M_3) = 1 - P(D_1 | M_3) = 1/2\\) so we gain nothing by switching to door #2.\nThe problem is that this doesn’t condition on the fact that Monty will never choose the door with the car. In other words, the door Monty picks depends on your choice: when your door contains a goat, Monty only has one choice: the remaining door with a goat.\nIn essence, we need to define the joint-distribution of \\(D_i\\) and \\(M_j\\).\n\nThe intuition is that the probability gets squished into the other doors, so you get probabilities of \\(1/3\\) and \\(2/3\\)."
  },
  {
    "objectID": "week2/week2.html#independence",
    "href": "week2/week2.html#independence",
    "title": "Week 2",
    "section": "Independence",
    "text": "Independence\nIf we had a coin and we flipped it multiple times, where \\(A\\) represents the event where it comes up heads the first time and \\(B\\) is the event where it comes up heads the second time.\nWe would assume that \\(P(B|A) = P(B)\\).\nBy Bayes’ theorem, \\(P(B|A) = P(A \\cap B)/P(A)\\), that implies that \\(P(A \\cap B) = P(A)P(B)\\). When this holds, we say that \\(A\\) and \\(B\\) are independent.\nOne difference is that the second statement doesn’t require that \\(P(A)\\) is nonzero, but the definition using \\(P(B|A)\\) does.\nIf \\(A\\) and \\(B\\) are independent, then so are \\(A\\) and \\(B^c\\), \\(A^c\\) and \\(B\\), and \\(A^c\\) and \\(B^c\\).\nEvents \\(A_1, ..., A_n\\) are mutually independent if \\[P(\\cap_{i \\in I} A_i) = \\prod_{i \\in I} P(A_i)\\] for every subset \\(I \\subset \\{ 1, ..., n \\}.\\)\nYou might think that \\(P(A_1 \\cap \\cdots \\cap A_n) = P(A_1) \\cdots P(A_n)\\) would be a simpler definition of independence of multiple events, but this is not correct. See Casella & Berger (1.3.10)."
  },
  {
    "objectID": "week2/week2.html#continuing-on-random-variables",
    "href": "week2/week2.html#continuing-on-random-variables",
    "title": "Week 2",
    "section": "Continuing on Random Variables",
    "text": "Continuing on Random Variables\nIf the range of a random variable \\(\\{X(s) : s \\in S \\}\\) is countable then \\(X\\) is a discrete random variable.\nFor a discreet random variable, the function \\(f(x) = P(X = x)\\) is called a probability mass function.\nRecall how we define probability on random variables: \\[P(X = x)  = P(\\{ s \\in S : X(s) = x \\})\\]"
  },
  {
    "objectID": "week2/week2.html#cumulative-distribution-functions",
    "href": "week2/week2.html#cumulative-distribution-functions",
    "title": "Week 2",
    "section": "Cumulative Distribution Functions",
    "text": "Cumulative Distribution Functions\nThe cumulative distribution function (cdf) of \\(X\\) is defined to be \\[F(x) = P(X \\leq x).\\]\nDefinition. A function \\(F \\colon \\mathbb R \\to \\mathbb R\\) is a cdf if and only if\n\n\\(\\lim_{x \\to -\\infty} F(x) = 0\\) and \\(\\lim_{x \\to \\infty} F(x) = 1\\)\n\\(F(x)\\) is non-decreasing.\n\\(F(x)\\) is right-continuous, that is for all \\(x' \\in \\mathbb R\\) \\[ \\lim_{x \\downarrow x'} F(x) = F(x')\\]\n\nHere’s an example for a binomial distribution:\n\n\n\nBinomial distribution CDF\n\n\nFor any discrete probability distribution, the cdf will necessarily be discontinuous.\nAn example of a continuous cdf is \\[F(x) = (1- e^{-\\lambda x}) \\mathbb 1 (x > 0),\\] where \\(\\lambda\\) is a positive fixed number.\n\n\n\nExponential distribution CDF\n\n\n\nContinuous Random Variables\nA random variable is continuous if its cdf is a continuous function.\nIn other words, if \\(\\lim_{x \\to x'} F(x) = F(x')\\) for every \\(x' \\in \\mathbb R\\).\nA probability density function of a continuous random variable function \\(f: \\mathbb R \\to [0,\\infty)\\) such that for all \\(x \\in \\mathbb R\\), \\[F(x) = \\int_{-\\infty}^\\infty f(t) dt.\\]\nTherefore we need to integrate to get probabilities:\n\\[P(X \\in A) = \\int_A f(x) dx\\]"
  },
  {
    "objectID": "week2/week2.html#relationship-between-cdf-and-pdf",
    "href": "week2/week2.html#relationship-between-cdf-and-pdf",
    "title": "Week 2",
    "section": "Relationship between CDF and PDF",
    "text": "Relationship between CDF and PDF\n“Randavble” = “Random Variable” (Nice abbreviation, but a joke)\nSuppose we had a cdf \\(F\\), we could derive a pdf \\(f\\) by differentiating (usually).\nSimilarly, we can obtain \\(F\\) from \\(f\\).\nIf \\(f(x)\\) is continuous at \\(x\\), then \\[f(x) = \\frac{d}{dx} F(x),\\] by the fundamental theorem of calculus.\nA function \\(f: \\mathbb R \\to [0,\\infty)\\) is a probability density function if and only if \\(\\int f(x) dx = 1\\).\nIn fact any integrable non-negative function can be normalized to a pdf, by performing \\(f(x) = \\tilde{f}(x)/C\\) for a constant \\(C\\).\n\n\n\nNormalizing a pdf to have integral 1\n\n\nThere is not a unique pdf for a given distribution since the pdf can change arbitrarily on sets with Lebesgue measure 0. However, any two pdfs for the same distribution will agree “almost everywhere.”\nNot every continuous distribution has a pdf, since there are continuous cdfs that are nondifferentiable at uncountably many points, such as the Cantor function.\n\n\n\n\n\n\n\n\n\nRecall that the Exponential(\\(\\lambda\\)) distribution is defined by the cdf: \\[F(x) = (1-e^{-\\lambda x}) \\mathbb 1\\{x > 0\\}.\\]\nThe pdf can be obtained by differentiating:\n\\[f(x) = \\frac{d}{dx} F(x)= \\lambda e^{-\\lambda x} \\mathbb 1 \\{x > 0\\}.\\]\n\\(F(x)\\) is not differentiable at 0, but it doesn’t matter since \\(f(x)\\) can be defined arbitrarily on any countable set.\n\n\n\nThe exponential pdf\n\n\nFor measurable sets \\(A \\subset \\mathbb R\\), we define \\[X^{-1}(A) = \\{ s \\in S \\colon X(s) = x \\}\\]\nFor any random variable \\(X\\) not necessarily discrete or continuous, we define \\[P(X \\in A) = P(X^{-1}(A)).\\]"
  },
  {
    "objectID": "week2/week2.html#identically-distributed-random-variables",
    "href": "week2/week2.html#identically-distributed-random-variables",
    "title": "Week 2",
    "section": "Identically distributed random variables",
    "text": "Identically distributed random variables\nRandom variables \\(X\\) and \\(Y\\) are identically distributed if for every measurable set \\(A\\), \\[P(X \\in A) = P(Y \\in A).\\]\nThis is denoted by \\(P \\stackrel{d}{=} Y\\).\nIt turns out that \\(X \\stackrel{d}{=} Y\\) if and only if \\(F_X(x) = F_Y(x)\\) for all \\(x \\in \\mathbb R\\)."
  },
  {
    "objectID": "week2/week2.html#transformations-of-random-variables",
    "href": "week2/week2.html#transformations-of-random-variables",
    "title": "Week 2",
    "section": "Transformations of Random Variables",
    "text": "Transformations of Random Variables\nOften we are interested in functions \\(g(X)\\) of a random variable \\(X\\).\nIf \\(g\\) is measurable, then \\(g(X)\\) is a random variable.\nLetting \\(Y = g(X)\\), the distribution of \\(Y\\) is characterized by \\[P(Y \\in A) = P(g(X) \\in A) = P(X \\in g^{-1}(A)).\\]\nRecall that \\[g^{-1}(A) = \\{ x \\colon g(x) \\in A \\},\\] so this does not require that \\(g\\) be invertible.\nIn the discrete case,\n\\[f_Y(y) = P(Y = y) = P(g(X) = y) = \\sum_{x \\colon g(x) = y} f_X(x).\\]"
  },
  {
    "objectID": "week2/week2.html#binomial-variable-transformation-example",
    "href": "week2/week2.html#binomial-variable-transformation-example",
    "title": "Week 2",
    "section": "Binomial Variable Transformation Example",
    "text": "Binomial Variable Transformation Example\nSuppose \\(X \\sim \\text{Binomial}(N,q)\\) and \\(Y = N - X\\). Then \\[P(Y = k) = P(g(X) = k) = P(N - X = k) = P(X = N-k).\\]\nThus plugging in the pdf of \\(X\\):\n\\[ P(Y=k) = {N \\choose N-k} q^{N-k} (1-q)^{N-(N-k)}\\] \\[ = { N \\choose k} (1-q)^k q^{N-k}.\\]\nTherefore \\(Y \\sim \\text{Binomial}(N,1-q)\\)."
  },
  {
    "objectID": "week2/week2.html#transformations-of-continuous-random-variables",
    "href": "week2/week2.html#transformations-of-continuous-random-variables",
    "title": "Week 2",
    "section": "Transformations of continuous random variables",
    "text": "Transformations of continuous random variables\nFor a continuous r.v. \\(X\\) we have to take more care.\n\nIf \\(g\\) is invertible, then you might mistakenly think the pdf of \\(Y = g(X)\\) equals \\(f_X(g^{-1}(y))\\), but this is not true in general. The reason is that the pdf \\(f_Y\\) is a density, not a probability.\nThe correct formula accounts for the derivative.\nSuppose \\(X\\) is a continuous r.v. and let \\(\\mathcal X = \\{ x \\colon f_X(x) > 0 \\}\\). If \\(Y = g(X)\\) where \\(g \\colon \\mathcal X \\to \\mathbb R\\) is a strictly monotone function such that the inverse \\(g^{-1}(y)\\) has a continuous derivative then \\[f_Y(y) = f_X(g^{-1}(y)) \\left \\lvert \\frac{d}{dy}g^{-1}(y) \\right\\rvert\\]\nfor \\(y \\in \\mathcal Y \\coloneqq \\{ g(x) \\colon x \\in \\mathcal X \\}\\) and \\(f_Y(y) = 0\\) elsewhere.\n“This is a great source for exam problems!”\n\n\nMonotonicity\nA function is monotone increasing if \\[x < x' \\Longrightarrow g(x) \\leq g(x').\\] We also call this non-decreasing.\nSimilarly, a function is monotone decreasing (or non-increasing) if \\[ x < x' \\Longrightarrow g(x) \\geq g(x').\\]\nNote that Casella and Berger use “monotone” to imply strictly monotone, though the definitions above are the more conventional meaning."
  },
  {
    "objectID": "week2/week2.html#example-square-root-of-an-exponential-r.v.",
    "href": "week2/week2.html#example-square-root-of-an-exponential-r.v.",
    "title": "Week 2",
    "section": "Example: Square root of an Exponential r.v.",
    "text": "Example: Square root of an Exponential r.v.\nSuppose \\(X \\sim \\text{Exponential}(\\lambda)\\) and \\(Y=\\sqrt{X}\\).\nThen \\(f_X(x) = \\lambda \\exp(-\\lambda x) \\mathbb 1 \\{ x > 0 \\}\\), so \\(\\mathcal X = (0,\\infty).\\)\nWe write \\(Y = g(X)\\) where \\(g(x) = \\sqrt{x}\\) for \\(x \\in \\mathcal X\\).\nFor \\(y \\in \\mathcal Y = (0,\\infty)\\), the inverse of \\(g\\) is \\[g^{-1}(y) = y^2.\\]\n\\[\\frac{d}{dx} g^{-1}(y) = 2y.\\]\nThus by the change of variables formula above,\n\\[f_Y(y) = f_X(g^{-1}(y)) \\left\\lvert \\frac{d}{dx} g^{-1}(y) \\right\\rvert\\] \\[ = 2\\lambda y \\exp (-\\lambda y^2) \\mathbb 1 \\{ y > 0 \\}.\\]"
  },
  {
    "objectID": "week3/week3.html",
    "href": "week3/week3.html",
    "title": "Week 3",
    "section": "",
    "text": "Probability Distributions of Transformed Random Variables\nThe uniform distribution has a particularly important role in probability. We define the uniform distribution on \\((a,b)\\) where \\(a < b\\) as\n\\[f_U(u) = \\frac{1}{b-a} \\mathbb 1 ( a < u < b) \\]\nfor \\(u \\in \\mathbb R\\). This is denoted \\(U \\sim \\text{Uniform}(a,b)\\).\nThe cdf of \\(U \\sim \\text{Uniform}(a,b)\\) is\n\\[F_U(u) = \\left\\{ \\begin{array}{ll} 0 \\quad \\quad & \\text{ if } u \\leq a \\\\ (u-a)/(b-a) & \\text{ if } u \\in (a,b) \\\\ 1 & \\text{ if } u \\geq b \\end{array}\\right.\\]\nThe standard uniform distribution Uniform(0,1) has the special property that \\(CDF(u) = u\\).\nThe expected value denoted \\(\\mathbb E(X)\\) or \\(\\mathbb EX\\) is the average over all values that the random variable takes weighted according to their probability or probability density. It is also referred to as the expectation or mean of \\(X\\).\nLet \\(X\\) be a random variable and let \\(g(x)\\) be a measurable function. If \\(X\\) is discrete then the expected value is\n\\[\\mathbb Eg(X) = \\sum_{x \\in \\mathcal X} g(x) f(x)\\]\nWhere \\(\\mathcal X = \\{ X(s) : s \\in S \\}\\) is the range of \\(X\\) and \\(f(x)\\) is the pmf of \\(X\\).\nIf \\(X\\) is continuous then the expected value of \\(g(X)\\) is \\[\\mathbb Eg(X) = \\int_{-\\infty}^\\infty g(x) f(x) \\, dx\\] where \\(f(x)\\) is the pdf of \\(X\\)."
  },
  {
    "objectID": "week3/week3.html#change-of-variables-piecewise",
    "href": "week3/week3.html#change-of-variables-piecewise",
    "title": "Week 3",
    "section": "Change of variables piecewise",
    "text": "Change of variables piecewise\nOften \\(g(x)\\) is not strictly monotone, but is piecewise strictly monotone, e.g., \\(g(x) = x^2, \\, \\forall x \\in \\mathbb R\\).\nSuppose that \\(X\\) is a continuous random variable and let \\(\\mathcal X = \\{ x \\colon f_X(x) > 0 \\}\\).\nSuppose \\(g \\colon \\mathcal X \\to \\mathbb R\\) and \\(A_0, A_1, ..., A_k\\) is a partition of \\(\\mathcal X\\) such that\n\n\\(P(X \\in A_0) = 0\\),\n\\(g\\) is strictly monotone on \\(A_i\\) for \\(i = 1,...,k\\),\nthe inverse of \\(g\\) on \\(A_i\\), say \\(g_i^{-1}\\), has a continuous derivative on \\(g(A_i) = \\{ g(x) \\colon x \\in A_i \\}\\),\n\nthen\n\\[f_Y(y) = \\sum_{i = 1}^k f_X(g^{-1}_i(y)) \\left| \\frac{d}{dy} g_i^{-1}(y) \\right| \\mathbb 1(y \\in g(A_i)).\\]\n\n\n\n\n\nExample of partitioning a function into monotone sections\n\n\n\n\nWe were discussing in class whether or not the partitioning needs to be finite, and we think that one could use infinitely many partitions in some cases (say, for example, a distribution convolved with the \\(\\sin\\) function).\n\nExample: Normal to Chi-squared Transformation\nA random variable \\(X\\) has the standard normal distribution if\n\\[f_X(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp(-\\frac{1}{2} x^2)\\] for \\(x \\in \\mathbb R\\). This is denoted \\(X \\sim \\mathcal N(0,1)\\).\nIf \\(X \\sim \\mathcal N(0,1)\\), then the random variable \\(Y = X^2\\) has the chi-squared distribution, denoted \\(Y \\sim \\chi^2\\).\n\\(g(x) = x^2\\) is strictly monotone on \\(A_1 = (-\\infty, 0)\\) and \\(A_2 = (0,\\infty)\\), wiith inverses \\(g_1^{-1}(y) = -\\sqrt{x}\\) and \\(g_2^{-1}(y) = \\sqrt{x}\\).\nBy the change of variables formula (in the piecewise case), for \\(y > 0\\) (remember we have to check when \\(y \\in g^{-1}(A_i)\\)),\n\\[f_Y(y) = \\frac{1}{\\sqrt{2\\pi}} \\exp(-\\frac{1}{2}(-\\sqrt{y})^2)\\left| \\frac{-1}{2\\sqrt{y}} \\right| +\n\\frac{1}{\\sqrt{2\\pi}} \\exp(-\\frac{1}{2}(\\sqrt{y})^2)\\left| \\frac{1}{2\\sqrt{y}} \\right| \\] \\[ = \\frac{1}{\\sqrt{2\\pi}} \\frac{1}{\\sqrt{y}} \\exp(-\\frac{1}{2} y).\\]\nAlong the way, we had to evaluate \\(\\frac{d}{dy} (-\\sqrt{y}) = \\frac{d}{dy} (-y^{1/2}) = \\frac{-1}{2}y^{-1/2}.\\)\nWe could write this ever-so-slightly more precisely:\n\\[ = \\left\\{ \\begin{array}{ll} \\frac{1}{\\sqrt{2\\pi}} \\frac{1}{\\sqrt{y}} \\exp(-\\frac{1}{2} y) \\quad & \\text{ if } y > 0 \\\\\n0 & \\text{ otherwise } \\end{array} \\right.\\]\n\nThese kinds of problems are great for exams!"
  },
  {
    "objectID": "week3/week3.html#probability-integral-transform",
    "href": "week3/week3.html#probability-integral-transform",
    "title": "Week 3",
    "section": "Probability Integral Transform",
    "text": "Probability Integral Transform\nLet \\(X\\) be a random variable with cdf \\(F\\). If \\(F\\) is a continuous function, then \\(F(X) \\sim \\text{Uniform}(0,1)\\).\nThis is called the probability integral transform.\nExample: Let \\(X \\sim \\text{Exponential}(\\lambda)\\). The cdf of \\(X\\) is \\[F(x) = (1-\\exp(-\\lambda x)) \\mathbb 1(x > 0),\\] which is a continuous function. Therefore, \\[1 - \\exp(-\\lambda X) \\sim \\text{Uniform}(0,1).\\]\nWhy can we drop the \\(\\mathbb 1 (X > 0)\\) factor? Because \\(X\\) is an exponential variable, so \\(X > 0\\) with probability 1. This is like asking \\[(1-e^{-\\lambda x}) \\mathbb 1 (X > 0) \\stackrel{d}{=} (1-e^{-\\lambda x})?\\]\nCan we simplify further? Yes! \\(\\text{Uniform}(0,1) = 1 - \\text{Uniform}(0,1)\\).\nSo we can rewrite this:\n\\[\\exp{-\\lambda x} = \\text{Uniform}(0,1).\\]"
  },
  {
    "objectID": "week3/week3.html#generalized-inverse-of-a-cdf",
    "href": "week3/week3.html#generalized-inverse-of-a-cdf",
    "title": "Week 3",
    "section": "Generalized Inverse of a cdf",
    "text": "Generalized Inverse of a cdf\nA cdf \\(F\\) can fail to be invertible in two ways:\n\n\\(F(x) = F(y)\\) for some \\(x \\neq y\\) (it is flat in some region), or\n\\(F\\) is discontinuous at some \\(x\\) (it has a jump at some point).\n\nThe generalized inverse of a cdf \\(F\\) is the function\n\\[G(u) = \\inf\\{x \\in \\mathbb R\\colon F(x) \\geq u \\}\\] for \\(u \\in (0,1).\\) We write \\(F_{-1}\\) to denote this function.\nWhen \\(F\\) is invertible, the generalized inverse equals the inverse, so there is no conflict in notation."
  },
  {
    "objectID": "week3/week3.html#inverse-probability-integral-transform",
    "href": "week3/week3.html#inverse-probability-integral-transform",
    "title": "Week 3",
    "section": "Inverse Probability Integral Transform",
    "text": "Inverse Probability Integral Transform\nLet \\(F\\) be any cdf. If \\(U \\sim \\text{Uniform}(0,1)\\) then \\(F^{-1}(U)\\) is a random variable with cdf \\(F\\).\nThis is called the inverse probability integral transform or the Smirnov transform.\nThe two transforms can be summarized as follows. Suppose \\(U \\sim \\text{Uniform}(0,1)\\) and \\(X\\) is a random variable with cdf \\(F\\). Then\n\n\\(F^{-1}(U) \\stackrel{d}{=} X.\\)\n\\(F(X) \\stackrel{d}{=} U\\) if \\(F\\) is continuous, but not otherwise.\n\n\nActivity: What is the distribution of \\(F(X)\\) in this example?\nMy thoughts:\nIf \\[F(x) = \\left\\{ \\begin{array}{ll}\n0 \\quad & \\text{ if } x \\leq 0 \\\\\nx \\quad & \\text{ if } 0 < x < .5 \\\\\n1 & \\text{ if } .5 \\leq x\n\\end{array}\\right.\\]\nSo if we take the derivative, \\[\\frac{d}{dx} F(x) = f_X(x) = \\left\\{\n\\begin{array}{ll}\n0 \\quad & \\text{ if } x \\leq 0 \\\\\n1 \\quad & \\text{ if } 0 < x < .5 \\\\\n0 & \\text{ if } .5 \\leq x\n\\end{array}\n\\right. \\]\nAnother student pointed out that looking at the height of the jump, there’s \\(Pr(X = .5) = .5\\),\nSo is it \\(f_X(x) = \\mathbb 1(0 < x < 0.5) + .5 \\times \\delta_{x = .5}(x)\\)?\nThere is a small mistake. The delta-distribution should be at 1, because when \\(x=1\\), \\(F(x) = 1\\).\nSo \\[F(x) = \\left\\{ \\begin{array}{ll}\n1 & \\text{ with probability }\\frac{1}{2} \\\\\n\\text{Uniform}(0,\\frac{1}{2}) & \\text{ with probability } \\frac{1}{2}\n\\end{array} \\right.\\]\n\nWhy should we care about this? Well it’s really useful computationally for generating random numbers that have different probability distributions from the uniform distribution. The Mersenne-Twister algorithm is the state of the art for generating uniform distribution random samples, and then often to get random numbers from other distributions one employs the above type of transformations under-the-hood."
  },
  {
    "objectID": "week3/week3.html#properties-of-expectation",
    "href": "week3/week3.html#properties-of-expectation",
    "title": "Week 3",
    "section": "Properties of Expectation",
    "text": "Properties of Expectation\nLet \\(X\\) be a random variable with range \\(\\mathcal X\\) and let \\(g,h \\colon \\mathcal X \\to \\mathbb R\\) be measurable functions such that \\(\\mathbb Eg(X)\\) and \\(\\mathbb Eh(X)\\) exist and are finite.\nThe following properties hold:\n\n\\(\\mathbb E(c\\, g(X)) = c \\mathbb Eg(X)\\) for any \\(c \\in \\mathbb R\\).\n\\(\\mathbb E(g(X) + h(X)) = \\mathbb Eg(X) + \\mathbb Eh(X)\\)\nIf \\(g(x) \\leq h(x)\\) for all \\(x \\in \\mathcal X\\), then \\(\\mathbb Eg(X) \\leq \\mathbb Eh(X)\\).\n\n\nExpectation minimizes the mean squared error\nSuppose we want to choose one point \\(a\\) that predicts \\(Y\\) as closely as possible (for instance, as in regression).\nIf we define “close” in terms of mean squared error, \\(\\mathbb E(|Y - a|^2)\\), then \\(\\mathbb EY\\) is the optimal choice of \\(a\\).\nTo see this, observe that \\[\\mathbb E(|Y-a|^2) = \\mathbb E(Y^2 - 2aY + a^2)\\] \\[ = \\mathbb EY^2 - 2a\\mathbb EY + a^2.\\]\nTo minimize, set the derivative equal to zero:\n\\[ 0 = \\frac{d}{da} \\mathbb E(|Y-a|^2) = -2\\mathbb EY + 2a\\]\nand solve for \\(a\\) to obtain \\(a = \\mathbb EY\\).\n\n\nExistence of Expected Values\nThe positive part of \\(X\\), denoted \\(X^+\\), is \\(X^+ = \\max(X,0)\\).\nThe negative part of \\(X\\), denoted \\(X^-\\), is \\(X^- = -\\min(X,0)\\).\nNote that \\(X = X^+ - X^-\\) and \\(|X| = X^+ + X^-\\).\n\n \n\n\nExistence vs. Finiteness\nWe say that the expected value of \\(X\\) exists if either \\(\\mathbb EX^+ < \\infty\\) or \\(\\mathbb EX^- < \\infty\\) or both. This definition differs from Casella & Berger’s definition.\nIf \\(\\mathbb E|X| < \\infty\\) then \\(\\mathbb EX\\) exists.\n\nThis is a sufficient (but not necessary) condition for existence.\nRecall that \\(|X| = X^+ + X^-\\), so \\(\\mathbb E|X| = \\mathbb EX^+ + \\mathbb EX^-\\).\nTherefore if \\(\\mathbb E|X| < \\infty\\), then \\(\\mathbb EX^+ < \\infty\\) and \\(\\mathbb EX^- < \\infty\\), since \\(X^+ \\geq 0\\) and \\(X^- \\geq 0\\).\n\nIn fact, if \\(\\mathbb E|X| < \\infty\\), then \\(\\mathbb EX\\) is finite. That is, \\(\\mathbb EX \\in \\mathbb R\\). This is because \\(\\mathbb EX = \\mathbb EX^+ - \\mathbb EX^-\\).\n\nWhy does \\(\\mathbb E|X|\\) always exist? Because \\(|X|^- = 0\\) and hence \\(\\mathbb E|X|^- < \\infty\\), and therefore we satisfy the definition of existence for an expectation that either the expectation of the positive part or the negative part are finite.\nThe only time the expectation doesn’t exist is when the expectation of the positive part and negative part are both infinite (resulting in \\(\\mathbb EX = \\mathbb EX^+ - \\mathbb EX^- = \\infty - \\infty = \\text{undefined}\\))."
  },
  {
    "objectID": "week4/week4.html#well-defined-vs.-undefined-expectations",
    "href": "week4/week4.html#well-defined-vs.-undefined-expectations",
    "title": "Week 4",
    "section": "Well-Defined vs. Undefined Expectations",
    "text": "Well-Defined vs. Undefined Expectations\nThe positive part of \\(X\\) is well-defined if either \\(E X^+ < \\infty\\) or \\(E X^- < \\infty\\).\nA random variable \\(X\\) has the Zeta(\\(s\\)) distribution for \\(s > 1\\), if it has pmf\n\\[f_X(k) = P(X = k) = \\frac{1}{\\zeta(s)k^s} \\mathbb 1 (k \\in \\mathbb \\{ 1, 2, ... \\})\\]\nwhere \\(\\mathbb \\zeta (s)\\) is the Riemann zeta function.\nSince \\(EX^- = E(-\\min(X,0)) = 0\\), \\(EX\\) is well-defined.\nRecall that \\(\\zeta(s) = \\sum_{i=1}^\\infty \\frac{1}{k^2}\\).\nHowever, if \\(s \\leq 2\\) then the mean is infinite:\n\\[EX = \\sum_{i=1}^\\infty k f_X(k) = \\sum_{i=1}^\\infty\n\\frac{1}{\\zeta (s) k^{s-1}} = \\infty.\\]"
  },
  {
    "objectID": "week4/week4.html#recap-well-defined-vs.-undefined-expectations",
    "href": "week4/week4.html#recap-well-defined-vs.-undefined-expectations",
    "title": "Week 4",
    "section": "Recap: Well-Defined vs. Undefined Expectations",
    "text": "Recap: Well-Defined vs. Undefined Expectations\nThe positive part of \\(X\\) is well-defined if either \\(E X^+ < \\infty\\) or \\(E X^- < \\infty\\).\nA random variable \\(X\\) has the Zeta(\\(s\\)) distribution for \\(s > 1\\), if it has pmf\n\\[f_X(k) = P(X = k) = \\frac{1}{\\zeta(s)k^s} \\mathbb 1 (k \\in \\mathbb \\{ 1, 2, ... \\})\\]\nwhere \\(\\mathbb \\zeta (s)\\) is the Riemann zeta function. The use of the Riemann zeta function may seem scary, but it’s really just acting as the normalizing constant here so that this is a proper pmf.\nSince \\(EX^- = E(-\\min(X,0)) = 0\\), \\(EX\\) is well-defined.\nRecall that \\(\\zeta(s) = \\sum_{i=1}^\\infty \\frac{1}{k^s}\\).\nHowever, if \\(s \\leq 2\\) then the mean is infinite:\n\\[EX = \\sum_{i=1}^\\infty k f_X(k) = \\sum_{i=1}^\\infty\n\\frac{1}{\\zeta (s) k^{s-1}} = \\infty.\\]\nNow suppose that \\(Y\\) is a discrete random variable with pmf\n\\[f_Y(k) = P(Y=k) = \\frac{1}{2ck^2} \\mathbb 1 (|k| \\in \\{ 1, 2, ... \\})\\]\nwhere \\(c = \\zeta(2)\\).\nThen \\(EY^+ = \\infty\\) and \\(EY^- = \\infty\\). For example,\n\\[EY^+ = \\sum_{k=0}^\\infty kP(Y^+ = k) = \\sum_{k=1}^\\infty \\frac{1}{2ck} = \\infty.\\]\nSo the mean of \\(Y\\) is not well-defined (or, undefined).\n\nCauchy Distribution Example\nA random variable \\(X\\) has the Cauchy(0,1) distribution if it has the pmf\n\\[f_X(x) = \\frac{1}{p} \\frac{1}{1+x^2}\\]\nfor \\(x \\in \\mathbb R\\). If \\(X \\sim \\text{Cauchy}(0,1)\\) then \\(EX\\) is undefined.\n\n\n\n\n\n\n\n\n\nThe Cauchy distribution has heavy tails, meaning it can take very large values with non-negligible probability."
  },
  {
    "objectID": "week4/week4.html#moments",
    "href": "week4/week4.html#moments",
    "title": "Week 4",
    "section": "Moments",
    "text": "Moments\nLet \\(k\\) be a positive integer. The \\(k\\)th moment of \\(X\\) is \\(E(X^k)\\). The \\(k\\)th central moment of \\(X\\) is \\(E((X-EX)^k)\\).\nThe variance of a random variable is the 2nd central moment:\n\\[\\text{Var}(X) \\stackrel{def}{=} E((X-EX)^2).\\]\n\\(\\text{Var}(X)\\) is sometimes denoted \\(\\sigma^2(X)\\) or simply \\(\\sigma^2\\).\nThe standard deviation of \\(X\\) is \\(\\sqrt{\\text{Var}(X)}\\).\nBoth the variance \\(\\sigma^2\\) and the standard deviation \\(\\sigma\\) quantify how spread out a distribution is. However, \\(\\sigma\\) is more interpretable since it is in the same units as \\(X\\).\n\nIf the mean is undefined for a distribution, the variance and standard deviaion will also be undefined, as in the Cauchy distribution. How might we quantify the spread of the distribution? One might use quantiles. Median absolute deviation. A really simple approach might be the difference between the 95th and 5th percentiles.\n\n\nProperties of Variance\n\nIf \\(\\text{Var}(X) < \\infty\\), then for any \\(a,b \\in \\mathbb R\\),\n\n\\[\\text{Var}(aX + b) = a^2 \\text{Var}(X).\\]\n\nA useful formula for the variance is \\(\\text{Var}(X) = EX^2 - (EX)^2\\).\nSuppose \\(Y\\) is an estimator of some quantity \\(y_0\\). Then the mean squared error is \\[mse = E(|Y - y_0|^2) = (EY - y_0)^2 + E((Y-EY)^2).\\]\n\n\\[ = \\text{bias}^2 + \\text{variance}\\]\n\nProof of 2. \\[\\text{Var}(X) = E((X - E X)^2)\\] \\[ = E(X X - X E X - E X X + (E X)^2)\\] \\[ = E X^2 - 2E XE X + (E X)^2\\] \\[ = E (X^2) - (E X)^2\\]\nProof of 1 using 2. Now apply the 2nd to the first question:\n\\[\\text{Var}(aX+b) = E((aX+b - E(aX+b))^2)\\]\nor \\[\\text{Var}(aX+b) = E((aX+b)^2) - E(aX+b)^2 \\] \\[ = E(a^2X^2+2abE X + b^2) - (aE X+b)^2 \\]\n\\[ =  (a^2E(X^2)+\\cancel{2abE X} + \\cancel{b^2}) - (a^2(E X)^2+\\cancel{2abE X}+\\cancel{b^2})\\] \\[ = a^2((E X^2)^2 - E(X)^2) \\] \\[ = a^2\\text{Var}(X)\\]\nProof of 1 using definitions. Using the 2nd central moment formula:\n\\[\\text{Var}(aX+b) = E((aX+b - E(aX+b))^2)\\] \\[ = E((aX+b - (aE X+b))^2)\\] \\[ = E((aX - (aE X)^2)\\] \\[ = a^2E((X - (E X)^2)\\] \\[ = a^2\\text{Var}(X)\\]\nProof of 3. Mean squared error is defined as \\[mse = E((Y - y_0)^2)\\]\nA nice trick is to add and subtract by the same thing.\n\\[ \\text{mse} = E((Y - EY + EY - y_0)^2)\\] \\[ = E((Y-EY)^2 + 2(Y-EY)(EY-y_0) + (EY - y_0)^2)\\] \\[ = E((Y-EY)^2) + 2\\underbrace{(EY-EY)}_{=0}(EY-y_0) + (EY - y_0)^2\\] \\[ = \\underbrace{E((Y-EY)^2)}_{\\text{variance}} + \\underbrace{(EY - y_0)^2}_{\\text{bias}^2}\\]\n\n\nA good illustration of the bias-variance tradeoff is in estimating the sample variance of normally distributed values.\nSuppose that \\(X_1, X_2, ... \\sim \\mathcal N(\\mu, \\sigma^2)\\).\n\\[\\bar x = \\frac{1}{n} \\sum_{i=1}^n X_i\\]\n\\[\\hat \\sigma^2 = \\frac{1}{n-1} \\sum_{i=1}^n (X_i-\\bar X)^2\\]\nThe above is unbiased, but it’s not the estimator with lowest mse. One can get a better estimator with lower mean-squared-error by using either \\(1/n\\) or \\(1/(n+1)\\). For more details on the \\(1/(n+1)\\) correction, look at page 351 in Casella and Berger.\nThe usual \\(1/(n-1)\\) correction is known as Bessel’s correction.\nEven further, suppose that \\(X_1, X_2, ... \\sim \\mathcal N(\\mu_i, \\sigma^2)\\).\nNaively, one would think that the best estimates for \\(\\hat \\mu_i\\) is just \\(X_i\\), but the James-Stein estimator/paradox shows that by decreasing the variance we can come up with estimators that have lower mean-squared-error.\n\nA common misperception is that bias is always bad. In fact, allowing some bias usually improves performance by reducing variance. This is especially important when building a prediction model. Less flexible models tend to have greater bias, since they cannot fit the distributions as closely. More flexible models tend to have greater variance, since they have more parameters to estimate. Since \\(\\text{mse} = \\text{bias}^2 + variance\\), there is a trade-off, and mse is minimized by setting the flexibility equal to some critical point."
  },
  {
    "objectID": "week4/week4.html#moment-generating-functions",
    "href": "week4/week4.html#moment-generating-functions",
    "title": "Week 4",
    "section": "Moment Generating Functions",
    "text": "Moment Generating Functions\nThe moment generating function (mgf) of a random variable \\(X\\) is\n\\[M_X(t) = E[e^{tX}]\\] for \\(t \\in \\mathbb{R}\\).\nThe mgf is said to exist if \\(M_X(t)\\) is finite in a neighborhood of zero. In other words, if there is some \\(h > 0\\) such that \\(M_X(t) < \\infty\\) whenever \\(|t| < h\\).\nThis terminology is a little weird since the function always exists but might be infinite.\nWhy is it called the “moment generating function”?\nFor all \\(k \\in \\{ 1, 2, 3, ... \\}\\),\n\\[EX^k = \\frac{d^k}{dt^k} M_X(t) \\lvert_{t=0.}\\]\nThat is, the \\(k\\)th moment of \\(X\\) equals the \\(k\\)th derivative of \\(M_X(t)\\) evaluated at \\(t=0\\).\nSo \\(M_X(t)\\) is a function from which one can “generate” the moments simply by differentiating and evaluating at \\(t=0\\).\n\nExponential Example\nIf \\(X \\sim \\text{Exponential}(\\lambda)\\), then for \\(|t| < \\lambda\\),\n\\[M_X(t) = E[e^{tx}] = \\int_0^\\infty \\exp(tx) \\lambda \\exp(-\\lambda x) dx\\] \\[ = \\lambda \\int_0^\\infty \\exp(-(\\lambda - t)x) dx\\]\n\\[= \\frac{\\lambda}{\\lambda - 1} \\int_0^\\infty (\\lambda - t)\\exp(-(\\lambda - t)x)dx\\]\nIn the last step, we multiplied and divided by \\(\\lambda - t\\) so that the inside is an exponential pdf with parameter \\(\\lambda - t)\\) (and thus has integral 1).\n\\[ = \\frac{\\lambda}{\\lambda - 1} < \\infty\\]\nfor \\(|t| < \\lambda.\\)\nWe can easily compute the moments of \\(X\\) using the mgf.\nWithout using the mgf, we’d have to use integration by parts to solve:\n\\[EX^k = \\int_0^\\infty x^k \\lambda e^{-\\lambda x} dx,\\] which could be a bit painful for larger \\(k\\).\nSo instead, using the mgf, we get that the 1st and 2nd moments are:\n\\[E X = \\frac{d}{dt} \\frac{\\lambda}{\\lambda - t} \\big\\lvert_{t=0} = \\frac{\\lambda}{(\\lambda - t)^2} \\big\\lvert_{t=0} = \\frac{1}{\\lambda}\\]\n\\[EX^2 = \\frac{d^2}{dt^2} \\frac{\\lambda}{\\lambda-t} \\big\\lvert_{t=0} = \\frac{d}{dt} \\frac{\\lambda}{(\\lambda - t)^2} \\big\\lvert_{t=0} = \\frac{2\\lambda(\\lambda- t)}{(\\lambda-t)^4} \\big\\lvert_{t=0} = \\frac{2}{\\lambda^2}. \\]\nThus the variance of \\(X\\) is\n\\[\\text{Var}(X) = EX^2 - (EX)^2 = \\frac{2}{\\lambda^2} - \\frac{1}{\\lambda^2} = \\frac{1}{\\lambda^2}.\\]\n\nCould it be that the moments still exist even if the mgf does not take on a finite value?\nRecall that \\[e^{tX} = \\sum_{k=0}^\\infty \\frac{(tX)^k}{k!} \\geq \\frac{t^kX^k}{k!} \\quad (X \\geq 0)\\]\nSo it might be that the moment generating function doesn’t exist while the moments themselves do exist.\nWe’ll get to the characteristic function soon:\n\\[\\phi_X(t) = E(e^{itX}).\\]\nAnd \\(|e^{itX}| = 1\\).\n\n\n\nUniqueness of Moments\n\\(X\\) has bounded support if \\(P(|X| < c) = 1\\) for some \\(c \\in \\mathbb{R}\\).\nSuppose \\(X\\) and \\(Y\\) have bounded support. Then \\(X \\stackrel{d}{=} Y\\) if and only if \\(EX^k = EY^k\\) for all \\(k \\in \\{ 1, 2, ... \\}\\).\nIf \\(M_X(t)\\) and \\(M_Y(t)\\) exist and are equal on a neighborhood of zero, then \\(X \\stackrel{d}{=} Y\\).\n\nThis does not hold in general for unbounded distributions. There’s such an example in Casella and Berger."
  },
  {
    "objectID": "week4/week4.html",
    "href": "week4/week4.html",
    "title": "Week 4",
    "section": "",
    "text": "Families of Distributions\nIn statistics, families of distributions play a key role. Many statistical methods are based on assuming that the data are distributed according to some family of distributions. Estimation and inference then proceeds by finding the parameters of the family that could plausibly have generated the observed data.\nFor instance, if one assumes that data \\(X_1, ..., X_n\\) are \\(\\mathcal N(\\mu, \\sigma^2)\\) distributed, then we could use maximum likelihood to estimate \\(\\mu\\) and \\(\\sigma^2\\) as:\n\\[\\hat \\mu = \\frac{1}{n} \\sigma_{i=1}^n X_i \\quad \\quad \\hat \\sigma^2 = \\frac{1}{n} \\sum_{i=1}^n (X_i - \\bar X)^2.\\]\nMany commonly used distributions have special properties that make them well-justified in particular applications.\nExamples:\nSelecting, combining, and/or transforming distributions according to the “physics” of the data generating process is important for good statistical modeling.\nWe often write the name of the distribution itself to denote the pdf/pmf. For instance, \\(\\text{Uniform(x|a,b)\\) denotes the pdf of \\(\\text{Uniform(a,b)}\\).\nIf the distribution \\(X\\) has been defined, say as \\(X \\sim \\text{Uniform}(a,b)\\), another shorthand is to write \\(p(x|a,b)\\) for the pdf/pmf.\nWe will often denote pdfs or pmfs as \\(p(\\cdot)\\) instead of \\(f(\\cdot)\\).\nWhen dealing with multiple r.v.s, say \\(X\\) and \\(Y\\), it is common to simply write \\(p(x)\\) and \\(p(y)\\) for the pdf/pmf of \\(X\\) and \\(Y\\), respectively, instead of \\(p_X(x)\\) and \\(p_Y(y)\\). In other words, the letters used (\\(x\\) or \\(y\\)) indicates which random variable we’re talking about."
  },
  {
    "objectID": "week4/week4.html#differentiating-under-the-integral-sign",
    "href": "week4/week4.html#differentiating-under-the-integral-sign",
    "title": "Week 4",
    "section": "Differentiating under the integral sign",
    "text": "Differentiating under the integral sign\nOften we want to interchange the order of differentiation and integration.\nFor example, for mgfs:\n\\[\\frac{d^k}{dt^k} M_X(t) \\lvert_{t=0} = \\frac{d^k}{dt^k} E\\big(\\exp(tX)\\big) \\lvert_{t=0}\\] \\[ = E\\big(\\frac{d^k}{dt^k} \\exp(tX)\\lvert_{t=0}\\big)\\] \\[ = E\\big( X^k \\exp(tX) \\lvert_{t=0}\\big)\\] \\[ = E(X^k).\\]\nThis is using the fact that \\(\\frac{d}{dt} e^{tx} = x e^{tx}\\), and hence \\(\\frac{d^k}{dt^x} e^{tx} = x^k e^{tx}\\).\nThe step where we swap the order of \\(\\frac{d^k}{dt^k}\\) and \\(E\\) is called differentiating under the integral sign.\nHowever, regularity conditions are needed for this to hold.\nSuppose that \\(f(x,t)\\) is differentiable with respect to \\(t\\) for each \\(x\\), and there exists a function \\(g(x,t)\\) such that\n\nfor all \\(x\\) and all \\(t'\\) in a neighborhood of \\(t\\) \\[\\left\\lvert \\frac{\\partial }{\\partial t} f(x,t) \\lvert_{t=t'} \\right\\rvert \\leq g(x,t)\\]\n\\(\\int_{-\\infty}^\\infty g(x,t) dx < \\infty\\). Then \\[\\frac{d}{dt} \\int_{-\\infty}^\\infty f(x,t) dx = \\int_{-\\infty}^\\infty \\frac{\\partial}{\\partial t} f(x,t) dx.\\]\n\nSee Casella & Berger (Theorem 2.4.3) for a slightly more general version. This proof uses one of the most important results in measure theory: the dominated convergence theorem.\nWe present a non-measure theoretic version of the dominated convergence theorem here, which is not fully general but gets the main idea across.\nSuppose that \\(f(x,t)\\) is continuous at \\(t_0\\) for each \\(x\\) and there exists \\(g(x)\\) such that\n\n\\(|f(x,t)| \\leq g(x)\\) for all \\(x\\) and all \\(t\\), and\n\\(\\int_{-\\infty}^\\infty g(x) dx < \\infty\\).\n\nThen \\[\\lim_{t\\to t_0} \\int_{-\\infty}^\\infty f(x,t) dx = \\int_{-\\infty}^{\\infty} \\lim_{t\\to t_0} f(x,t) dx.\\]\nThe dominated convergence theorem allows us to justify switching the order of limits and integrals.\nWe can think about the dominated convergence theorem as describing a situation where we have a sequence of functions:\n\\[f_1(x),\\, f_2(x),\\, f_3(x),\\, \\cdots\\]\nAnd what we’re saying is \\[\\lim_{n \\to \\infty} \\int f_n(x) dx = \\int \\left( \\lim_{n \\to \\infty} f_n(x))\\right) dx.\\]\n\n\n\n\n\n\n\n\n\n\nA counter-example would be \\(f_n(x) = 1/n\\), so \\(f_*(x) = 0\\), but \\(\\int f_n(x) dx > 0\\) for all \\(n\\).\nAnother counter-example is \\(f_n(x) = \\mathbb 1(n < x < n+1)\\). The limit \\(f_*(x) = 0\\) because for every \\(x\\), as \\(n\\to \\infty\\), there is an \\(N \\in \\mathbb N\\) such that for all \\(N' > N\\) \\(f_{N'}(x) = 0\\).\nThis is what the requirements around the existence of such a function \\(g(x)\\) are telling us (that \\(g(x)\\) is an envelope for all \\(f_n(x)\\) and \\(\\int_{-\\infty}^\\infty g(x) dx < \\infty\\)."
  },
  {
    "objectID": "week4/week4.html#independence-of-random-variables",
    "href": "week4/week4.html#independence-of-random-variables",
    "title": "Week 4",
    "section": "Independence of Random Variables",
    "text": "Independence of Random Variables\nRandom variables \\(X_1, ..., X_n\\) are independent if\n\\[P(X_1 \\in A_1, ..., X_n \\in A_n) = P(X_1 \\in A_1) \\cdots P(X_n \\in A_n)\\] for all measurable subsets \\(A_1, ..., A_n \\subset \\mathbb{R}\\).\nThe \\(\\text{Bernoulli}(q)\\) distribution is the special case of \\(\\text{Binomial}(N,q)\\) when \\(N=1\\).\nIf \\(X_1, ..., X_n \\sim \\text{Bernoulli}(q)\\) are independent, then\n\\(\\sum_{i=1}^n X_i \\sim \\text{Binomial}(N,q).\\)\nFrom this, it is easy to derive the mean of the Binomial distribution:\n\\[\\mathbb{E}\\left( \\sum_{i=1}^n X_i \\right) = \\sum_{i=1}^n \\mathbb{E}X_i = \\sum_{i=1}^n q = nq.\\]\n\nPoisson Distribution\nThe \\(\\text{Poisson}(\\lambda)\\) distribution has pmf\n\\[p(x|\\lambda) = e^{-\\lambda} \\frac{\\lambda^x}{x!} \\mathbb 1(x \\in \\mathcal X)\\]\nwhere \\(\\mathcal X = \\{ 0, 1, 2,...\\}\\). The parameter \\(\\lambda > 0\\) is referred to as the rate, for reasons that will become clear when we study Poisson processes.\nThe mean and variance of \\(X \\sim \\text{Poisson}(\\lambda)\\).\nThe Poisson model is often a good model for counting the occurrences of independent rare events.\nExamples:\n\nIn genomics, the number of reads covering a given locus is well-modeled as Poisson.\nIn physics, the number of photons hitting a detector during a given period of time is Poisson distributed.\nIn ecology, the number of organisms in a given region is often well-modeled as Poisson.\n\nThis is all due to a special property of the Poisson distribution jokingly referred to as the “law of small numbers”.\nThe Poisson is a limit of Binomials: if \\(q_N \\in (0,1)\\) is such that \\[N q_N \\to \\lambda\\] as \\(N \\to \\infty\\) for some \\(\\lambda > 0\\), then for all \\(x \\in \\{ 0, 1, 2, ...\\}\\),\n\\[\\text{Binomial}(x|N,q_N) \\longrightarrow \\text{Poisson}(x|\\lambda).\\]\n\n\nGeometric Distribution\nThe \\(\\text{Geometric}(q)\\) distribution has pmf\n\\[p(x|q) = (1-q)^{x-1}q \\mathbb 1(x \\in \\mathcal X).\\]\n\\[\\mathbb{E}X = 1/q\\]\n\\[\\text{Var}(X) = \\frac{1-q}{q^2}.\\]"
  },
  {
    "objectID": "week5/week5.html",
    "href": "week5/week5.html",
    "title": "Week 5",
    "section": "",
    "text": "Families of Distribution (cont. Discrete Distributions)"
  },
  {
    "objectID": "week5/week5.html#poisson-as-limit-of-binomials-law-of-small-numbers",
    "href": "week5/week5.html#poisson-as-limit-of-binomials-law-of-small-numbers",
    "title": "Week 5",
    "section": "Poisson as limit of Binomials (“Law of small numbers”)",
    "text": "Poisson as limit of Binomials (“Law of small numbers”)\n\nx <- rbinom(n = 10000, size = 100, prob = .05)\ny <- dpois(x = seq(0,20), lambda = 5)"
  },
  {
    "objectID": "week5/week5.html#geometric-distribution",
    "href": "week5/week5.html#geometric-distribution",
    "title": "Week 5",
    "section": "Geometric Distribution",
    "text": "Geometric Distribution\nThe Geometric(q) distribution has pmf\n\\[p(x|q) = (1-q)^{x-1} q \\mathbb 1(x \\in \\mathcal X),\\]\nwhere \\(\\mathcal X = \\{ 1, 2,... \\}\\). The parameter \\(q \\in (0,1)\\).\nThe mean and variance of \\(X \\sim \\text{Geometric}(q)\\) are:\n\\[EX = 1/q\\] \\[\\text{Var}(X) = \\frac{1-q}{q^2}.\\]\n\nGambler’s Fallacy\nSuppose you are playing a game with dice and someone notices that 1 hasn’t been rolled yet. Since the proportion of times that 1 is rolled must converge to 1/6, they think there is a high probability of rolling a 1 next.\nDo you think that the probability of rolling 1 next is (a) higher than 1/6, (b) equal to 1/6, or (c) lower than 1/6?\nA statistician might say we actually don’t know that the die is fair, and would need to come up with some empirical evidence describing our uncertainty around the fairness of the die.\nHowever, in general, if we assume that the die is fair, then the correct answer is that the probability of rolling 1 does not depend on the prior rolls, and hence would be 1/6.\n\n\nMemorylessness property\nThe gambler’s fallacy is related to a special property of the Geometric distribution.\nSuppose you have flipped tails \\(t\\) times. What is the probability that it will take \\(\\geq x\\) more flips to get heads? It doesn’t matter that we got tails \\(t\\) times already!\nMemorylessness property: If \\(X \\sim \\text{Geometric}(q)\\), then for all integers \\(t, x \\geq 0\\),\n\\[P(X > t + x | X > t) = P(X > x).\\]\nGeometric distributions are the only discrete distributions on \\(\\{1,2,...\\}\\) satisfying this property.\n\nWhy would we say that a normal distribution doesn’t have this memorylessness property? Well, if \\(X \\sim \\mathcal N(\\mu, \\sigma^2)\\) then it’s not true that \\(X | X > x_0\\) is a normal distribution."
  },
  {
    "objectID": "week5/week5.html#uniform-distribution",
    "href": "week5/week5.html#uniform-distribution",
    "title": "Week 5",
    "section": "Uniform Distribution",
    "text": "Uniform Distribution\nThe uniform distribution from \\(a\\) to \\(b\\) has pdf\n\\[p(x| a,b) = \\frac{1}{b-a} \\mathbb 1(x \\in \\mathcal X)\\]\nwhere \\(\\mathcal X = (a,b).\\) The parameters are \\(a,b \\in \\mathbb R\\) with \\(a < b\\).\nThe mean and variance of \\(X \\sim \\text{Uniform}(a,b)\\) are:\n\\[EX = (a+b)/2\\] \\[\\text{Var}(X) = \\frac{(b-a)^2}{12}.\\]"
  },
  {
    "objectID": "week5/week5.html#normal-gaussian-distribution",
    "href": "week5/week5.html#normal-gaussian-distribution",
    "title": "Week 5",
    "section": "Normal (Gaussian) Distribution",
    "text": "Normal (Gaussian) Distribution\nThe \\(\\mathcal N(\\mu, \\sigma^2)\\) distribution has pdf\n\\[p(x | \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\n\\left(\n-\\frac{1}{2\\sigma^2} (x-\\mu)^2\n\\right)\\]\nfor all \\(x \\in \\mathbb R\\). The parameters are \\(\\mu \\in \\mathbb R\\) and \\(\\sigma^2 > 0.\\)\nThe mean and variance of \\(X \\sim \\mathcal N(\\mu, \\sigma^2)\\) are \\[EX = \\mu\\] \\[\\text{Var}(X) = \\sigma^2.\\]\nBe careful that some people write \\(\\mathcal N(\\mu, \\sigma)\\) to denote the normal distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma\\). This comes up when there’s a formula in the second parameter position.\n\nSpecial Properties of the Normal Distribution\nThe normal distribution’s most special property relates to the central limit theorem (CLT).\n\nCLT tells us that the sum of a large number of independent random variables is approximately normal.\nConsequently, many real-world quantities tend to be normally distributed.\nWhen designing models, the CLT helps us understand when a normal model would be appropriate.\n\nWhy would human height be roughly normal? Because there are so many little factors (huge numbers of genetic loci, specific environmental factors, etc.) that add up together to form individual observations leading to the variability observed being reminiscent of the CLT.\nAnalytic tractability:\n\nCalculations can often be done in closed form, making normal models computationally convenient. Normal distributions can be combined to build complex models that are still tractable, such as Kalman filters.\n\nFor details on the analytic properties of the normal distribution and how its “niceness” led to its derivation:\n\nhttps://link.springer.com/chapter/10.1007/978-0-387-46409-1_7\nhttps://www3.nd.edu/~rwilliam/stats1/x21.pdf\nhttps://maa.org/sites/default/files/pdf/upload_library/22/Allendoerfer/stahl96.pdf\nhttps://math.stackexchange.com/questions/384893/how-was-the-normal-distribution-derived"
  },
  {
    "objectID": "week5/week5.html#chi-squared-distribution",
    "href": "week5/week5.html#chi-squared-distribution",
    "title": "Week 5",
    "section": "Chi-Squared Distribution",
    "text": "Chi-Squared Distribution\nSuppose that \\(X_1, ..., X_n \\sim \\mathcal N(0,1)\\) independently.\nThe distribution of \\(\\sum_{i=1}^n X_i^2\\) is called the chi-squared distribution with \\(n\\) degrees of freedom, denoted \\(\\chi^2(n)\\) or \\(\\chi_n^2\\).\nThe chi-squared distribution comes up a lot in statistical hypothesis testing, for instance when performing a \\(t\\) test.\nIf \\(X_1,...,X_n \\sim \\mathcal N(\\mu, \\sigma^2)\\) independently, then\n\\[(1/\\sigma^2) \\sum_{i=1}^n (X_i - \\bar X)^2 \\sim \\chi^2(n-1).\\]\nWhy the \\(n-1\\) instead of \\(n\\)? One already uses one degree of freedom to estimate the sample mean.\nIt turns out that \\(\\chi^2(n)\\) is a special case of the Gamma distribution."
  },
  {
    "objectID": "week5/week5.html#gamma-distribution",
    "href": "week5/week5.html#gamma-distribution",
    "title": "Week 5",
    "section": "Gamma Distribution",
    "text": "Gamma Distribution\nThe Gamma(a,b) distribution with shape \\(a>0\\) and rate \\(b>0\\) has pdf\n\\[p(x | a,b) = \\frac{b^a}{\\Gamma(a)} x^{a-1}\\exp(-bx) \\mathbb 1(x \\in \\mathcal X)\\]\nwhere \\(\\mathcal X = (0,\\infty)\\). Here, \\(\\Gamma(a) = \\int_0^\\infty t^{a-1}e^{-t} dt.\\)$\nThe mean and variance of \\(X \\sim \\Gamma(a,b)\\) are:\n\\[EX = a/b\\]\n\\[\\text{Var}(X) = a/b^2.\\]\nBe careful that there is another parameterization of the Gamma distribution that is often used. (In fact, Casella & Berger seem to prefer this parameterization.)\n\\(\\text{Gamma}(a, \\theta)\\) with shape \\(a > 0\\) and scale \\(\\theta > 0\\) (where \\(1/\\theta\\) is the same as the rate parameter from before) has pdf:\n\\[p(x | a, \\theta) = \\frac{(1/\\theta)^a}{\\Gamma(a)} x^{a-1} \\exp(-x / \\theta) \\mathbb 1 (x \\in \\mathcal X)\\]\nwhere \\(\\mathcal X = (0,\\infty)\\). When coding and reading books, make sure one knows which is being used. In what follows, unless otherwise specified, we’ll use the shape and rate parameterization.\n\nGamma relationships\n\n\\(\\text{Gamma}(1,\\lambda) = \\text{Exponential}(\\lambda).\\)\nIf \\(X \\sim \\text{Gamma}(a,b)\\) then \\(cX \\sim \\text{Gamma}(a,b/c)\\).\nIf \\(X_i \\sim \\text{Gamma}(a,b)\\) independently for \\(i = 1,...,n\\), then \\[\\sum_{i=1}^n X_i \\sim \\text{Gamma}(a_1 + ... + a_n, b).\\]\n\\(\\text{Gamma}(n/2,1/2) = \\chi^2(n)\\), the chi-squared distribution with \\(n\\) degrees of freedom.\n\n\n\nExponential Distribution: Memorylessness Property\nThe exponential distribution is a special case of the Gamma distribution.\nThe exponential distribution has the special property that it is the only continuous distribution on \\((0,\\infty)\\) with this property.\nIf \\(X \\sim \\text{Exponential}(\\lambda)\\), then for all \\(x, t > 0\\), \\[P(X > t+x | X>t) = P(X>x).\\]\nThis is the same as the memorylessness property of the Geometric distribution, but in the continuous case.\nMemorylessness will come up again when we study stochastic processes, particularly Poisson processes."
  },
  {
    "objectID": "week5/week5.html#the-log-normal-distribution",
    "href": "week5/week5.html#the-log-normal-distribution",
    "title": "Week 5",
    "section": "The Log-Normal Distribution",
    "text": "The Log-Normal Distribution\nThe \\(\\text{LogNormal}(\\mu, \\sigma^2)\\) distribution has pdf\n\\[p(x | \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\frac{1}{x}\n\\exp\\left(-\\frac{1}{2\\sigma^2}(\\log(x) - \\mu)^2\\right) \\mathbb 1(x \\in \\mathcal X)\\] where \\(\\mathcal X = (0, \\infty)\\). The parameters are \\(\\mu \\in \\mathbb R\\) and \\(\\sigma^2 > 0\\).\nThe mean and variance of \\(X \\sim \\text{LogNormal}(\\mu, \\sigma^2)\\) are \\[EX = \\exp(\\mu + \\frac{1}{2}\\sigma^2)\\] \\[\\text{Var}(X) = \\exp(2\\mu + 2\\sigma^2) - \\exp(2\\mu + \\sigma^2).\\]\n\nLog-Normal Relationships\nIf \\(X \\sim \\text{LogNormal}(\\mu, \\sigma^2)\\), then \\(\\log(X) \\sim \\mathcal N(\\mu, \\sigma^2)\\).\nThe log-normal is very useful for regression models of nonnegative continuous outcomes.\nIn such applications, it is often preferable to parameterize in terms of \\(\\theta = \\log(EX)\\) instead of \\(\\mu = E\\log(X)\\):\n\\[\\theta = \\log(EX) = \\mu + \\frac{1}{2}\\sigma^2.\\]\nWhile the log-normal looks similar to the Gamma distribution, the log-normal tends to be better behaved for regression.\nSuppose \\(X \\sim \\text{Gamma}(a,b)\\) and \\(Y = \\log(X)\\). Then the pdf of \\(Y\\) is asymmetric and has a factor of \\(\\exp(-be^y)\\), which makes it strongly disfavor larger values of \\(y\\).\nJensens Inequality tells us that if we have a convex function like \\(g(X) = -\\log (X)\\), we have that \\[g(EX) \\leq Eg(X).\\]"
  },
  {
    "objectID": "week5/week5.html#beta-distribution",
    "href": "week5/week5.html#beta-distribution",
    "title": "Week 5",
    "section": "Beta distribution",
    "text": "Beta distribution\nThe \\(\\text{Beta}(a,b)\\) distribution with parameters \\(a,b > 0\\) has pdf\n\\[p(x | a,b) = \\frac{1}{B(a,b)}x^{a-1}(1-x)^{b-1} \\mathbb 1(x \\in \\mathcal X),\\]\nwhere \\(\\mathcal X = (0,1)\\) and \\(B(a,b)\\) is the beta function,\n\\[B(a,b) = \\frac{\\Gamma(a)\\Gamma(b)}{\\Gamma(a+b)},\\]\nrecalling that \\(\\Gamma(a) = \\int_0^\\infty t^{a-1}e^{-t} dt\\)$ is the gamma function.\nThe mean and variance of \\(X \\sim \\text{Beta}(a,b)\\) are:\n\\[EX = \\frac{a}{a+b}\\]\n\\[\\text{Var}(X) = \\frac{ab}{(a+b)^2(a+b+1)}.\\]\n\\(\\text{Beta}(a,b)\\) often arises as the distribution of a random variable that is the probability of some event.\nIn Bayesian statistics, \\(\\text{Beta}(a,b)\\) is often used as a prior on probabilities \\(q\\), such as the parameters of Bernoulli, Binomial, Geometric, or Negative Binomial distributions.\nIf \\(X_1 \\sim \\text{Gamma}(a_1, b)\\) and \\(X_2 \\sim \\text{Gamma}(a_2, b)\\) are independent then \\(X_1 / (X_1 + X_2) \\sim \\text{Beta}(a_1, a_2)\\)."
  },
  {
    "objectID": "week5/week5.html#location-scale-families",
    "href": "week5/week5.html#location-scale-families",
    "title": "Week 5",
    "section": "Location-Scale Families",
    "text": "Location-Scale Families\nLocation-scale families are formed by starting from a single standard pdf \\(f(x)\\) and considering all pdfs of the form\n\\[f(x | m, s) = \\frac{1}{s} f\\left( \\frac{x - m}{s} \\right)\\]\nfor \\(m \\in \\mathbb{R}\\) and \\(s > 0\\). The location is \\(m\\) and the scale is \\(s\\).\nIf \\(X\\) has pdf \\(f(x)\\) then \\(sX + m\\) has pdf \\(f(x | m, s)\\) by the change of variable formula.\nIf \\(g(x) = sx + m\\), then \\(g^{-1}(y) = \\frac{y-m}{s}\\) and \\(\\frac{d}{dy}g^{-1} (y) = \\frac{1}{s}.\\)\n\\(\\mathcal N(\\mu, \\sigma^2)\\) is a location scale family with standard pdf \\[f(x) = \\mathcal N(x | 0,1) = \\frac{1}{\\sqrt{2\\pi}} \\exp(-\\frac{1}{2} x^2).\\]\nIf \\(X \\sim \\mathcal N(0,1)\\) then \\(\\sigma X + \\mu \\sim \\mathcal N(\\mu, \\sigma^2)\\)."
  },
  {
    "objectID": "week5/week5.html#laplace-distribution-aka-the-double-exponential",
    "href": "week5/week5.html#laplace-distribution-aka-the-double-exponential",
    "title": "Week 5",
    "section": "Laplace Distribution (aka the Double Exponential)",
    "text": "Laplace Distribution (aka the Double Exponential)\nThe \\(\\text{Laplace}(\\mu, s)\\) distribution has pdf\n\\[p(x | \\mu, s) = \\frac{1}{2s} \\exp\\left( -\\frac{1}{s} |x-\\mu|\\right),\\]\nfor all \\(x \\in \\mathbb R\\). The location is \\(\\mu \\in \\mathbb{R}\\) and the scale is \\(s > 0\\).\n\nlibrary(ggplot2)\nggplot(data.frame(x=seq(-4,4,.1)), aes(x)) + \n  stat_function(fun=\\(x) { (1/2)*exp(-abs(x)) }) + \n  theme_bw() + \n  ylab(\"f(x)\") + \n  ggtitle(\"Laplace Distribution\") \n\n\n\n\n\n\n\n\nThe mean and variance of \\(X \\sim \\text{Laplace}(\\mu, s)\\) are\n\\[EX = \\mu\\]\n\\[\\text{Var}(X) = 2s^2.\\]\nIf \\(X \\sim \\text{Laplace}(0,1)\\) then \\(sX+\\mu \\sim \\text{Laplace}(\\mu, s)\\).\nThe standard Laplace pdf is \\(f(x) = \\frac{1}{2} \\exp(-|x|).\\)\nThe Laplace distribution has heavier tails than the Gaussian but still has finite moments of all order, that is \\(E|X|^k < \\infty\\) for all \\(k \\in \\{ 1, 2, ... \\}\\).\nIt is called the “double exponential” because the Laplace(0,1) pdf is proportional to an Exponential(1) pdf reflected around 0.\nIf \\(X \\sim \\text{Laplace}(\\mu, s)\\) then \\(\\left \\lvert \\frac{X-\\mu}{s}\\right\\rvert \\sim \\text{Exponential}(1)\\)."
  },
  {
    "objectID": "week5/week5.html#cauchy-distribution",
    "href": "week5/week5.html#cauchy-distribution",
    "title": "Week 5",
    "section": "Cauchy Distribution",
    "text": "Cauchy Distribution\nThe Cauchy(m,s) distribution has pdf\n\\[p(x | m,s) = \\frac{1}{\\pi s\\left( 1 + \\left( \\frac{x-m}{s} \\right)^2 \\right)}\\]\nfor all \\(x \\in \\mathbb{R}\\). The location is \\(m \\in \\mathbb{R}\\) and the scale is \\(s > 0\\).\nNo moments of the Cauchy distribution are well-defined.\n\nWhy is it that the tails of the Cauchy distribution are so much larger than the normal distribution?\nThe density of the normal distribution are decaying like \\(\\mathcal O(e^{-x^2})\\), whereas the Cauchy distributions tails are decaying like \\(\\mathcal O(\\frac{1}{x^2})\\). Even worse is the log-Gamma distribution which decays like \\(e^{-e^x}\\).\n\nIf \\(X \\sim \\text{Cauchy}(0,1)\\) then $sX + m (m,s).\nEven though the Cauchy distribution looks roughly similar to a Gaussian, the Cauchy distributions have much heavier tails than Gaussian or Laplace distributions.\nWhat happens if you try to estimate \\(m\\) via the sample mean?\nIf \\(X_1,...,X_n \\sim \\text{Cauchy}(m,s)\\) independently, then\n\\[\\frac{1}{n} \\sum_{i=1}^n X_i \\sim \\text{Cauchy}(m,s).\\]\nIf \\(X,Y \\sim \\mathcal N(0,1)\\) independently, then \\(X/Y \\sim \\text{Cauchy}(0,1)\\)."
  },
  {
    "objectID": "week5/week5.html#exponential-families",
    "href": "week5/week5.html#exponential-families",
    "title": "Week 5",
    "section": "Exponential Families",
    "text": "Exponential Families\nExponential families are a unifying generalization of many common distributions and possess many nice properties.\nExamples include:\n\nBernoulli, Binomial, Poisson, Exponential, Beta, Gamma, Inverse-Gamma, Normal (Gaussian), Multivariate Gaussian, Log-Normal, Inverse Gaussian, Multinomial, Dirichlet\n\nNon-examples include:\n\nUniform, Cauchy, Students’ t-Distribution\n\nA one parameter exponential family is a collection of distributions indexed by \\(\\theta \\in \\Theta\\) with pdfs/pmfs of the form\n\\[p(x \\mid \\theta) = \\exp (\\phi(\\theta) t(x) - \\kappa(\\theta)) h(x)\\]\nfor some real-valued functions \\(\\phi(\\theta), t(x), \\kappa(\\theta),\\) and \\(h(x)\\).\n\\(h(x)\\) has to be non-negative.\n\\(k(\\theta)\\) is a log-normalization constant: since \\(\\int p(x | \\theta) dx = 1\\),\n\\[\\kappa(\\theta) = \\log \\int \\exp (\\phi(\\theta) t(x)) h(x) dx.\\]\n\\(t(x)\\) is called the sufficient statistic.\n\nExamples of One-parameter Exponential Families\n\nThe Exponential Distribution Family\nThe simplest example is the \\(\\text{Exponential}(\\theta)\\) distribution family:\nsince the pdfs are:\n\\[p(x \\mid \\theta) = \\theta e^{-\\theta sx} \\mathbb 1 (x > 0) =\n\\exp (\\phi(\\theta) t(x) - \\kappa(\\theta)) h(x),\\] for \\(\\theta \\in \\Theta = (0,\\infty)\\), where \\(t(x)= -x\\), \\(\\phi(\\theta) = \\theta\\), \\(\\kappa(\\theta) = -\\log(\\theta)\\), and \\(h(x) = \\mathbb 1 (x > 0)\\).\nWe could just as easily move the \\(-\\) sign on \\(t(x) = -x\\) to \\(\\phi(\\theta)\\), so there’s not necessarily a unique choice of \\(\\phi\\) and \\(t\\). There are multiple equivalent formulations. Sometimes people use different notation, such as \\[h(x) c(\\theta) e^{\\phi(\\theta)t(x)},\\]\nwhere \\(c(\\theta) = e^{-\\kappa(\\theta)}\\).\nIn this case we’re just defining \\(t(x)\\) to be the sufficient statistic, but there’s another sense in which a sufficient statistic is information that tells one all the information about a parameter in question — these turn out to be equivalent.\n\n\nThe Poisson Distribution Family\nThe \\(\\text{Poisson}(\\theta)\\) distributions form an exponential family since the pmfs are\n\\[p(x \\mid \\theta) = \\frac{\\theta^x e^{-\\theta}}{x!} \\mathbb 1 \\left(x \\in \\mathcal X\\right) = \\exp (x \\log (\\theta) - \\theta) \\frac{1}{x!} \\mathbb 1\\left(x \\in \\mathcal X\\right) = \\exp (\\phi(\\theta) t(x) - \\kappa(\\theta)) h(x)\\]\nfor \\(\\theta \\in \\Theta = (0,\\infty)\\), where \\(\\mathcal X = \\{0,1,2,...\\}\\), \\(t(x) = x\\), \\(\\phi(\\theta) = \\log(\\theta)\\), \\(\\kappa(\\theta) = \\theta\\), and \\(h(x) = \\frac{1}{x!} \\mathbb 1(x \\in \\mathcal X)\\)."
  },
  {
    "objectID": "week5/week5.html#multi-parameter-exponential-families",
    "href": "week5/week5.html#multi-parameter-exponential-families",
    "title": "Week 5",
    "section": "Multi-Parameter Exponential Families",
    "text": "Multi-Parameter Exponential Families\nAn exponential family is a collection of distributions indexed by \\(\\theta \\in \\Theta\\) with pdfs/pmfs of the form\n\\[p(x \\mid \\theta) = \\exp( \\phi(\\theta) t(x) - \\kappa(\\theta)) h(x)\n\\]\n\nGamma Example\nThe \\(\\text{Gamma}(a,b)\\) distributions, with \\(a, b > 0\\) are an exponential family:\n\\[p(x \\mid \\theta) = \\exp ( \\phi(\\theta)^T t(x) - \\kappa(\\theta)) h(x),\\]\nfor some vector-valued functions\n\\[\\phi(\\theta) = \\begin{pmatrix} \\phi_1(x) \\\\ \\vdots \\\\ \\phi_k(\\theta) \\end{pmatrix} \\quad \\text{ and } \\quad\n\\text{(}x) = \\begin{pmatrix} t_1(x) \\\\ \\vdots \\\\ t_k(x) \\end{pmatrix}\\]\n\\[\\begin{aligned}\n\\text{Gamma}(x \\mid a,b) & = \\frac{b^a}{\\Gamma(a)}x^{a-1} \\exp (-bx) \\mathbb 1(x > 0)  \\\\\n& = \\exp(\\phi(\\theta)^Tt(x) - \\kappa(\\theta)) h(x)\n\\end{aligned}\\]\nwhere \\(\\theta = (a,b)^T\\), \\(\\phi(\\theta) = (-b, a-1)^T\\), \\(t(x) = (x,\\log x)^T\\), and \\(h(x) = \\mathbb 1(x > 0)\\), and since \\(\\kappa(\\theta)\\) is always the normalizing constant, that’s where the \\(\\Gamma(a)\\) term goes.\nSo we’d have to get that \\[\\kappa(\\theta) = -a \\log b + \\log \\Gamma(a),\\] \\[\\text{since } e^{-\\kappa(\\theta)} = \\frac{b^a}{\\Gamma(a)}.\\]\n\n\nExponential Families: Special Properties\nThe entropy of a random variable \\(X\\) is\n\\[H(X) = -\\sum_{x \\in \\mathcal X} p(x) \\log(p(x)) \\quad \\text{ if $X$ is discrete},\\] \\[h(X) = -\\int p(x) \\log (p(x)) dx \\quad \\text{ if $X$ is continuous.}\\]\nExponential families have maximum entropy subject to a linear constraint. More precisely, among all distributions satisfying the constraint that \\(\\mathbb{E}t(X) = \\tau\\) for some \\(\\tau \\in \\mathbb{R}^k\\), an exponential family distribution with sufficient statistic \\(t(x)\\) has maximum entropy.\n\nCan we use this framework to show how the discrete uniform distribution is the maximum entropy distribution on a discrete set?\nYes — in that case, the idea is that there is no constraint \\(t(x)\\), or \\(t(x)\\) could be said to be a zero-dimensional vector.\n\n\n\nOne might be interested in the fact that entropy represents the average minimum number of bits needed to encode a message.\nhttps://en.wikipedia.org/wiki/Entropy_(information_theory)\nhttps://machinelearningmastery.com/what-is-information-entropy/\nhttps://math.stackexchange.com/questions/2299145/why-does-the-average-number-of-questions-bits-needed-for-storage-in-shannons-en\n\nThe rough interpretation is that if all you know is that \\(\\mathbb E t(X) = \\tau\\) then an exponential family distribution makes the fewest assumptions about the rest of that distribution.\nIn Bayesian statistics, exponential families are (incredibly) useful because they admit conjugate priors, facilitating posterior computation."
  },
  {
    "objectID": "week5/week5.html#random-vectors",
    "href": "week5/week5.html#random-vectors",
    "title": "Week 5",
    "section": "Random Vectors",
    "text": "Random Vectors\nIt’s often important to consider multiple random variables at a time, like clinical measurements on height, weight, age, sex, blood pressure, etc. on a set of subjects.\nA random vector is a function from the sample space to a \\(d\\)-dimensional real space.\nSometimes we denote a random vector by a single letter like \\(X\\), like \\[X(s) = (X_1(s), ..., X_d(s)) \\in \\mathbb{R}^d.\\]\nOther times we’ll use different letters for each entry, like \\((X,Y,Z)\\) that takes values \\((X(s), Y(s), Z(s)) \\in \\mathbb{R}^3\\).\nWe’ll return now to the Monty Hall problem to illustrate multiple random variables.\nLet \\(X\\) be the door that the car is behind, and \\(Y\\) is the door that Monty opens.\nThe pair \\((X,Y)\\) is a random vector taking values in \\(\\mathbb{R}^2\\) (or more specifically \\(\\{1,2,3\\} \\times \\{1,2,3\\}\\)).\nSince we chose door #1 at first, our assumptions imply that\n\\[X \\sim \\text{Uniform}(\\{1,2,3\\})\\] \\[P(Y = 2 | X = 1) = P(Y = 3 | X= 1) = 1/2\\] \\[P(Y = 3 | X = 2) = 1\\] \\[P(Y = 2 | X = 3) = 1\\]\nRecall that we wrote down the joint distribution in a table:\n\n\n\n\nMonty Opens Door #1\nOpen #2\nOpen #3\n\n\n\n\nCar is behind #1\n0\n1/6\n1/6\n\n\n… behind #2\n0\n0\n1/3\n\n\n… behind #3\n0\n1/3\n0\n\n\n\nThis table is implied by the assumptions written down above.\nThe probability of the event that \\(X = x, Y = y\\) is denoted \\(P(X = x, Y = y)\\)."
  },
  {
    "objectID": "week5/week5.html#joint-probability-mass-functions",
    "href": "week5/week5.html#joint-probability-mass-functions",
    "title": "Week 5",
    "section": "Joint Probability Mass Functions",
    "text": "Joint Probability Mass Functions\nA random vector is discrete if its range \\(X(S) \\subset \\mathbb{R}^d\\) is countable.\nThe joint pmf of a discrete random vector \\(X = (X_1, ..., X_d)\\) is\n\\[f_X(x) = f_X(x_1, ..., x_d) = P(X_1 = x_1, ..., X_d = x_d)\\]\nfor \\(x = (x_1, ..., x_d) \\in \\mathbb{R}^d\\).\nAs before, it’s common to drop subscripts and or use \\(p\\) instead of \\(f\\), as in:\n\\[p(x,y) = p_{X,Y}(x,y) = f(x,y) = f_{X,Y}(x,y).\\]\nIf \\(X \\in \\mathbb{R}^d\\) is a discrete random vector with range \\(\\mathcal X\\), then\n\\[P(X \\in A) = \\sum_{x \\in \\mathcal X \\cap A} p(x)\\]\n\\[E h(X) = \\sum_{x \\in \\mathcal X} h(x) p(x).\\]\nHere, though, \\(x = (x_1, ..., x_d)\\), so writing out the formulas more explicitly, \\[P((X_1, ..., X_d) \\in A) = \\sum_{(x_1, ..., x_d) \\in \\mathcal X \\cap A} p(x_1, ..., x_d),\\] \\[E h(X_1, ..., X_d) = \\sum_{(x_1, ..., x_d) \\in \\mathcal X} h(x_1, ..., x_d)p(x_1, ..., x_d).\\]\n\nExpected winnings in the Monty Hall problem\nSay the car is worth $30,000 and a goat is worth $150.\nIf you stick with door #1, then your winnings are\n\\[h(x,y) = 30000 \\times \\mathbb 1(x = 1) + 150 \\times \\mathbb 1(x \\neq 1),\\]\nso your expected winnings are\n\\[\\mathbb{E}h(X,Y) = 30000 \\times \\frac{1}{3} + 150 \\times \\frac{2}{3} = 10100.\\]\nIf you always switch doors, then your winnings are\n\\[h(x,y) = 30000 \\times \\mathbb 1 (x \\neq 1) + 150 \\times \\mathbb 1(x=1),\\]\nso your expected winnings are\n\\[\\mathbb{E}h(X,Y) = 30000 \\times \\frac{2}{3} + 150 \\times \\frac{1}{3} = 20050.\\]\nIt’s interesting to note that these expected values only depend on what door the car is behind (\\(X\\)), not what Monty does (\\(Y\\)).\n\nOne might say we’ve “conditioned” on our strategy in these two calculations. One could introduce another random variable \\(Z\\) to indicate our strategic choice, which we would be conditioning on.\n\n\n\nThe expected value of an indicator with respect to a random variable is always the probability of that event."
  },
  {
    "objectID": "week5/week5.html#marginal-probability-mass-functions",
    "href": "week5/week5.html#marginal-probability-mass-functions",
    "title": "Week 5",
    "section": "Marginal Probability Mass Functions",
    "text": "Marginal Probability Mass Functions\nFor a discrete random vector \\((X_1, ..., X_d)\\), the marginal pmf of \\(X_i\\), denoted \\(f_{X_i}(x_i)\\) or \\(p(x_i)\\), is just the pmf of the random variable \\(X_i\\).\nThat is \\[f_{X_i}(x_i) = p(x_i) = P(X_i = x_i).\\]\nWe call it a “marginal pmf” in the context of a joint distribution on multiple variables.\nThe marginal pmfs can be represented in terms of the joint pmf:\n\\[p(x) = \\sum_{y \\in \\mathcal Y} p(x,y)\\]\n\\[p(y) = \\sum_{x \\in \\mathcal X} p(x,y)\\]\nwhere \\(\\mathcal X\\) and \\(\\mathcal Y\\) are the ranges of \\(X\\) and \\(Y\\), respectively.\nIn general for a random vector, \\((X_1, ..., X_d)\\),\n\\[p(x_i) = \\sum_{x_{-i}} p(x_1, ..., x_d)\\]\nwhere the sum is over all values of\n\\[x_{-i} = (x_1, ..., x_{i-1}, x_{i+1}, ..., x_d).\\]\nReturning to the Monty Hall probability table:\n\n\n\n\nMonty Opens Door #1\nOpen #2\nOpen #3\n\n\n\n\nCar is behind #1\n0\n1/6\n1/6\n\n\n… behind #2\n0\n0\n1/3\n\n\n… behind #3\n0\n1/3\n0\n\n\n\nWe can read off the marginal pmfs from the table by summing across rows or columns. This is why they’re called marginal pmfs. And the joint probability for specific combinations of \\((X=x, Y=y)\\) are the individual cell entries."
  },
  {
    "objectID": "week5/week5.html#conditional-probability-mass-functions",
    "href": "week5/week5.html#conditional-probability-mass-functions",
    "title": "Week 5",
    "section": "Conditional Probability Mass Functions",
    "text": "Conditional Probability Mass Functions\nConsider a discrete random vector \\((X,Y)\\). For any \\(y\\) such that \\(p(y) > 0\\), the conditional pmf of \\(X\\) given \\(Y\\) is \\[f_{X|Y}(x \\mid y) = p(x \\mid y) = P(X = x \\mid Y = y) = \\frac{p(x,y)}{p(y)}.\\]\nLikewise for any \\(x\\) such that \\(p(x) > 0\\),\n\\[f_{Y|X}(y \\mid x) = P(Y = y \\mid X = x) = \\frac{p(x,y)}{p(x)}.\\]\nFor a discrete random vector \\((X_1, ..., X_d),\\)\n\\[p(x_i | x_{-i}) = \\frac{p(x_1, ..., x_d)}{p(x_{-i})}.\\]\nThe conditional pmf can be seen in a tabular fashion if we take the table and (un-) normalize it so that each row sums to 1 if we’re interested in conditioning on the row-variables, and vice-versa for the columns.\nIn other words, the conditional pmf \\(p(y|x) = p(x,y) / p(x)\\) can be seen as\n\n\n\n\n\n\n\n\n\n\nMonty Opens Door #1 (\\(Y = 1 | X=x\\))\nOpen #2 (\\(Y = 2 | X = x\\))\nOpen #3 (\\(Y = 3 | X = x\\))\n\n\n\n\nCar is behind #1 (\\(X = 1\\))\n0\n1/2\n1/2\n\n\n… behind #2 (\\(X = 2\\))\n0\n0\n1\n\n\n… behind #3 (\\(X = 3\\))\n0\n1\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\nMonty Opens Door #1 (\\(Y = 1\\))\nOpen #2 (\\(Y = 2\\))\nOpen #3 (\\(Y = 3\\))\n\n\n\n\nCar is behind #1 (\\(X = 1 | Y = y\\))\nundefined\n1/3\n1/3\n\n\n… behind #2 (\\(X = 2 | Y = y\\))\nundefined\n0\n2/3\n\n\n… behind #3 (\\(X = 3 | Y = y\\))\nundefined\n2/3\n0"
  },
  {
    "objectID": "week5/week5.html#joint-marginal-and-conditional-probability-density-functions",
    "href": "week5/week5.html#joint-marginal-and-conditional-probability-density-functions",
    "title": "Week 5",
    "section": "Joint, marginal, and conditional probability density functions",
    "text": "Joint, marginal, and conditional probability density functions\nBasically all the math translates over directly, as long as one isn’t worried about measure-theoretic questions.\nSuppose \\((X,Y)\\) is a bivariate random vector and \\(f(x,y) \\geq 0\\) is a function such that for all measurable \\(A \\subset \\mathbb{R}^2\\),\n\\[P((X,Y) \\in A) = \\int_{-\\infty}^\\infty \\int_{-\\infty}^\\infty\nf(x,y) \\mathbb 1((x,y) \\in A) \\text{d}x \\text{d}y.\\]\nThen \\(f(x,y)\\) is the joint pdf of \\((X,Y)\\).\nMore generally, \\((X_1, ..., X_d)\\) has joint pdf \\(f(x_1, ..., x_d)\\) if \\[P((X_1, ..., X_d) \\in A) = \\int_A f(x_1, ..., x_d) \\text{d}x_1 \\cdots \\text{d}x_d\\]\nfor all measurable \\(A \\subset \\mathbb{R}^d\\). Here \\(\\int_A\\) denotes integration over the set \\(A\\).\nIf such a function \\(f\\) exists, then the random vector is continuous.\nFor a random vector \\((X_1, ..., X_d)\\) with pdf \\(p(x_1, ..., x_d)\\),\n\\[\\mathbb{E}h(X_1, ..., X_d) = \\int h(x_1, ..., x_d) p(x_1, ..., x_d) \\text{d}x_1 \\cdots \\text{d}x_d.\\]\nUsually we write this more compactly as\n\\[\\mathbb{E}h(X) = \\int h(x) p(x) \\text{d}x,\\]\nwhere \\(X = (X_1, ..., X_d)\\) and \\(x = (x_1, ... x_d)\\).\nIf \\((X,Y)\\) is continuous, then \\(X\\) and \\(Y\\) are continuous random variables and their marginal pdfs are just their pdfs as random variables.\nMarginal pdfs can be expressed in terms of the joint pdf:\n\\[f_X(x) = p(x) = \\int p(x,y) \\text{d}y\\] \\[f_Y(y) = p(y) = \\int p(x,y) \\text{d}x\\]\nFor \\(y\\) such that \\(p(y) > 0\\), the conditional pdf of \\(X\\) given \\(Y\\) is defined as\n\\[f_{X|Y}(x|y) = p(x|y) = \\frac{p(x,y)}{p(y)}.\\]\nLikewise, for \\(x\\) such that \\(p(x) > 0\\),\n\\[f_{Y|X}(y|x) = p(y|x) = \\frac{p(x,y)}{p(x)}.\\]\n\nThere is something really subtle going on here: we’re not quite “conditioning” on the probability of the conditioned-variable taking on a particular value, since we’re using the density of the random variable in the denominator."
  },
  {
    "objectID": "week6/week6.html",
    "href": "week6/week6.html",
    "title": "Week 6",
    "section": "",
    "text": "Recap\nMarginal and Conditional pdfs\nIf \\((X,Y)\\) is continuous, then \\(X\\) and \\(Y\\) are continuous random variables and their marginal pfs are just their pdfs as random variables.\nMarginal pdfs can be expressed in terms of the joint pdf:\n\\[\nf_X(x) = p(x) = \\int p(x,y) dy\n\\]\n\\[\nf_Y(y) = p(y) = \\int p(x,y) dx\n\\]\nThis is similar to the discrete case, but with integrals.\nFor \\(y\\) such that \\(p(y) > 0\\), the conditional pdf of \\(X\\) given \\(Y\\) is defined as:\n\\[\nf_{X|Y} (x | y) = p(x|y) = \\frac{p(x,y)}{p(y)}.\n\\]\nLikewise, for \\(x\\) such that \\(p(x) > 0\\),\n\\[\nf_{Y|X}(y|x) = p(y|x) = \\frac{p(x,y)}{p(x)}.\n\\]\nThus, technically the conditional pdf is only defined when the marginal pdf is \\(>0.\\)\nRandom variables \\(X_1, …, X_n\\) are independent if\n\\[\nP(X_1 \\in A_1, ..., X_n \\in A_n) = P(X_1 \\in A_1) \\cdots P(X_n \\in A_n)\n\\]\nfor all (measurable) subsets \\(A_1, …, A_n \\subset \\mathbb R\\).\nIntuitively, independence captures the idea that \\(X_1, …, X_n\\) contain no information about one another.\nIn terms of the pdf/pmf, \\(X_1, …, X_n\\) are independent if\n\\[\nf(x_1, ..., x_d) = f_{X_1}(x_1) \\cdots f_{X_n}(x_d)\n\\]\nfor all \\(x_1, …, x_d\\).\nThis isn’t strictly an if-and-only-if statement. For pdfs, we won’t always be able to factor the pdf doesn’t mean that \\(X_1, …, X_n\\) are independent.\nLet \\(X, Y \\sim \\text{Uniform}(0,1)\\) be independent. Then we have that\n\\[f(x,y) = \\mathbb 1(0 < x,y< 1)\\]\n\\[\nf(x) = \\mathbb 1(0 < x < 1)\n\\]\n\\[\nf(y) = \\mathbb 1(0 < y < 1)\n\\]\n\\[\n\\tilde{f}(x) = \\mathbb 1(0 < x < 1) + \\underline{\\mathbb 1 \\left(x = \\frac{1}{2}\\right)}\n\\]\nThe underlined part adds nothing to the pdf since it occurs with measure zero. We couldn’t necessarily factor \\(f(x,y) = \\tilde{f}(x) f(y)\\).\nThe opposite of the statement above would require an additional caveat of “if there exists \\(f_{X_1}, … f_{X_d}\\).”"
  },
  {
    "objectID": "week6/week6.html#conditional-expectations",
    "href": "week6/week6.html#conditional-expectations",
    "title": "Week 6",
    "section": "Conditional Expectations",
    "text": "Conditional Expectations\nSuppose \\((X,Y)\\) is a random vector and \\(g(x)\\) is a measurable function. The conditional expectation of \\(g(X)\\) given that \\(Y = y\\) is\n\\[\n\\mathbb E(g(X) | Y = y) = \\sum_{x \\in \\mathcal X} g(x) f_{X|Y} (x|y)\n\\]\nin the discrete case and \\[\n\\mathbb E(g(X) | Y=y) = \\int g(x) f_{X|Y} (x|y) dx\n\\]\nin the continuous case.\nOften we abbreviate $\\(\\mathbb E (g(X) | Y = y)\\) as \\(\\mathbb E(g(X)|y)\\).\nAs before \\(\\mathbb{E}(g(X)|y)\\) is defined as long as \\(f(y) \\neq 0\\).\n\nCaveats, Interpretation\nWhen \\(Y\\) is a discrete r.v. the conditional expectation is equivalent to conditioning on \\(Y=y\\) as the notation suggests. However, if \\(Y\\) is a continuous random variable, the interpretation is much more subtle since it requires measure theoretic considerations."
  },
  {
    "objectID": "week6/week6.html#conditional-distributions",
    "href": "week6/week6.html#conditional-distributions",
    "title": "Week 6",
    "section": "Conditional Distributions",
    "text": "Conditional Distributions\nSuppose \\((X,Y)\\) is a random vector.\nThe conditional distribution of \\(X\\) given that \\(Y= y\\) is the probability measure \\(Q\\) such that\n\\[\nQ(A) = \\mathbb{E}\\left( \\mathbb 1 (X \\in A) \\mid Y = y \\right)\n\\]\nfor all measurable sets \\(A\\).\nThe conditional expectation \\(\\mathbb{E}(g(X) | Y = y)\\) can be thought of as the expected value of \\(g(X)\\) under the conditional distribution of \\(X\\) given \\(Y = y\\).\nThat is \\[\n\\mathbb{E}(g(X) | Y = y) = \\mathbb{E}g(\\tilde{X}) \\text{ where } \\tilde{X} \\sim Q.\n\\]\nThis is a very useful way to think of conditional expectations.\n\nBasic Properties\nIn particular,\n\n\\(\\mathbb{E}cg(X) | y) = c \\mathbb{E}(g(X) | y)\\) for any \\(c \\in \\mathbb{R}\\).\n\\(\\mathbb{E}(g(X) + h(X) | y) = \\mathbb{E}(g(X) | y) + \\mathbb{E}(h(X)|y)\\)\n\\(g(x) \\leq h(x) \\to \\mathbb{E}(g(X) | y) \\leq \\mathbb{E}(h(X)|y)\\).\n\n\n\nConditional expectations as random variables\nIt’s often useful to leave the condition as a random variable.\nConsider the function \\(h(y) = \\mathbb{E}(X|Y = y)\\). Then \\(h(Y)\\) is a random variable, denoted \\(\\mathbb{E}(X|Y)\\).\nA key property of conditional expectations is that\n\\[\n\\mathbb{E}(g(Y) X | Y) = g(Y) \\mathbb{E}(X|Y)\n\\]\nThe basic idea here is that if we were to condition on a particular value of \\(Y\\), then \\(g(Y)\\) would just be constant on the left-hand-side of the expectation.\n\n\nLaw of Total Expectation & Law of Total Variance\nConditional expectations follow two very useful properties:\nThe law of total expectation\n\\[\n\\mathbb{E}X = \\mathbb{E}(\\mathbb{E}(X \\mid Y))\\]\nAnd the law of total variance:\n\\[\n\\text{Var}X = \\mathbb{E}( \\text{Var}X \\mid Y ) + \\text{Var}( \\mathbb{E}(X \\mid Y ))\\]\n\nExercise: Derive these two properties.\nNote that \\[ \\mathbb{E}(\\mathbb{E}(X \\mid Y)) = \\mathbb{E}\\left(\\sum x f_{X|Y} (x|y)\\right)\n\\]\n\\[ \\mathbb{E}(\\mathbb{E}(X \\mid Y)) = \\sum_{y \\in \\mathcal Y} \\left(\\sum_{x \\in \\mathcal X} x f_{X|Y} (x|y)\\right) \\cdot P(Y = y)\n\\]\nIf I recall correctly, \\(P(X|Y) = P(X,Y)/P(Y)\\).\n\\[ \\mathbb{E}(\\mathbb{E}(X \\mid Y)) = \\sum_{y \\in \\mathcal Y} \\left(\\sum_{x \\in \\mathcal X} x P(X = x, Y = y)\\right)\n\\]\nInterchange ordering of summation:\n\\[ \\mathbb{E}(\\mathbb{E}(X \\mid Y)) = \\sum_{x \\in \\mathcal X} \\left(\\sum_{y \\in \\mathcal Y} x P(X = x, Y = y)\\right)\n\\]\nPull out the \\(x\\) from the inner sum:\n\\[ \\mathbb{E}(\\mathbb{E}(X \\mid Y)) = \\sum_{x \\in \\mathcal X} x \\left(\\sum_{y \\in \\mathcal Y} P(X = x, Y = y)\\right)\n\\]\n\\[ \\mathbb{E}(\\mathbb{E}(X \\mid Y)) = \\sum_{x \\in \\mathcal X} \\left( x P(X = x) \\right)\n\\]\n\\[ \\mathbb{E}(\\mathbb{E}(X \\mid Y)) = \\mathbb{E}X\n\\]\n\nIs there a way to apply\n\\[\n\\text{Var}X = \\mathbb{E}X^2 - (\\mathbb{E}X)^2 \\quad ?\n\\]\n\\[\n\\text{Var}X = \\mathbb{E}X^2 - (\\mathbb{E}(\\mathbb{E}(X \\mid Y)))^2\n\\]\n\n\n\nAnswers. Recall our definition:\n\\[\n\\mathbb{E}(X \\mid Y) = h(Y) \\quad \\text{where} \\quad h(y) = \\mathbb{E}(X | Y = y).\n\\]\nInsert them into the problem stem:\n\\[\n\\begin{aligned}\n\\mathbb{E}( \\mathbb{E}(X \\mid Y)) & = \\mathbb{E}h(Y) = \\sum_{y} h(y) p(y)  \\\\\n& = \\sum_y \\mathbb{E}(X \\mid Y = y) p(y) = \\sum_y \\sum_x x p(x \\mid y) p(y) \\\\\n& = \\sum_y \\sum_x x p(x,y) = \\sum_x x \\sum_y p(x,y) \\\\\n& = \\sum_x x p(x) = \\mathbb{E}X.\n\\end{aligned}\n\\]\nAside: How would one know what the expectation is with respect to in the outer expectations? The expectation is always with respect to whatever is random in the argument.\nAside #2: Isn’t the \\(h(Y)\\) notation a little bit confusing? What’s the intuition? Well, one can call \\(h(Y)\\) the “average value of \\(X\\) at a given level of \\(Y\\)”.\nMaybe a more intuitive way to look at it is to think about the usual setting where we’re regressing \\(Y\\) on \\(X\\)."
  },
  {
    "objectID": "week7/week7.html",
    "href": "week7/week7.html",
    "title": "Week 7",
    "section": "",
    "text": "Recap\nRecall that if \\((X,Y)\\) is a random vector and \\(g(x)\\) is a measurable function. The conditional expectation of \\(g(X)\\) given that \\(Y=y\\) is\n\\[\n\\mathbb E(g(X) | Y=y) = \\int_{x \\in \\mathcal X} g(x) f_{X|Y}(x|y) dx\n\\]\nWe have to be a bit careful with the notation \\(\\mathbb E[X|Y=y]\\) which might lead you to believe we’re conditioning on the probability that \\(Y=y\\), but the probability \\(Y=y\\) is vanishingly small (0), though there is a density for the probability distribution of \\(Y\\) at \\(y\\).\nSuppose the heights of adult women and men are \\(\\mathcal N(\\mu_1, \\sigma_1^2)\\) and \\(\\mathcal N(\\mu_2, \\sigma_2^2)\\), respectively.\nThen the pdf of heights for women and men combined is\n\\[\\pi \\mathcal N(\\mu_1, \\sigma_1^2) + (1-\\pi) \\mathcal N(\\mu_2, \\sigma_2^2)\n\\]\nwhere \\(\\pi\\) is the proportion of women in the population.\nThis is called a mixture of Gaussians.\nMore generally, mixture distributions are obtained by taking a weighted sum or integral of pdfs/pmfs.\nMixtures are a useful way of enrichiing a simple family of distributions."
  },
  {
    "objectID": "week7/week7.html#independence-and-dependence",
    "href": "week7/week7.html#independence-and-dependence",
    "title": "Week 7",
    "section": "Independence and Dependence",
    "text": "Independence and Dependence\n\\(X\\) and \\(Y\\) are independent if and only if for all (measurable) functions \\(g(x)\\) and \\(h(y)\\),\n\\[\n\\mathbb{E}(g(X)h(Y)) = \\mathbb{E}(g(X))\\mathbb{E}(h(Y))\n\\]"
  },
  {
    "objectID": "week7/week7.html#sums-of-gaussians",
    "href": "week7/week7.html#sums-of-gaussians",
    "title": "Week 7",
    "section": "Sums of Gaussians",
    "text": "Sums of Gaussians\nThis property of mgfs can simplify derivations of the distribution of a sum of independent random variables.\nThe mgf of \\(X \\sim \\mathcal N(\\mu, \\sigma^2)\\) is\n\\[\nM_X(t) = \\exp(\\mu t + \\frac{1}{s} \\sigma^2 t^2).\n\\]\nSuppose \\(X_1 \\sim \\mathcal N(\\mu_1, \\sigma^2_1)\\) and \\(X_2 \\sim \\mathcal N(\\mu_2, \\sigma^2_2)\\) independently. Then\n\\[\nM_{X_1 + X_2}(t) = M_{X_1}(t)M_{X_2}(t)\n\\]\n\\[\n=\\exp\\left( (\\mu_1 + \\mu_2) t + \\frac{1}{2}(\\sigma_1^2 + \\sigma^2_2) t^2\\right),\n\\]\nwhich is the mgf of \\(\\mathcal N(\\mu_1 + \\mu_2, \\sigma^2_1 + \\sigma^2_2)\\) for all \\(t\\).\nRecall that if two variables have the mgfs that are equal and finite around some interval of the origin, then they are the variables are equal in distribution. (It turns out that if two mgfs are finite and equal on a neighborhood around zero, they must be equal everywhere).\nTherefore we’re allowed to make the jump that \\[\nY \\sim \\mathcal N(\\mu_1 + \\mu_2, \\sigma_1^2 + \\sigma_2^2) \\Rightarrow X_1 + X_2 \\stackrel{d}{=} Y.\n\\]"
  },
  {
    "objectID": "week7/week7.html#conditional-expectations",
    "href": "week7/week7.html#conditional-expectations",
    "title": "Week 7",
    "section": "Conditional Expectations",
    "text": "Conditional Expectations\nRecall that if \\((X,Y)\\) is a random vector and \\(g(x)\\) is a measurable function. The conditional expectation of \\(g(X)\\) given that \\(Y=y\\) is\n\\[\n\\mathbb E(g(X) | Y=y) = \\int_{x \\in \\mathcal X} g(x) f_{X|Y}(x|y) dx\n\\]\nWe have to be a bit careful with the notation \\(\\mathbb E[X|Y=y]\\) which might lead you to believe we’re conditioning on the probability that \\(Y=y\\), but the probability \\(Y=y\\) is vanishingly small (0), though there is a density for the probability distribution of \\(Y\\) at \\(y\\).\n\nLaw of Total Expectation & Law of Total Variance\n\n\\(\\mathbb{E}X = \\mathbb{E}(\\mathbb{E}(X \\mid Y))\\)\n\\(\\text{Var}X = \\mathbb{E}(\\text{Var}(X \\mid Y)) = \\text{Var}(\\mathbb{E}(X\\mid Y))\\)\n\n\nLooking at claim 2, let’s expand the two terms on the right:\n\\[\n\\begin{aligned}\n\\mathbb{E}(\\text{Var}(X \\mid Y)) & = \\mathbb{E}(\\mathbb{E}(X^2 \\mid Y ) - \\mathbb{E}(X \\mid Y)^2) \\\\\n& = \\mathbb{E}(\\mathbb{E}(X^2 \\mid Y)) - \\mathbb{E}(\\mathbb{E}(X \\mid Y)^2) \\\\\n& = \\mathbb{E}(X^2) - \\mathbb{E}(\\mathbb{E}(X \\mid Y)^2) \\quad \\text{ by Law of Total Expectation}\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n\\text{Var}(\\mathbb{E}(X \\mid Y)) & = \\mathbb{E}(\\mathbb{E}(X \\mid Y)^2) - \\mathbb{E}(\\mathbb{E}(X \\mid Y))^2 \\\\\n& = \\mathbb{E}(\\mathbb{E}(X \\mid Y)^2) - (\\mathbb{E}X)^2 \\quad \\text{ by Law of Total Expectation}\n\\end{aligned}\n\\]\nTherefore,\n\\[\n\\begin{aligned}\n\\mathbb{E}(\\text{Var}(X \\mid Y)) + \\text{Var}(\\mathbb{E}(X \\mid Y)) & = \\mathbb{E}X^2 - (\\mathbb{E}X)^2 \\\\\n& = \\text{Var}X.\n\\end{aligned}\n\\]\n\n\nWe define conditional expectation before we define conditional probability. Why? It seems almost backwards?\nThere is measure-theoretic difficulty with continuous distributions. There are paradoxes that can come up if this is done too naively. This is called the Borel paradox.\nThere was one study in the 1990s on tracking populations of fish, but it was based on a faulty construction of a conditional distribution due to the Borel paradox."
  },
  {
    "objectID": "week7/week7.html#law-of-total-expectation-law-of-total-variance",
    "href": "week7/week7.html#law-of-total-expectation-law-of-total-variance",
    "title": "Week 7",
    "section": "Law of Total Expectation & Law of Total Variance",
    "text": "Law of Total Expectation & Law of Total Variance\n\n\\(\\mathbb{E}X = \\mathbb{E}(\\mathbb{E}(X \\mid Y))\\)\n\\(\\text{Var}X = \\mathbb{E}(\\text{Var}(X \\mid Y)) = \\text{Var}(\\mathbb{E}(X\\mid Y))\\)\n\n\nLooking at claim 2, let’s expand the two terms on the right:\n\\[\n\\begin{aligned}\n\\mathbb{E}(\\text{Var}(X \\mid Y)) & = \\mathbb{E}(\\mathbb{E}(X^2 \\mid Y ) - \\mathbb{E}(X \\mid Y)^2) \\\\\n& = \\mathbb{E}(\\mathbb{E}(X^2 \\mid Y)) - \\mathbb{E}(\\mathbb{E}(X \\mid Y)^2) \\\\\n& = \\mathbb{E}(X^2) - \\mathbb{E}(\\mathbb{E}(X \\mid Y)^2) \\quad \\text{ by Law of Total Expectation}\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n\\text{Var}(\\mathbb{E}(X \\mid Y)) & = \\mathbb{E}(\\mathbb{E}(X \\mid Y)^2) - \\mathbb{E}(\\mathbb{E}(X \\mid Y))^2 \\\\\n& = \\mathbb{E}(\\mathbb{E}(X \\mid Y)^2) - (\\mathbb{E}X)^2 \\quad \\text{ by Law of Total Expectation}\n\\end{aligned}\n\\]\nTherefore,\n\\[\n\\begin{aligned}\n\\mathbb{E}(\\text{Var}(X \\mid Y)) + \\text{Var}(\\mathbb{E}(X \\mid Y)) & = \\mathbb{E}X^2 - (\\mathbb{E}X)^2 \\\\\n& = \\text{Var}X.\n\\end{aligned}\n\\]\n\n\n\nWe define conditional expectation before we define conditional probability. Why? It seems almost backwards?\nThere is measure-theoretic difficulty with continuous distributions. There are paradoxes that can come up if this is done too naively. This is called the Borel paradox.\n\nWikipedia article\nPhilosophical ramifications\nSlides\nWriteup\nPreprint\n\nThere was one study in the 1990s on tracking populations of fish, but it was based on a faulty construction of a conditional distribution due to the Borel paradox.\n\n\nExample: Compound Distributions\nSay you roll a die and it comes up \\(Y\\). Then you roll the die \\(Y\\) times, getting outcomes \\(X_1,...,X_Y\\). What is \\(\\mathbb{E}(\\sum_{i=1}^Y X_i)?\\).\nThis is two-stage procedure leads to something we call a compound distribution, which are also called mixture models or hierarchical model.\n\\[\n\\begin{aligned}\n\\mathbb{E}\\left( \\sum_{i=1}^Y X_i \\right) & = \\mathbb{E}\\left( \\mathbb{E}\\left( \\sum_{i=1}^Y X_i \\mid Y \\right) \\right) \\\\\n& = \\mathbb{E}\\left( \\sum_{i=1}^Y \\mathbb{E}(X_i \\mid Y ) \\right)  \\\\\n& = \\mathbb{E}(3.5 Y) = 3.5^2.\n\\end{aligned}\n\\]\n\\(Y\\) itself is a discrete uniform on \\(\\{1,...,6\\}\\) and each \\(X_i\\) is also a discrete uniform on \\(\\{1,...,6\\}\\), but the composition of the two is not.\nThe sum of a random number of random variables is said to follow a compound distribution.\n\n\nExample 2\nSuppose \\(N \\sim \\text{Poisson}(\\lambda)\\) and \\(X_1,...,X_N \\sim \\text{Geometric}(q)\\) independently given \\(N\\). Let \\(S = \\sum_{i=1}^N X_i\\).\nBy the law of total expectation\n\\[\\mathbb{E}S = \\mathbb{E}(\\mathbb{E}(S \\mid N)) = \\mathbb{E}( N / q) = \\lambda / q.\n\\]\nRemember that if two variables are independent, then \\(\\text{Var}(X + Y) = \\text{Var}(X) + \\text{Var}(Y)\\), which is also true of conditional variances.\nSo we can say that\n\\[\n\\text{Var}(S \\mid N) = \\sum_{i=1}^N \\text{Var}(X_i \\mid N) = N \\frac{1-q}{q^2}.\n\\]\nTherefore, by the law of total variance,\n\\[\n\\begin{aligned}\n\\text{Var}S & = \\mathbb{E}(\\text{Var}(S \\mid N)) + \\text{Var}(\\mathbb{E}(S \\mid N)) \\\\\n& = \\mathbb{E}(N(1-q)/q^2) + \\text{Var}(N/q) \\\\\n& = \\lambda(1-q)/q^2 + \\lambda/q^2 \\\\\n& = \\lambda(2-q)/q^2.\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "week7/week7.html#the-exchange-paradox",
    "href": "week7/week7.html#the-exchange-paradox",
    "title": "Week 7",
    "section": "The Exchange Paradox",
    "text": "The Exchange Paradox\nAt a carnival, there is a mysterious man with two envelopes on the table in front of him. He tells you that one of the envelopes has twice as much money as the other, and you can pick one. You randomly pick one envelope and find \\(x\\) dollars inside. Now, he says, you can keep that money or take the money in the other envelope.\nYour friend says you should switch, because you will either get \\(x/2\\) or \\(2x\\) dollars, each with probability 1/2, so the expected value of switching is\n\\[\n(1/2)(x/2) + (1/2)(2x) = 5x/4\n\\]\nWhat would you do?\n\nThe problem is that what’s in the envelope is not a random quantity.\nWe want to evaluate\n\\[\n\\mathbb{E}(\\text{How Much Money You'd Get} \\mid \\text{You Switch})\n\\]\nAnd we want to see if this is the same, larger, or smaller than:\n\\[\n\\mathbb{E}(\\text{How Much Money You'd Get} \\mid \\text{You Don't Switch})\n\\]\nLet \\(M\\) be the amount of money in the lower value envelope.\nIf you first pick the envelope with the higher amount, and you switch you’d get \\(M\\) money, and if you pick the envelope with the lower amount and switch, you’d get \\(2M\\) money.\n\\[\n\\mathbb{E}(\\text{How Much Money You'd Get} \\mid \\text{You Switch}) = (1/2)M + (1/2)2M\n\\]\nIf you pick the envelope with the higher amount and you don’t switch, then you get \\(2M\\) and you don’t …\n\\[\n\\mathbb{E}(\\text{How Much Money You'd Get} \\mid \\text{You Don't Switch}) = (1/2)M + (1/2)2M\n\\]\n\n\n\nLet \\(X =\\) the amount in the envelope selected and \\(Y =\\) amount in the other envelope.\nLet \\(m\\) and \\(2m\\) denote the (unknown) amount of money in the two envelopes.\nThen \\[P(Y = 2m \\mid X = m) = 1\n\\] \\[P(Y = m \\mid X = 2m) = 1\n\\]\n\\[P(X = m) = P(X = 2m) = 1/2.\\]\nThus by law of total expectation\n\\[\n\\begin{aligned}\n\\mathbb{E}Y & = \\mathbb{E}(\\mathbb{E}(Y \\mid X))  \\\\\n& = \\mathbb{E}(Y \\mid X=m)P(X=m) + \\mathbb{E}(Y \\mid X = 2m) P(X = 2m) \\\\\n& = (2m)(1/2) + m(1/2) = 3m/2.\n\\end{aligned}\n\\]\nMeanwhile,\n\\[\\mathbb{E}X = mP(X = m) + 2m P(X = 2m) = 3m/2.\n\\]\nThere is no advantage to switching.\nMorals: Write down your assumptions carefully, and realize that your intuition can lead you astray."
  },
  {
    "objectID": "week7/week7.html#discrete-case",
    "href": "week7/week7.html#discrete-case",
    "title": "Week 7",
    "section": "Discrete Case",
    "text": "Discrete Case\nSuppose \\((U,V) = g(X,Y)\\) for some function \\(g\\).\nIf \\((X,Y)\\) is discrete, then the joint pmf of \\((U,V)\\) is\n\\[f_{U,V}(u,v) = sum_{x,y} f_{X,Y} (x,y) \\mathbb 1(g(x,y) = (u,v)).\n\\]\nThis is really just the same as the univariate case, except that we are considering bivariate rather than univariate elements."
  },
  {
    "objectID": "week7/week7.html#continuous-case",
    "href": "week7/week7.html#continuous-case",
    "title": "Week 7",
    "section": "Continuous Case",
    "text": "Continuous Case\nIt is less obvious how to handle the case where \\((X,Y)\\) is continuous. Fortunately, however, there is still a nice formula.\nSuppose \\((X,Y)\\) is a continuous random vector, and \\((U,V) = g(X,Y)\\) for some function \\(g\\) such that\n\n\\(g\\) is one-to-one, with inverse \\(h(u,v) = (x,y)\\) on its range,\nthe partial derivatives of \\(g(x,y)\\) exist and are continuous\nthe Jacobian matrix \\(Dh\\) is nonsingular, where\n\n\\[Dh = \\begin{bmatrix}\n  \\frac{\\partial x}{\\partial u} & \\frac{\\partial x}{\\partial v} \\\\\n  \\frac{\\partial y}{\\partial u} & \\frac{\\partial y}{\\partial v}\n  \\end{bmatrix} =\n  \\begin{bmatrix}\n  \\frac{\\partial h_1}{\\partial u} & \\frac{\\partial h_1}{\\partial v} \\\\\n  \\frac{\\partial h_2}{\\partial u} & \\frac{\\partial h_2}{\\partial v}\n  \\end{bmatrix}.\n\\]\nThe joint pdf of \\((U,V)\\) is\n\\[f_{U,V}(u,v) = f_{X,Y}(h_1(u,v), h_2(u,v)) \\lvert \\det (Dh) \\rvert\n\\]\nfor \\((u,v)\\) in the range of \\(g(X,Y)\\) and is zero elsewhere.\n\\(g(x,y)\\) being one-to-one means that if \\((x,y) \\neq (x',y')\\) then \\(g(x,y) \\neq g(x',y')\\). A one-to-one function has a inverse from its range back to its domain.\nThe determinant factor is\n\\[\\lvert \\det (Dh) \\rvert = \\lvert \\frac{\\partial h_1}{\\partial u} \\frac{\\partial h_2}{\\partial v} - \\frac{\\partial h_1}{\\partial v} \\frac{\\partial h_2}{\\partial u} \\rvert.\n\\]\nSometimes \\(\\det (Dh)\\) is referred to as the Jacobian of \\(h\\), often denoted \\(J_h\\) or simply \\(J\\). The notation is not totally standard though, and sometimes \\(J_h\\) denotes the matrix \\(Dh\\)."
  },
  {
    "objectID": "week8/week8.html",
    "href": "week8/week8.html",
    "title": "Week 8",
    "section": "",
    "text": "Recap\nSuppose \\((X,Y)\\) is a random vector, and \\((U,V) = g(X,Y)\\) for some function \\(g\\).\nThat is \\(U = g_1(X,Y)\\) and \\(V = g_2(X,Y)\\).\nHow can we derive the joint pdf/pmf of \\((U,V)\\) from the joint pdf/pmf of \\((X,Y)\\)?\nThe discrete case is pretty straightforward, but in the continuous case we need to get comfortable with the Jacobian matrix.\nSuppose \\((X,Y)\\) is a continuous random vector and \\((U,V) = g(X,Y)\\) for some function \\(g\\) such that\n\\[Dh = \\begin{bmatrix}\n  \\frac{\\partial x}{\\parital u} & \\frac{ \\partial x}{\\partial v} \\\\\n  \\frac{\\partial y}{\\partial u} & \\frac{\\partial y}{\\partial v}\n  \\end{bmatrix} =\n  \\begin{bmatrix}\n  \\frac{\\partial h_1}{\\parital u} & \\frac{ \\partial h_1}{\\partial v} \\\\\n  \\frac{\\partial h_2}{\\partial u} & \\frac{\\partial h_2}{\\partial v}\n  \\end{bmatrix}\n  \\]\nThe determinant factor is \\[|\\det Dh| = \\lvert \\frac{\\partial h_1 }{\\partial u} \\frac{\\partial h_2}{\\partial v} - \\frac{\\partial h_1}{\\partial v} \\frac{\\partial h_2}{\\partial u } \\rvert.\n\\]\nSuppose \\(X,Y \\sim \\mathcal N(0,1)\\) independently and \\(\\rho \\in (-1, 1)\\).\nDefine \\(U = X\\) and \\(V = \\rho X + \\sqrt{1 - \\rho^2}Y\\). That is\n\\[u = g_1(x,y) = x\n\\]\n\\[v = g_2(x,y) = \\rho x + \\sqrt{1 - \\rho^2} y.\n\\]\nThe inverse is defined by\n\\[x = h_1(u,v) = u\n\\] \\[y = h_2(u,v) = \\frac{v - \\rho u }{\\sqrt{1-\\rho^2}}.\n\\]\nThus the Jacobian matrix is\n\\[\nDh =\n  \\begin{bmatrix}\n  \\frac{\\partial h_1}{\\partial u} & \\frac{ \\partial h_1}{\\partial v} \\\\\n  \\frac{\\partial h_2}{\\partial u} & \\frac{\\partial h_2}{\\partial v}\n  \\end{bmatrix} =\n  \\begin{bmatrix}\n  1 & 0 \\\\\n  -\\frac{\\rho}{\\sqrt{1 - \\rho^2}} & \\frac{1}{\\sqrt{1 - \\rho^2}}\n  \\end{bmatrix}.\n\\]\n\\[\\det Dh = \\frac{1}{\\sqrt{1 - \\rho^2}}.\n\\]\n\\[\n\\begin{aligned}\nf_{X,Y}(x,y) & = \\frac{1}{\\sqrt{2\\pi}}\\exp \\biggr ( -\\frac{1}{2} y^2\\biggr ) \\\\\n& = \\frac{1}{\\sqrt{2\\pi}}\\exp \\biggr ( -\\frac{1}{2} u^2\\biggr ) \\exp \\biggr ( - \\frac{1}{2} \\left( \\frac{v - \\rho u}{\\sqrt{1-\\rho^2}} \\right)^2 \\biggr )\n\\biggr \\lvert \\det Dh \\biggr \\rvert \\\\\n& = \\frac{1}{2\\pi \\sqrt{1-\\rho^2}}\\exp \\biggr ( -\\frac{1}{2} u^2 - \\frac{1}{2} \\left( \\frac{v^2 - 2v \\rho u + \\rho^2 u^2}{1-\\rho^2} \\right) \\biggr ) \\\\\n& = \\frac{1}{2\\pi \\sqrt{1-\\rho^2}} \\exp \\biggr ( -\\frac{1}{2 (1 - \\rho^2)} \\left( u^2 - 2\\rho uv + v^2 \\right) \\biggr ).\n\\end{aligned}\n\\]\nThe bivariate normal distribution with means \\(\\mu_X\\) and \\(\\mu_Y \\in \\mathbb R\\), variances \\(\\sigma_X^2, \\sigma_Y^2 > 0\\), and correlation \\(\\rho \\in (-1,1)\\) has pdf\n\\[f(x,y) = \\frac{1}{2 \\pi \\sigma_X \\sigma_Y \\sqrt{1-\\rho^2}} \\exp \\left(\n-\\frac{1}{2 (1-\\rho^2)} ( \\tilde x^2 - 2 \\rho \\tilde x\\tilde y + \\tilde y ^2)\n  \\right),\n\\]\nwhere\n\\[\\tilde x = \\frac{x - \\mu_X}{\\sigma_X} \\quad \\quad \\text{ and } \\quad \\quad \\tilde y = \\frac{y - \\mu_Y}{\\sigma_Y}\n\\]\nIf \\((X,Y)\\) have this bivariate normal distribution, then\n\\[\n\\begin{aligned}\n\\mathbb{E}(Y | X = x) & = \\mu_Y + \\rho \\sigma_Y \\frac{x - \\mu_X}{\\sigma X} \\\\\n\\text{Var}(Y | X = x ) & = (1 - \\rho^2) \\sigma_Y^2.\n\\end{aligned}\n\\]\nIn other words, the conditional pdf of \\(Y\\) given \\(X =x\\) is\n\\[p(y\\mid x) = \\mathcal N \\left( y \\mid \\mu_y + \\rho \\sigma_Y \\frac{x - \\mu_X}{\\sigma_X}, (1-\\rho^2) \\sigma_Y^2 \\right).\n\\]\nIf \\(X\\) and \\(Y\\) are each normally distributed, then \\((X,Y)\\) is not necessarily bivariate normal.\nOne example is where \\(\\rho = 1\\) or \\(\\rho = -1\\) since this violates the constraint that \\(\\rho \\in (-1,1)\\).\nBut what about a more satisfying counterexample?\nLet’s say \\(X \\sim \\mathcal N(0,1)\\) and\n\\[Y = \\left\\{ \\begin{array}{ll}\nX \\quad & \\text{ if } |X| \\leq 1 \\\\\n-X \\quad & \\text{ if } |X| > 1\n\\end{array} \\right. .\n\\]\nWe defined the bivariate normal distribution in term of its pdf, but there is a more general definition that we will use.\nDefinition: We will say that \\((X,Y)\\) is bivariate normal if \\(aX + bY\\) is normally distributed for \\(a,b \\in \\mathbb R\\).\nThis definition applies in the degenerate scenario where \\(\\rho = 1\\) or -1."
  },
  {
    "objectID": "week8/week8.html#correlation-and-covariance",
    "href": "week8/week8.html#correlation-and-covariance",
    "title": "Week 8",
    "section": "Correlation and Covariance",
    "text": "Correlation and Covariance\nWhen random variables \\(X\\) and \\(Y\\) are not independent, they are dependent. However, the dependence may be weak, or it may be strong. Correlation is an important way of quantifying the dependence between random variables. Covariance is a related concept that also depends on the scales of \\(X\\) and \\(Y\\).\nTo declutter the notation, let’s denote\n\\[\n\\begin{aligned}\n\\mu_X = \\mathbb{E}X \\quad & \\quad \\sigma_x = \\sqrt{\\text{Var}X } \\\\\n\\mu_Y = \\mathbb{E}Y \\quad & \\quad \\sigma_y = \\sqrt{\\text{Var}Y }\n\\end{aligned}\n\\]\nThe covariance of \\(X\\) and \\(Y\\) is\n\\[\n\\text{Cov}(X,Y) = \\mathbb{E}\\bigg( (X - \\mu_X) ( Y - \\mu_Y )\\bigg).\n\\]\nThe correlation of \\(X\\) and \\(Y\\) is\n\\[\n\\begin{aligned}\n\\rho_{X,Y} = \\text{Cor}(X,Y) & = \\frac{\\text{Cov}(X,Y)}{\\sigma_X \\sigma_Y} \\\\\n& = \\mathbb{E}\\left( \\left(\\frac{X - \\mu_X }{\\sigma_X}\\right) \\left(\\frac{Y - \\mu_Y}{\\sigma_Y}\\right) \\right).\n\\end{aligned}\n\\]\nThis is sometimes called the correlation coefficient, or Pearson’s correlation coefficient.\n\n\n\n\nExample of correlations from Wikimedia\n\n\nNote that in the center figure, division by zero (with \\(\\sigma_Y = 0\\)) implies that the correlation is undefined. Notably, that scenario would have covariance.\nIt’s important to acknowledge that correlation is not going to relate necessarily to the slope of a regression line because we are standardizing by \\(\\sigma_Y\\).\nProperties of \\(-1 \\leq \\rho_{X,Y} \\leq 1\\).\n\\(\\rho_{X,Y} > 0\\) implies a positive association (direct relationship); \\(\\rho_{X,Y} < 0\\) implies a negative association.\nIf \\(\\sigma_X = 0\\) or \\(\\sigma_Y = 0\\), then \\(\\rho\\) is undefiend.\n\\(|\\rho_{X,Y}| = 1\\) if and only if there exists \\(a \\neq 0\\) and \\(b \\in \\mathbb R\\) such that \\(P(Y = aX+b) = 1\\). The sign of \\(\\rho\\) equals the sign of \\(a\\).\nCorrelation captures the strength of the association in terms of how close to linear the relationship is, but not the magnitude of the slope.\nIf \\(X\\) and \\(Y\\) are independent, then \\(\\rho_{X,Y} = 0\\). However, if \\(\\rho_{X,Y} = 0\\) then \\(X\\) and \\(Y\\) are not necessarily independent.\n\nIf we want to show that independence \\(\\to \\rho = 0\\), we can see that by writing that we want to show:\n\\[\\text{Cov}(X,Y) = \\mathbb{E}\\biggr( (X - \\mu_X) (Y - \\mu_Y) \\biggr) = 0.\n\\]\nBecause \\(X\\) and \\(Y\\) are independent, and the expectation is over a product of a piece that’s just a function of \\(X\\) and another piece that’s just a function of \\(Y\\), we can write that\n\\[\n\\text{Cov}(X,Y) = \\mathbb{E}\\biggr( X - \\mu_X \\biggr)\\mathbb{E}\\biggr( Y - \\mu_Y \\biggr) = 0 \\times 0 = 0.\n\\]\n\\[\n\\Longrightarrow \\text{Cor}(X,Y) = \\rho_{X,Y} = 0\n\\]\n\nProperties of covariance\n\n\\(\\text{Cov}(X,Y) = \\rho_{X,Y} \\sigma_X \\sigma_Y\\)\n\\(\\text{Cov}(X,Y) = \\mathbb{E}XY - (EX)(EY)\\)\n\\(\\text{Cov}(X,X) = \\text{Var}X\\)\n\\(\\text{Cov}(aX + b, cY+d) = ac \\text{Cov}(X,Y)\\)\n\\(\\text{Var}(X + Y) = \\text{Var}(X) + \\text{Var}(Y) + 2 \\text{Cov}(X,Y)\\)\n\n\nShowing 1:\nObserve that \\(\\rho_{X,Y} = \\text{Cov}(X,Y) / (\\sigma_X \\sigma_Y)\\) implies this.\nShowing 2: We can write that\n\\[\n\\begin{aligned}\n\\text{Cov}(X,Y) & = \\mathbb{E}(XY - \\mu_X Y - \\mu_Y X + \\mu_X \\mu_Y) \\\\\n& = \\mathbb{E}(XY) - \\mathbb{E}(X) \\mathbb{E}(Y) - \\mathbb{E}(Y) \\mathbb{E}(X) + \\mathbb{E}(X)\\mathbb{E}(Y) \\\\\n& = \\mathbb{E}(XY) - \\mathbb{E}(X) \\mathbb{E}(Y).\n\\end{aligned}\n\\]\nShowing 3:\n\\[\\text{Cov}(X,X) = \\mathbb{E}((X - \\mu_X)(X-\\mu_X)) = \\mathbb{E}((X-\\mu_X)^2) = \\text{Var}(X).\n\\]\nShowing 5:\n\n\\[\n\\begin{aligned}\n\\text{Var}(X+Y) & = \\mathbb{E}((X+Y)^2) - \\mathbb{E}(X+Y)^2 \\\\\n& = \\mathbb{E}(X^2 + 2XY + Y^2) - (\\mathbb{E} X^2 + 2\\mathbb{E} X\\mathbb{E} Y+ \\mathbb{E} Y^2) \\\\\n& = \\mathbb{E}(X^2) + 2\\mathbb{E}(XY) + \\mathbb{E}(Y^2) - \\mathbb{E} X^2 - 2\\mathbb{E} X\\mathbb{E} Y- \\mathbb{E} Y^2 \\\\\n& = \\underbrace{\\mathbb{E}(X^2) - \\mathbb{E} X^2}_{\\text{Var}(X)} + \\underbrace{\\mathbb{E}(Y^2) - \\mathbb{E} Y^2}_{\\text{Var}(Y)} + \\underbrace{2\\mathbb{E}(XY) - 2\\mathbb{E} X\\mathbb{E} Y}_{=\\text{Cov}(X,Y) \\text{, by property 2}}\n\\end{aligned}\n\\]\n\n\n\nThe simpler way to derive 5 is to first consider a simpler case where \\(\\mu_X = 0\\) and \\(\\mu_Y = 0\\).\n\\[\n\\begin{aligned}\n\\text{Var}(X + Y) & = \\mathbb{E}((X+Y)^2) \\\\\n& = \\mathbb{E} X^2 + 2\\mathbb{E} X\\mathbb{E} Y+ \\mathbb{E} Y^2 \\\\\n& = \\text{Var}(X) + \\text{Var}(Y) + 2 \\text{Cov}(X,Y)\n\\end{aligned}\n\\]\nAnd if the means are not zero, we can apply that \\(\\text{Var}(X + b) = \\text{Var}(X)\\) and \\(\\text{Var}(Y + d) = \\text{Var}(d)\\), and by property 4, \\(\\text{Cov}(X + b, Y + d) = \\text{Cov}(X,Y)\\).\n\n\n\nAs a side-note, if one wants to speak the formula\n\\[\\text{Var}(X) = \\mathbb{E}(X^2) - \\mathbb{E}(X)^2\n\\]\naloud, then the best way to say it seems to be\n“the variance of \\(X\\) is the second moment of \\(X\\) minus the mean of \\(X\\) squared.”\n\nIf we’re doing linear regression with a single covariate, then the formula is \\(Y = \\alpha + \\beta x + \\varepsilon\\). If we assume that \\(x \\perp\\!\\!\\!\\perp\\varepsilon\\), then\n\\[\n\\text{Cor}(X,Y) = \\rho_{X,Y} = \\beta \\frac{\\sigma_X}{\\sigma_Y}\n\\]"
  },
  {
    "objectID": "week8/week8.html#transforming-two-variables-into-one-variable",
    "href": "week8/week8.html#transforming-two-variables-into-one-variable",
    "title": "Week 8",
    "section": "Transforming two variables into one variable",
    "text": "Transforming two variables into one variable\nOften we want to know the distribution of a single random variable \\(U = g_1(X,Y)\\) that is a function of \\((X,Y)\\). However, this is hardly ever an invertible transformation. Fortunately, it turns out that we can still use the bivariate transformation technique as follows.\nIntroduce a new “auxiliary” variable \\(V = g_2(X,Y)\\), chosen to make calculations as easy as possible.\nCompute \\(f_{U,V}(u,v)\\) from \\(f_{X,Y}(x,y)\\) using the bivariate transformation formula.\nThen, integrate to get the marginal density of \\(U\\):\n\\[f_U(u) = \\int f_{U,V} (u,v) \\mathrm d v.\n\\]\n\nExample: Transformation of Uniforms\nLet \\(X,Y \\sim \\text{Uniform}(-1,1)\\) independently and suppose \\(U = (X+Y)/2\\) and \\(V = X-Y\\).\nWhat is the joint pdf of \\((U,V)\\)? Also draw a picture of the joint pdf.\n\nSince we’ll need this down the line, why don’t we go ahead and evaluate the elements of the Jacobian matrix:\nWe first need to invert these functions:\n\\(2X = 2U + V \\Longrightarrow X = U + V/2,\\)\nand \\(2Y = 2U - V \\Longrightarrow Y = U - V/2\\).\n\\[\nDh = \\begin{bmatrix}\n\\frac{\\partial x}{\\partial u} & \\frac{\\partial x}{\\partial v} \\\\\n\\frac{\\partial y}{\\partial u} & \\frac{\\partial y}{\\partial v} \\\\\n\\end{bmatrix} =\n\\begin{bmatrix}\n1 & 1/2 \\\\\n1 & -1/2 \\\\\n\\end{bmatrix}.\n\\]\nSo therefore we have that \\(| \\det Dh | = |-1/2 - 1/2| = |-1| = 1.\\)\nSo the pdf will be\n\\[f_{U,V}(u,v) = f_{X,Y}(x(u,v), y(u,v)) | \\det Dh |\n\\]\nTherefore\n\\[f_{U,V}(u,v) = \\frac{1}{4} \\mathbb 1 (x(u,v) \\in (-1,1)) \\mathbb 1(y(u,v) \\in (-1,1))\n\\]\n\\[f_{U,V}(u,v) = \\frac{1}{4} \\mathbb 1 (-1 < u + v/2 < 1) \\mathbb 1(-1 < u - v/2 < 1)\n\\]\n\\[f_{U,V}(u,v) = \\frac{1}{4} \\mathbb 1 (-2 < 2u + v < 2) \\mathbb 1(-2 < 2u - v < 2)\n\\]\nOne way to think through what these boundary conditions are is to treat these inequalities as four linear constraints.\nReplacing \\(u\\) with \\(x\\) and \\(v\\) with \\(y\\), we would get linear constraints like \\(x + y/2 = 1 \\Longrightarrow y = 2 - 2x\\).\n\n\nu <- seq(-1,1,length.out=100)\nv <- seq(-2,2, length.out=100)\n\nuv_mat <- expand.grid(u,v)\ncolnames(uv_mat) <- c('u', 'v')\n\nuv_mat$density <- with(uv_mat, 1/4 * (u + v/2 < 1 & u + v/2 > -1 & u-v/2 < 1 & u-v/2 > -1))\n\nsuppressMessages(library(tidyverse))\n\nuv_mat |> \n  ggplot(aes(x = u, y = v, fill = as.factor(density),\n  alpha = as.factor(density))) + \n  geom_tile() + \n  scale_fill_manual(values = c('0' = 'white', '0.25' = 'cornflowerblue')) + \n  scale_alpha_manual(values = c('0' = 0, '0.25' = 1)) + \n  labs(fill = \"Density\") + \n  theme_bw() + \n  guides(alpha = guide_none()) + \n  xlim(c(-2,2)) + \n  ggtitle(expression(paste(\"Probability density function of f\"[\"U,V\"], \"(u,v)\")))\n\n\n\n\n\n\nExample: Ratio of Standard Normals\nTo illustrate this technique, suppose \\(X,Y \\sim \\mathcal N(0,1)\\) independently. What is the distribution of \\(X/Y\\)?\nDefine \\(U = X / Y\\) and \\(V = Y\\). That is\n\\[u = g_1(x,y) = x/y\n\\]\n\\[v = g_2(x,y) = y.\n\\]\nIntroducing \\(V = Y\\) makes \\(g\\) invertible, so we can use the bivariate transformation formula.\nThe inverse is \\(x = h_1(u,v) = uv\\) and \\(y = h_2(u,v) = v.\\)\nThus the Jacobian matrix is\n\\[Dh = \\begin{bmatrix}\n\\frac{\\partial h_1}{\\partial u} & \\frac{ \\partial h_1}{\\partial v} \\\\\n  \\frac{\\partial h_2}{\\partial u} & \\frac{\\partial h_2}{\\partial v}\n\\end{bmatrix} = \\begin{bmatrix}\nv & u \\\\ 0 & 1\n\\end{bmatrix}.\n\\]\nThe punchline (which we’ll see soon) is Cauchy.\nTherefore the joint pdf of \\((U,V)\\) is\n\\[\n\\begin{aligned}\nf_{U,V}(u,v) & = f_{X,Y}(h_1(u,v), h_2(u,v)) \\lvert \\det Dh \\rvert \\\\\n& = \\frac{1}{\\sqrt{2\\pi}} \\exp \\left( - \\frac{1}{2} (uv)^2 \\right) \\frac{1}{\\sqrt{2\\pi}} \\exp \\left( -\\frac{1}{2} v^2 \\right) \\lvert v \\rvert \\\\\n\\frac{1}{2\\pi} |v| \\exp \\left( - \\frac{1}{2} v^2 (1 + u^2 )\\right).\n\\end{aligned}\n\\]\nTo find the marginal of \\(U\\), we need to integrate out \\(V\\). Making the change of variable \\(t = v^2, \\, \\, \\mathrm dt = 2v\\mathrm dv\\),\n\\[\n\\begin{aligned}\n\\int_0^\\infty v \\exp \\left( -\\frac{1}{2} v^2(1+u^2) \\right) \\mathrm dv & = \\frac{1}{2} \\int_0^\\infty \\exp \\left( -\\frac{1}{2} t (1+u^2) \\right) \\mathrm d t \\\\\n& = \\frac{1}{1 + u^2}.\n\\end{aligned}\n\\]\nTherefore since \\(f_{U,V}(u,v) = f_{U,V}(u,-v),\\)\n\\[f_U(u) = \\int_{-\\infty}^\\infty f_{U,V}(u,v) \\mathrm dv = 2 \\int_0^\\infty f_{U,V}(u,v) \\mathrm dv = \\frac{1}{\\pi} \\frac{1}{1+u^2}.\n\\]\nWe can recognize this as a Cauchy distribution."
  },
  {
    "objectID": "week8/week8.html#mean-and-covariance-of-a-random-vector",
    "href": "week8/week8.html#mean-and-covariance-of-a-random-vector",
    "title": "Week 8",
    "section": "Mean and Covariance of a Random Vector",
    "text": "Mean and Covariance of a Random Vector\nThe covariance matrix of a random vector \\((X,Y)^T = \\begin{bmatrix} X \\\\ Y \\end{bmatrix}\\) is\n\\[\n\\begin{aligned}\n\\text{Cov}\\left( \\begin{bmatrix} X \\\\ Y \\end{bmatrix} \\right) & =\n\\begin{bmatrix}\n\\text{Cov}(X,X) & \\text{Cov}(X,Y) \\\\\n\\text{Cov}(Y,X) & \\text{Cov}(Y,Y)\n\\end{bmatrix} \\\\\n& =\n\\begin{bmatrix}\n\\sigma_X^2 & \\rho_{X,Y} \\sigma_X \\sigma_Y \\\\\n\\rho_{X,Y} \\sigma_X \\sigma_Y & \\sigma_Y^2\n\\end{bmatrix}.\n\\end{aligned}\n\\]\nThe mean of a random vector \\((X,Y)^T\\) is defined to be the vector of the means of its entries:\n\\[\\mathbb{E}\\left( \\begin{bmatrix} X \\\\ Y \\end{bmatrix} \\right) = \\begin{bmatrix} \\mathbb{E}X \\\\ \\mathbb{E}Y \\end{bmatrix} .\n\\]\nFor any random vector \\(X = \\begin{bmatrix} X_1 \\\\ X_2 \\end{bmatrix}\\) and any \\(2 \\times 2\\) matrix \\(A\\),\n\\[\\text{Cov}(AX) = A \\text{Cov}(X) A^T.\n\\]\n(Actually this works for any dimensional \\(A\\) as long as the number of rows of \\(A\\) is the same as \\(\\text{length}(X)\\).)\nIt is common to parameterize bivariate (and more generally, multivariate) normal distributions in terms of the mean vector and covariance matrix. We write\n\\[\n\\begin{bmatrix}\nX \\\\ Y\n\\end{bmatrix} \\sim \\mathcal N(\\mu, \\Sigma)\n\\]\nto denote that \\((X,Y)^T\\) is bivariate normal such that\n\\[\\mu = \\mathbb{E}\\left( \\begin{bmatrix} X \\\\ Y \\end{bmatrix} \\right) \\quad \\quad \\text{ and } \\quad \\quad \\Sigma = \\text{Cov}\\left( \\begin{bmatrix} X \\\\ Y \\end{bmatrix} \\right).\n\\]\nHowever, not just any \\(2 \\times 2\\) matrix \\(\\Sigma\\) can be used. \\(\\Sigma\\) must be a symmetric positive semi-definite matrix, that is\n\n\\(\\Sigma = \\Sigma^T\\) (symmetric) and\n\\(t^T \\Sigma t \\geq 0\\) for all \\(t \\in \\mathbb R^2\\) (positive semi-definiteness)o\n\nWe can write \\(t^T \\Sigma t = t_1^2 \\Sigma_{11} + t_1 t_2 \\Sigma_{12} + t_2 t_1 \\Sigma_{21} + t_2^2 \\Sigma{22}.\\)\nIf \\(\\rho = 1\\), then \\[\n\\begin{aligned}\nt^T \\Sigma t & = t_1^2 \\sigma_1^2 + 2t_1t_2 \\sigma_1 \\sigma_2 + t_2^2 \\sigma_2^2 \\\\\n& = (t_1 \\sigma_1 + t_2 \\sigma_2)^2\n\\end{aligned}.\n\\]\nThis leads to a useful way of constructing bivariate normal distributions. Let \\(s_1 \\geq s_2 \\geq 0\\) and \\(\\theta \\in [0, 2\\pi )\\). Let \\(Z_1, Z_2 \\sim \\mathcal N(0,1)\\) independently and define\n\\[\n\\begin{bmatrix}\nX_1 \\\\ X_2\n\\end{bmatrix} =\n\\underbrace{\\begin{bmatrix}\n\\cos \\theta & - \\sin \\theta \\\\\n\\sin \\theta & \\cos \\theta\n\\end{bmatrix}}_{\\text{rotation}}\n\\underbrace{\\begin{bmatrix}\ns_1 & 0 \\\\\n0 & s_2\n\\end{bmatrix}}_{\\text{scaling}}\n\\begin{bmatrix}\nZ_1 \\\\ Z_2\n\\end{bmatrix}.\n\\]\nThen \\((X_1,X_2)^T\\) is bivariate normal such that the line along which \\(X_1\\) and \\(X_2\\) are correlated at angle \\(\\theta\\), the scale along this line is \\(s_1\\) and the scale orthogonal to the line is \\(s_2\\).\nConversely, given \\(\\Sigma\\) we can recover the scaling and rotation.\nCompute the eigendecomposition \\(\\Sigma = U \\Lambda U^T\\) where \\(U\\) is an orthogonal matrix and \\(\\Lambda = \\text{diag}(\\lambda_1, \\lambda_2)\\), with \\(\\lambda_1 \\geq \\lambda_2 \\geq 0\\).\nRecall that a matrix is orthogonal if \\(U^TU = I\\) and \\(UU^T = I\\).\nThen \\(\\lambda_1 = s_1^2\\), \\(\\lambda_2 = s_2^2\\), and \\(U\\) is the rotation matrix.\nThen we can represent \\(\\begin{bmatrix} X_1 \\\\ X_2 \\end{bmatrix} \\sim \\mathcal N(0,\\Sigma)\\) as\n\\[\n\\begin{bmatrix}\nX_1 \\\\ X_2\n\\end{bmatrix} =\n\\underbrace{\\begin{bmatrix}\nu_{11} & u_{12} \\\\\nu_{21} & u_{22} \\\\\n\\end{bmatrix}}_{U \\,\\text{ (rotation)}}\n\\underbrace{\\begin{bmatrix}\n\\sqrt{\\lambda_1} & 0 \\\\\n0 & \\sqrt{\\lambda_2}\n\\end{bmatrix}}_{\\Lambda^{1/2} \\,\\text{ (scaling)}}\n\\begin{bmatrix}\nZ_1 \\\\ Z_2\n\\end{bmatrix},\n\\]\nwhere \\(Z_1, Z_2 \\sim \\mathcal N(0,1)\\) independently.\nOr, more succinctly, \\(X = U \\Lambda^{1/2} Z\\).\n\nInstead of a location-scale family, this is like a rotation-scale+location family!\n\n\nSo the above show two methods:\n\nStarting from angles and scales to produce a bivariate matrix;\nOr starting from a covariance matrix using eigendecomposition to determine the rotation and scales necessary to yield that covariance matrix.\n\n\nIf this looks scary, brush up on your linear algebra:\nDiagonalization of symmetric matrices; eigendecomposition."
  },
  {
    "objectID": "week9/week9.html",
    "href": "week9/week9.html",
    "title": "Week 9",
    "section": "",
    "text": "Recap\nWe left off talking about bivariate normal distributions. We characterized them using their covariances, and showed that we could use matrix decomposition on the covariance matrix to give it an interpretation in terms of rotation and scaling matrices.\nSome handy definitions:\nThe eigendecomposition of \\(\\Sigma\\) is \\(\\Sigma = U \\Lambda U^T\\).\nA matrix is orthogonal if \\(U^TU = I\\) and \\(UU^T = I\\), a generalization of the unit-length vector.\nPrincipal components analysis can be done by applying this decomposition to the sample covariance \\(\\hat \\Sigma\\) estimated from data \\(x_1, ..., x_n \\in \\mathbb R^2\\).\nThe columns of \\(U\\) are PC directions, \\(s_1\\), \\(s_2\\) are the PC scales, \\(U^T x_i\\) are the PC scores.\nThe mgf of a bivariate random vector \\(X = (X_1, X_2)^T\\) is defined to be\n\\[M_X(t) = \\mathbb{E}\\exp (t_1 X_1 + t_2 X_2) = \\mathbb{E}\\exp (t^T X),\\]\nfor \\(t = \\begin{bmatrix} t_1 \\\\ t_1 \\end{bmatrix} \\in \\mathbb R^2\\).\nIf \\(X\\) is bivariate normal then\n\\[M_X(t) = \\exp \\left(\nt_1 \\mu_{x_1} + t_2 \\mu_{x_2} + \\frac{1}{2} \\left(t_1^2 \\sigma^2_{x_1} + 2t_1 t_2 \\rho \\sigma_{x_1} \\sigma_{x_2} + t_2^2 \\sigma_{x_2}^2 \\right)\n\\right).\n\\]\nIn matrix/vector notation, if \\(\\begin{bmatrix} X_1 \\\\ X_2 \\end{bmatrix} \\sim \\mathcal N(\\mu, \\Sigma)\\), then\n\\[M_X(t) = \\exp \\left( t^T \\mu + \\frac{1}{2} t^T \\Sigma t \\right)\n\\]\nEarlier we saw Boole’s Inequality and Bonferroni’s inequality. Inequalities are quite useful in probability because it’s often easier to bound some quantity of interest than to characterize it exactly. Often a decent bound is all that is needed.\nFor example, suppose you are manufacturing widgets. Each widget can be defective in three different ways, \\(A_1, A_2, A_3\\). You have data on the probability of each type of defect \\(P(A_k)\\), but you don’t have any data on the joint probability of these events.\nFortunately the probability of any defect can be bounded using Boole’s inequality:\n\\[P(A_1 \\cup A_2 \\cup A_3) \\leq P(A_1) + P(A_2) + P(A_3)\n\\]\nIf each \\(P(A_k)\\) is small, then we can guarantee that the probability of any defect occuring is small.\nif the number of possible defects is large, then their pairwise possible combinations are quite large, so we can save on data collection quite a bit.\nOne of the simplest but most useful inequalities in probability theory:\nMarkov’s inequality: if \\(X\\) is nonnegative and \\(a > 0\\), then\n\\[P(X \\geq a) \\leq \\frac{ \\mathbb{E}X }{a}.\n\\]\nProof:\nSince \\(1 \\geq \\mathbb 1(X \\geq a),\\)$\n\\[\\begin{aligned}\n\\mathbb{E}X & \\geq \\mathbb{E}X \\mathbb 1 (X \\geq a) \\\\\n& \\geq \\mathbb{E}a \\mathbb 1(X \\geq a) \\\\\n& = a P(X \\geq a).\n\\end{aligned}\n\\]\nDividing both sides by \\(a\\) yields the result."
  },
  {
    "objectID": "week9/week9.html#investing-returns-example",
    "href": "week9/week9.html#investing-returns-example",
    "title": "Week 9",
    "section": "Investing Returns Example",
    "text": "Investing Returns Example\nYou invest $1000 dollars in a holding where the annual returns are \\(\\mathrm{Pareto}(\\alpha, c)\\) distributed with \\(\\alpha = 2\\) and \\(c = 1/4\\).\nMore precisely, after \\(n\\) years, your investment is worth\n\\[Y_n = 1000 X_1 X_2 \\cdots X_n\\]\ndollars, where \\(X_1, ..., X_n \\sim \\mathrm{Pareto}(\\alpha, c)\\) independently.\nRecall that the pdf of \\(\\mathrm{Pareto}(\\alpha, c)\\) is\n\\[p(x) = \\frac{\\alpha c^{\\alpha}}{x^{\\alpha+1}} \\mathbb 1(x > c).\\]\n\nIs this a good investment?\nFirst, guess using your intuition. Then try to show something formally.\n\nf <- function(x) {2 * .25^(2) / x^(2 + 1) * (x > .25)}\ncurve(f, from = 0, to = 2)\n\n\n\n\nSince so much of the pdf appears to be concentrated in (0.25,1), we were wondering, given that the Pareto distrbution has heavy tails, what’s the probability of getting an outcome \\(\\geq 1\\)?\n\ncdf_f <- function(x) { 1 - (.25/x)^2 }\n1 - cdf_f(1)\n\n[1] 0.0625\n\n\nIt looks like that might happen 6.25% of the time, but we don’t really have a sense of how large those values might be.\n\n\n\nApplying Markov’s theorem:\nWe’re interested in \\(P(Y_n \\geq a) \\leq \\frac{\\mathbb{E}Y_n}{a}\\).\n\\[\n\\begin{aligned}\nP(Y_n \\geq a) & \\leq \\frac{\\mathbb{E}Y_n}{a} \\\\\n& = \\frac{\\mathbb{E}10000 X_1 \\cdots X_n}{a} \\\\\n& = \\frac{1000 (\\mathbb{E}X_1) \\cdots (\\mathbb{E}X_n)}{a} \\\\\n& = \\frac{1000}{a} \\left( \\frac{\\alpha c}{\\alpha - 1} \\right)^n \\\\\n& = \\frac{1000}{a} \\left( \\frac{1}{2} \\right)^n \\stackrel{n \\to \\infty }{\\Longrightarrow} 0 \\\\\n\\Longrightarrow & P(Y_n \\geq a) \\longrightarrow 0 \\quad \\text{ for all } a > 0.\n\\end{aligned}\n\\]\n\n\n\nIf one sees a probability of some large event that they want to bound, usually the first thing one should try is applying Markov’s theorem."
  },
  {
    "objectID": "week9/week9.html#corollaries-of-markovs-inequality",
    "href": "week9/week9.html#corollaries-of-markovs-inequality",
    "title": "Week 9",
    "section": "Corollaries of Markov’s Inequality",
    "text": "Corollaries of Markov’s Inequality\n\nFor any random variable \\(X\\) and any \\(a > 0\\),\n\n\\[P(|X| \\geq a) \\leq \\frac{\\mathbb{E}|X|}{a}.\\]\n\nFor any random variable \\(X\\), any \\(a \\in \\mathbb R\\), and any monotone increasing function \\(g(x) \\geq 0\\),\n\n\\[P(X \\geq a) \\leq \\frac{\\mathbb{E}g(X)}{g(a)}.\\]\n\nChebyshev’s inequality: For any random variable \\(X\\) and any \\(a > 0\\),\n\n\\[P(|X - \\mathbb{E}X| \\geq a) \\leq \\frac{\\text{Var}(X)}{a^2}.\\]\nChebyshev’s inequality allows us to bound the probability that a random variable is a certain distance from its mean.\n\nTry to show 2 and 3 using Markov’s inequality.\nBecause \\(x \\geq a \\to g(x) \\geq g(a)\\), \\[P(X \\geq a) \\leq P(g(X) \\geq g(a)) \\quad \\text{ since $g$ is monotonically increasing}.\\]\n(If the statement said strictly monotone, the first inequality would be an equality.)\nNote that \\(g(a)\\) needs to be positive.\nAnd then apply Markov’s inequality to get that\n\\[P(X \\geq a) = P(g(X) \\geq g(a)) = \\frac{\\mathbb{E}g(X)}{g(a)}.\\]\nThen for 3, we want to set \\(g(x) = x^2\\), which is monotonically increasing on \\(X \\geq 0\\).\n\\[\\begin{aligned}\nP(|X-\\mathbb{E}X| \\geq a) & = P(| X - \\mathbb{E}X |^2 \\geq a^2) \\\\\n& \\leq \\frac{ \\mathbb{E}|X-\\mathbb{E}X|^2 }{a^2} \\\\\n& = \\frac{\\text{Var}(X)}{a^2},\n\\end{aligned}\n\\]\nnoting that \\(|X - \\mathbb{E}X|\\) and \\(a^2\\) being non-negative is what makes this work."
  }
]