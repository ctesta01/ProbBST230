[
  {
    "objectID": "week10/week10.html",
    "href": "week10/week10.html",
    "title": "Week 10",
    "section": "",
    "text": "\\(L_p\\) Spaces\n\\(L^p\\) spaces are nice classes of functions that come up a lot.\nFor \\(p \\geq 1\\), the \\(L^p\\) norm of a random variable is \\((\\mathbb{E}|X|^p)^{1/p}.\\)\nExamples:\nThe set of random variables \\(X\\) such that \\((\\mathbb{E}|X|^p)^{1/p} < \\infty\\) is denoted \\(L^p\\).\nThat is, \\(X \\in L^p\\) means that \\((\\mathbb{E}|X|^p)^{1/p} < \\infty\\).\nNote that \\((\\mathbb{E}|X|^p)^{1/p} < \\infty\\) iff \\(\\mathbb{E}|X|^p < \\infty.\\) The purpose of the \\(1/p\\) is that it makes it have the properties of a norm.\nProperties that are required for a norm:\nBoole’s Inequality: if you’re trying to show something has small probability of happening, and it can happen a few ways each of which are bounded by small probability. Useful for bounding something close to zero.\nMarkov’s inequality: comes up a lot, whenever you’re trying to establish an upper bound on large probabilities — often good to try applying Markov’s first. E.g., showing some errors or something is converging to zero — then we want to show the probability that the error is big, is small.\nChebyshev’s inequality and Hoeffding’s inequality are kind of the same sort of thing. Usually what these are used for is situations like if \\(X\\) is the sample mean, then we can say that the sample mean is close to whatever it’s converging to with high probability. E.g., to show the law of large numbers.\nChernoff’s bound: again, same thing.\nHoeffding’s: again, the sample mean is converging to the mean.\nJensen’s inequality: Whenever you have a bound involving an expectation and it needs to be manipulated in some way, sometimes it’s easier to put the function on the inside of the expectation, and sometimes it’s easier to put it on the outside of the expectation depending on the problem. So it’s useful for both establishing an upper bound or a lower bound depending on whether putting a convex function on the inside or outside of the expectation is easier.\nInequalities due to Jensen’s inequality with moments, and the inequalities with \\(\\log\\) terms are especially useful.\nWeighted AM-GM is not super common, but when it does show up, it’s super nice. As we saw, useful in the proof of Hölder’s inequality.\nHölder’s and Cauchy-Schwarz are for when we have products of variables.\nThe multivariate normal (or multivariate Gaussian) family is a generalization of the univariate normal to random vectors.\nIt is, without a doubt, the most important family of distributions in probability and statistics.\nCentral Limit Theorem:\nMultivariate normal distributions have quite nice analytic tractability:\nA random vector \\(X = (X_1, ..., X_k)^T\\) is multivariate normal if \\(a^T X\\) (the dot-product of \\(a\\) and \\(X\\)) is univariate normal for all \\(a \\in \\mathbb{R}^k\\).\nWe consider the univariate normal family to include the degenerate case \\(\\mathcal N(\\mu,0)\\), defined as the point mass at \\(\\mu\\).\nMultivariate normals are determined by two parameters:\nWe write that \\(X \\sim \\mathcal N(\\mu, \\Sigma)\\) to denote \\(X\\) is multivariate normal with \\(\\mathbb{E}X = \\mu\\) and \\(\\text{Cov}(X) = \\Sigma\\).\nIf \\(\\Sigma\\) is an invertible matrix, then the pdf of \\(X \\sim \\mathcal N(\\mu, \\Sigma)\\) is\n\\[\\mathcal N(x \\mid \\mu, \\Sigma) = \\frac{1}{(2 \\pi )^{k/2} | \\det (\\Sigma) |^{1/2}}\n\\exp \\left( -\\frac{1}{2} (x-\\mu)^T \\Sigma^{-1} (x-\\mu) \\right)\\]\nfor all \\(x \\in \\mathbb{R}^k\\). Here \\(\\det (\\Sigma)\\) denotes the determinant of \\(\\Sigma\\).\n\\(\\mathcal N(0,I)\\) denotes the standard multivariate normal distribution.\nLet \\(\\Sigma\\) be any real symmetric positive semi-definite matrix.\nThe eigendecomposition of \\(\\Sigma\\) is the factorization \\(\\Sigma = U D U^T\\), where\nThe columns of \\(U\\) are called the eigenvectors, and the diagonal entries of \\(D\\) are the corresponding eigenvalues.\nWriting \\(U = [ u_1 \\cdots u_k]\\) yields the usual \\(\\Sigma u_i = d_{ii} u_i\\) property:\n\\[\\left[ \\Sigma u_1 \\cdots \\Sigma u_k \\right] = \\Sigma U = U D U^T U = U D = \\left[ d_{11} u_1 \\cdots d_{kk} u_k \\right].\\]"
  },
  {
    "objectID": "week10/week10.html#hölders-inequality",
    "href": "week10/week10.html#hölders-inequality",
    "title": "Week 10",
    "section": "Hölder’s Inequality",
    "text": "Hölder’s Inequality\nFor any random variables \\(X\\) and \\(Y\\), if \\(p,q > 1\\) such that\n\\[\\frac{1}{p} + \\frac{1}{q} = 1\\]\nthen\n\\[\\mathbb{E}|XY| \\leq (\\mathbb{E}| X|^p)^{1/p} (\\mathbb{E}|Y|^q)^{1/q}.\\]\n\nProof\nBy the weighted AM-GM inequality with \\(n = 2\\), with \\(w_1 = 1/p\\) and \\(w_2 = 1/q\\),\n\\[\\frac{1}{q} \\frac{|X|^q}{\\mathbb{E}|X|^q} + \\frac{1}{q} \\frac{|Y|^q}{\\mathbb{E}|Y|^q} \\geq\n\\frac{\\mathbb{E}|XY|}{(\\mathbb{E}|X|^p)^{1/p} (\\mathbb{E}|Y|^q)^{1/q}}. \\]"
  },
  {
    "objectID": "week10/week10.html#corollaries-of-hölder",
    "href": "week10/week10.html#corollaries-of-hölder",
    "title": "Week 10",
    "section": "Corollaries of Hölder",
    "text": "Corollaries of Hölder\n\nCauchy-Schwarz\nThe Cauchy-Schwarz inequality is an important special case of Hölder’s inequality.\nFor any random variables \\(X\\) and \\(Y\\),\n\\[E|XY| \\leq (\\mathbb{E}|X|^2)^{1/2} (\\mathbb{E}|Y|^2)^{1/2}.\\]\nProof: Apply Hölder’s inequality with \\(p = q = 2\\).\n\n\nLyapunov’s inequality\nIf \\(1 \\leq r < s < \\infty\\), then\n\\[(E|X|^r)^{1/r} \\leq (\\mathbb{E}|X|^s)^{1/s}.\\]\nThus if \\(X \\in L^s\\) then \\(X \\in L^r\\) for all \\(r \\in [1,s)\\).\nProof: Apply Hölder’s inequality to the random variables \\(|X|^r\\) and \\(Y=1\\) with \\(p = s/r\\) (and \\(q = 1/(1-1/p)\\)) to get\n\\[\\mathbb{E}|X|^r \\leq (\\mathbb{E}|X|^{rp})^{1/p} = (\\mathbb{E}|X|^s)^{r/s}.\\]\nRaising both sides to the power of \\(1/r\\) yields the result.\n\n\nCovariance Inequality\nIf \\(X\\) and \\(Y\\) have means \\(\\mu_X, \\mu_Y\\) and variances \\(\\sigma_X^2, \\sigma_Y^2\\), then\n\\[|\\text{Cov}(X,Y)| \\leq \\sigma_X \\sigma_Y. \\]\n\n\\[\n\\begin{aligned}\n|\\text{Cov}(X,Y)| & = |\\mathbb{E}(X - \\mu_X)(Y - \\mu_Y)| \\\\\n& \\leq |\\mathbb{E}(X-\\mu_X)(Y-\\mu_Y)| \\quad \\tiny \\text{by Jensen's inequality, since }|\\cdot|\\text{ is convex} \\\\\n& \\leq (\\mathbb{E}|X-\\mu_X|^2)^{1/2} (\\mathbb{E}|Y-\\mu_Y|^2)^{1/2} \\\\\n& = (\\sigma_X^2){1/2} (\\sigma_Y^2)^{1/2} \\\\\n& = \\sigma_X \\sigma_Y\n\\end{aligned}\n\\]\n\nThis shows that \\(-1 \\leq \\rho_{X,Y} \\leq 1\\) where \\(\\rho_{X,Y} = \\frac{\\text{Cov}(X,Y)}{\\sigma_X \\sigma_Y}\\)."
  },
  {
    "objectID": "week10/week10.html#minkowskis-inequality",
    "href": "week10/week10.html#minkowskis-inequality",
    "title": "Week 10",
    "section": "Minkowski’s Inequality",
    "text": "Minkowski’s Inequality\nFor any random variables \\(X\\) and \\(Y\\) and any \\(p \\geq 1\\),\n\\[(\\mathbb{E}|X + Y|^p)^{1/p} \\leq (\\mathbb{E}|X|^p)^{1/p} + (\\mathbb{E}|Y|^p)^{1/p}.\\]\nProof is in Casella & Berger, Theorem 4.7.5.\nMinkowski’s inequality establishes the triangle inequality for \\(L^p\\) norms."
  },
  {
    "objectID": "week1/week1.html",
    "href": "week1/week1.html",
    "title": "Week 1",
    "section": "",
    "text": "Introductory Overview\nJeffrey Miller’s office hours: Noon-1pm on Thursdays\nTA: Cathy Xue Office hours: 5:30-6:30pm on Tuesdays in 2-434 (Building 2, Room 434)"
  },
  {
    "objectID": "week1/week1.html#course-description",
    "href": "week1/week1.html#course-description",
    "title": "Week 1",
    "section": "Course description:",
    "text": "Course description:\n\nAxiomatic foundations of probability, independence, conditional probability, joint distributions, transformations, moment generating functions, characteristic functions, moment inequalities, sampling distributions, modes of convergence and their interrelationships, laws of large numbers, central limit theorem, and stochastic processes"
  },
  {
    "objectID": "week1/week1.html#course-readings",
    "href": "week1/week1.html#course-readings",
    "title": "Week 1",
    "section": "Course Readings:",
    "text": "Course Readings:\n\nStatistical Inference (Second Edition), by George Casella and Roger L. Berger. Cengage Learning, 2021.\nProbability: Theory and Examples (Fourth Edition), by Richard Durrett. Cambridge University Press, 2010. (https://services.math.duke.edu/~rtd/PTE/PTE5_011119.pdf)\nIntroduction to Stochastic Processes (Second Edition), by Gregory F. Lawler. Chapman & Hall/CRC, 2006.\n\nThe Durrett book is more measure-theoretic, but covers some things better (according to Miller) than Casella and Berger. Lawler’s book is a gentle introduction to stochastic processes."
  },
  {
    "objectID": "week1/week1.html#labs",
    "href": "week1/week1.html#labs",
    "title": "Week 1",
    "section": "Labs",
    "text": "Labs\nWeekly Tuesdays at 3:45-5:15 in FXB G10"
  },
  {
    "objectID": "week1/week1.html#outline-of-topics",
    "href": "week1/week1.html#outline-of-topics",
    "title": "Week 1",
    "section": "Outline of Topics",
    "text": "Outline of Topics\n\nFundamentals (CB 1.1 - 1.2.2)\n\nSet theory basics, Measure theory basics, Properties of probability measures\n\nProbability basics (CB 1.2.3 - 1.6)\n\nCombinatorics, Conditional probability and Independence, Random variables\n\nTransformations of random variables (CB 2.1)\n\nChange of variable formula for r.v.s, Probability integral transform\n\nExpectations of random variables CB 2.2 - 2.4)\n\nMean and variance, Moments, MGFs, Differentiation and limits of integrals\n\nFamilies of distributions (CB 3.1 - 3.5)\n\nDiscrete and continuous families, exponential families, location-scale families\n\nInequalities (CB 3.6, 3.8, 4.7)\n\nMarkov, Chebyshev, Gauss, Hölder, Cauchy-Schwarz, Minkowski, Jensen\n\nMultiple random variables (CB 4.1 - 4.6)\n\nRandom vectors, conditional distributions, independence, mixtures, covariance and correlation\n\nGaussian distributions (Bishop pp. 78-93, in Files/Reading on Canvas site)\n\nMultivariate normal, marginals and conditionals, linear-Gaussian model\n\nStatistics of a random sample (CB 5.1 - 5.4)\n\nSampling distributions, Sums of random variables, Student’s t and Snedecor’s F distribution, Order statistics and friends\n\nAsymptotics (CB 5.5)\n\nModes of convergence, Limit theorems, Delta method, Borel-Cantelli lemma\n\nLaws of large numbers (CB 5.5, D 2.2 - 2.4)\n\nWeak laws of large numbers, Strong laws of large numbers, Generalizations\n\nCentral limit theorems (CB 5.5, D 3.1 - 3.4)\n\nWeak convergence, characteristic functions, central limit theorems\n\nGenerating random samples (CB 5.6)\n\nInverse cdf method, accept/reject method, Markov chain Monte Carlo\n\nStochastics processes (L 1 - 3)\n\nMarkov chains, Random walks, Branching processes, Poisson processes"
  },
  {
    "objectID": "week1/week1.html#introduction",
    "href": "week1/week1.html#introduction",
    "title": "Week 1",
    "section": "Introduction",
    "text": "Introduction\nHow could we tell if either of the two sequences were faked.\n\nstr1 <- \"1 0 0 0 0 1 0 0 1 1 0 0 1 1 0 0 0 1 1 1 1 1 0 0 0 1 1 1 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 0 0 0 1 1 0 1 0 0 0 1 1 0 1 1 0 0 0 0 0 1 1 0 1 0 0 1 0 1 1 0 0 0 1 0 1 1 0 1 0 0 1 0 1 0 1 1 0 0 1 0 1\"\n\nstr2 <- \"1 1 0 0 1 0 0 0 1 1 1 0 1 0 1 0 0 0 1 0 1 1 1 0 1 0 0 0 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 0 0 0 1 0 1 1 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0 1 1 0 1 0 0 1 0 1 0 0 1 1 1 0 1 1 1 0 0 1 1 1 0 1 0 1 0 1 0 1 0\"\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nstr_count(str1, \"0|1\")\n\n[1] 100\n\nstr_count(str1, \"1\")\n\n[1] 43\n\nstr_count(str1, \"0\")\n\n[1] 57\n\nstr_count(str2, \"0|1\")\n\n[1] 100\n\nstr_count(str2, \"1\")\n\n[1] 53\n\nstr_count(str2, \"0\")\n\n[1] 47\n\nstr1_num <- as.numeric(unlist(stringr::str_split(str1, \" \")))\nstr2_num <- as.numeric(unlist(stringr::str_split(str2, \" \")))\n\n# the first way I proposed was to look at the probability of \n# the coin being \"fair\" given the beta distribution parameterized \n# by the observed coinflips \nx <- seq(0,1,0.01)\ncurve(dbeta(x, str_count(str1, \"0\")+1, str_count(str1, \"1\")+1))\n\n\n\ncurve(dbeta(x, str_count(str2, \"0\")+1, str_count(str2, \"1\")+1))\n\n\n\n# then we tried looking at the running mean\nplot(1:100, cummean(str1_num), type='l')\n\n\n\nplot(1:100, cummean(str2_num), type='l')\n\n\n\n# another classmate suggested that the human-generated \n# sequence may have more anti-correlation than the\n# real sequence because more anti-correlation \"looks\" more\n# random \ncor(lag(str1_num), str1_num, use = 'pairwise.complete.obs')\n\n[1] 0.007518797\n\ncor(lag(str2_num), str2_num, use = 'pairwise.complete.obs')\n\n[1] -0.2367884\n\n\nMiller suggests we could also look at it as a sequence of random variables.\n\nstr1_as_geometric_series <- sapply(unlist(stringr::str_split(stringr::str_remove_all(str1, \" \"), \"1\")), nchar)\nunname(str1_as_geometric_series)\n\n [1] 0 4 2 0 2 0 3 0 0 0 0 3 0 0 0 3 3 8 0 0 3 0 1 3 0 1 0 5 0 1 2 1 0 3 1 0 1 2\n[39] 1 1 0 2 1 0\n\nstr2_as_geometric_series <- sapply(unlist(stringr::str_split(stringr::str_remove_all(str2, \" \"), \"1\")), nchar)\nunname(str2_as_geometric_series)\n\n [1] 0 0 2 3 0 0 1 1 3 1 0 0 1 3 0 0 0 3 0 1 1 0 0 3 1 0 1 1 2 0 0 0 1 1 1 2 0 1\n[39] 2 1 2 0 0 1 0 0 2 0 0 1 1 1 1 1\n\ncurve(dgeom(x, prob = .5), from = 0, to = 10, n = 11)"
  },
  {
    "objectID": "week1/week1.html#history-of-probability",
    "href": "week1/week1.html#history-of-probability",
    "title": "Week 1",
    "section": "History of Probability",
    "text": "History of Probability\nGames of chance have been played for millenia. Early dice games were played with “astragali”, or “knucklebones”, from the ankle of a sheep or goat.\n\n\n\n\n\n\n\n\n\nEgyptian tomb paintings from 3500 BC show games played with astragali, and ancient Greek vases show young men tossing the bones into a circle. Gambling in these games was common, so it would have been advantageous to have some understanding of probability.\nInterest in gambling led mathematicians in the 1500-1600s to begin to formalize the rules of probability.\nTwo players put equal money in a pot. The first player to win 8 rounds of a game gets all the money. If they have to stop before finishing, how should the money be divided between them based on how much they would have won, on average?\nAround 1654, Blaise Pascal and Pierre de Fermat developed the concept of expected value to solve this problem.\nChristiaan Huuygens built upon this in his 1657 textbook on probability, “De Ratiociniis in Ludo Aleae” (“The Value of all Chances in Games of Fortune,”)\nIn the early 1700s, Jacob Bernoulli and Abraham De Moivre wrote foundational books on probability.\nThey systematically developed the mathematics of probability, focusing primarily on discrete problems.\nCombinatorial approaches were developed to handle difficult probability calculations.\nBernoulli proved the first version of the law of large numbers.\nIn 1812, Pierre-Simon Laplace published his book “Théorie analytique des probabilités”.\nLaplace developed or advanced many key methods and results in modern probability and statistics.\nGenerating functions, characteristic functions, linear regression, density functions, Bayesian inference, and hypothesis testing.\nHe employed advanced calculus and real/complex analysis,\ntaking probability calculations to a whole new level.\nLaplace proved the first general version of the central limit theorem.\nIn the 1930s, Andrey Kolmogorov introduced the measure theoretic foundations of modern probability.\nMeasure theory had recently been developed to resolve certain paradoxes that arose in defining volume and integration.\nKolmogorov applied measure theory to put probability on solid theoretical footing.\nThis is particularly important for limits and derivatives of integrals, conditional distributions, and stochastic processes."
  },
  {
    "objectID": "week1/week1.html#set-theory-basics",
    "href": "week1/week1.html#set-theory-basics",
    "title": "Week 1",
    "section": "Set Theory Basics",
    "text": "Set Theory Basics\nThe sample space denoted \\(S\\) is the set of possible outcomes of an experiment.\nExamples include \\(S = \\{ H, T \\}\\) for a coin toss, or math SAT scores \\(S = \\{ 200, 201, ..., 799, 800 \\}\\), or time-to-events: \\(S = (0, \\infty)\\).\nWe say that an event \\(E\\) is a subset of \\(S\\) that is \\(E \\subset S\\).\nA set \\(A\\) is a collection of elements. We say that \\[A \\cup B = \\{ x \\colon x \\in A \\text{ or } x \\in B \\}.\\]\n\\[A \\cap B = \\{ x \\colon x \\in A \\text{ and } x \\in B \\}.\\]\n\\[A^c = \\{ x \\in S \\colon x \\not \\in A \\}\\]\n\\[A \\backslash B = \\{ x \\colon x \\in A \\text{ and } x \\not \\in B \\}.\\]\nThe empty set is denoted \\(\\varnothing = \\{\\}\\).\n\\(A \\subset B\\) means that if \\(x \\in A\\) then \\(x \\in B\\).\n\nProperties of Set Operations\nCommutativity:\n\\[A \\cup B = B \\cup A, \\quad A \\cap B = B \\cap A\\]\nAssociativity:\n\\[\nA \\cup (B \\cup C) = (A \\cup B) \\cup C \\quad\nA \\cap (B \\cap C) = (A \\cap B) \\cap C\n\\]\nDistributive Laws:\n\\[ A \\cup (B \\cup C) = (A \\cap B) \\cup (A \\cap C)\n\\quad\nA \\cup (B \\cap C) = (A \\cup B) \\cap (A \\cup C)\\]\nDeMorgan’s Laws:\n\\[ (A \\cup B)^c = A^c \\cap B^c \\quad\n(A \\cap B)^c = A^c \\cup B^c \\]\n\n\nSigma Algebras\nDefinition. Suppose that \\(\\mathcal B\\) is a set of subsets of a sample space \\(S\\). Then \\(\\mathcal B\\) is a sigma-algebra if:\n\n\\(\\varnothing \\in \\mathcal B\\).\nif \\(A \\in \\mathcal B\\) then \\(A^c \\in \\mathcal B\\).\nif \\(A_1, A_2, ... \\in \\mathcal B\\) then \\(\\bigcup_{i=1}^\\infty A_i \\in \\mathcal B\\)\n\n\nThe powerset, denoted \\(2^S\\) is a specific example of a sigma algebra.\n\n\n\nWe have to be very careful that \\(\\varnothing\\) must be an element of \\(\\mathcal B\\) in order for \\(\\mathcal B\\) to be a sigma algebra. \\(\\varnothing\\) is certainly a subset of any set, but \\(\\varnothing\\) needs to be an element of \\(\\mathcal B\\) as a collection of sets.\n\nThe smallest sigma algebra, called the trivial sigma algebra, is \\(\\{ \\varnothing, S \\}\\) for a sample space \\(S\\). When \\(S\\) is uncountable, we usually don’t use the power set as a sigma algebra. Instead, we typically opt for using the Borel sigma algebra.\nFor a topological space \\(S\\), the Borel sigma algebra, denoted \\(\\mathcal B(S)\\) is the smallest sigma algebra containing all open sets.\nDefinition. Let \\(X\\) be a set and \\(\\tau\\) be a family of subsets of \\(X\\). Then \\(\\tau\\) is a topology and \\((X, \\tau)\\) is a topological space if\n\nBoth the empty set and \\(X\\) are elements in \\(\\tau\\).\nAny union of elements of \\(\\tau\\) is an element of \\(\\tau\\).\nAny intersection of finitely many elements of \\(\\tau\\) is an element of \\(\\tau\\).\n\nThe members of \\(\\tau\\) are called open sets in \\(X\\).\nWhen \\(A \\in \\mathcal B\\), we say that \\(A\\) is a measurable set.\n\n\nProbability Measures\nDefinition. If \\((S, \\mathcal B)\\) is a measurable space, then \\(P: \\mathcal B \\to \\mathbb R\\) is a probability measure if:\n\n\\(P(A) \\geq 0\\) for all \\(A \\in \\mathcal B\\). (non-negativity)\n\\(P(S) = 1\\) (unitarity)\nif \\(A_1, A_2, ... \\in \\mathcal B\\) are pairwise disjoint, then \\[P\\left(\\bigcup_{i=1}^\\infty A_i \\right) =\n  \\sum_{i=1}^\\infty P(A_i).\\] (countable additivity)\n\nThese properties are called the axioms of probability, or sometimes Kolmogorov’s axioms.\nIf \\(A \\in \\mathcal B\\) we call \\(A\\) a measurable set.\nIn this course, we may assume that the sets we are working with are measurable. Almost exclusively we will be working with the Borel sigma algebra. While non-measurable sets do exist in this setting, they do not often arise in practice.\n\nTJ asks: “I know it’s possible to demonstrate non-measureable sets non-constructively, but is it possible to demonstrate them constructively?”\nMiller: “I think you have to use infinite series/sets [and the axiom of choice].”"
  },
  {
    "objectID": "week1/week1.html#probability-measure-on-a-countable-set",
    "href": "week1/week1.html#probability-measure-on-a-countable-set",
    "title": "Week 1",
    "section": "Probability measure on a countable set",
    "text": "Probability measure on a countable set\nSuppose that \\(S = \\{ s_1, s_2, ... \\}\\) is a countable set.\nLet \\(p_1, p_2, ... \\geq 0\\) such that \\(\\sum_{i=1}^\\infty p_i = 1\\).\nFor \\(A \\subset S\\), define \\[P(A) = \\sum_{i=1}^\\infty p_i \\mathbb 1(i \\in A).\\]\nWe will write that \\(\\mathbb 1(\\cdot)\\) to denote the indicator function, where\n\\[\\mathbb 1(C) = \\left\\{ \\begin{array}{ll}\n1 & \\text{ if condition } C \\text{ is true} \\\\\n0 & \\text{ if condition } C \\text{ is false}\n\\end{array}\n\\right.\\]\nSuppose you toss a fair coin until you get heads. Then \\(p_k\\), the probability that you toss the coin \\(k\\) times, is \\((1/2)^k\\). This defines a probability measure on \\(S = \\{ 1, 2, ... \\}\\).\nWe could imagine measuring how long a lightbulb lasts until it dies after being left on. It could die at any non-negative time \\(t \\geq 0\\). The probability \\(P([0,t))\\) be the probability the lightbulb dies before time \\(t\\). This defines a probability measure on \\(S = [0,\\infty).\\) (Of course, \\([0,\\infty)\\) is a continuous example and not a countable set).\nFor any probability measure, we have that:\n\n\\(P(A^c) = 1 - P(A)\\)\n\\(P(\\varnothing) = 0\\)\n\\(P(A) \\leq 1\\)\nif \\(A \\subset B\\) then \\(P(A) \\leq P(B)\\)\n\\(P(A) = P(A \\cap B) + P(A \\cap B^c)\\)\n\\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\).\n\nProofs:\n\n\\(P(S) = 1\\), and \\(A, A^c\\) are pairwise disjoint and a partition of \\(S\\), so \\(1 = P(S) = P(A^c \\cup A) = P(A) + P(A^c)\\). Subtracting from both sides, \\(P(A) = 1 - P(A^c)\\).\n\\(\\varnothing\\) and \\(S\\) are disjoint, so \\(1 = P(S) = P(S) + P(\\varnothing)\\). Subtracting from both sides, we have that \\(1 - 1 = P(\\varnothing)\\)\nWe have from 1 that \\(1 - P(A^c) = P(A)\\), and \\(P(A^c) \\geq 0\\), so then \\(P(A) \\leq 1\\)\nWe can write that \\(B \\backslash A\\) and \\(A\\) as disjoint sets since \\(A \\subset B\\). Then \\(P(B) = P(A \\cup B \\backslash A) = P(A) + P(B \\backslash A)\\). Since \\(P(B \\backslash A)\\) we have that \\(P(B) \\geq P(A)\\).\nWe need to show that \\(A = (A \\cap B) \\cup (A \\cup B^c)\\). If \\(a \\in A\\) then either \\(a \\in B\\) or \\(a \\in B^c\\), but not both by the definition of complement. Therefore \\(A \\cap B\\) and \\(A \\cap B^c\\) are disjoint and their union is equal to \\(A\\). Hence \\(P(A) = P(A \\cap B \\bigcup A \\cap B^c) = P(A \\cap B) + P(A \\cap B^c)\\).\nFirst, note that \\(A \\cup B = A \\cap (B \\backslash A)\\). Thus \\(P(A \\cup B) = P(A \\cup (B \\backslash A))\\). Since the latter are disjoint, we establish that \\(P(A \\cup B) = P(A) + P(B \\backslash A)\\). Now if we consider that \\(B = (B \\backslash A) \\cup (A \\cap B)\\), and that these are disjoint sets, we have that \\(P(B) = P(B \\backslash A) + P(A \\cap B)\\). Rearranging, we have that \\(P(B \\backslash A) = P(B) - P(A \\cap B)\\). Substituting, now we have that \\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\) as desired."
  },
  {
    "objectID": "week1/week1.html#properties-of-probability-measures",
    "href": "week1/week1.html#properties-of-probability-measures",
    "title": "Week 1",
    "section": "Properties of Probability Measures",
    "text": "Properties of Probability Measures\nLaw of total probability: for any partition \\(B_1, B_2, ...\\) of \\(S\\), we have that \\[P(A) = \\sum P(A \\cap B_i)\\]\nBoole’s inequality (aka union bound): For \\(A_1, A_2, ...\\),\n\\[P(\\bigcup_{i=1}^\\infty A_i) \\leq \\sum_{i=1}^\\infty P(A_i).\\]\nBonferroni’s inequality: For any \\(A_1, A_2,...\\),\n\\[P\\left(\\bigcap_{i=1}^\\infty A_i \\right) \\geq 1 - \\sum_{i=1}^\\infty P(A_i^c).\\]\nBoole’s inequality is often useful when we want to show that some event has probability near zero. For example, \\(P(E) = P(A_1 \\cup A_2 \\cup A_3) \\leq P(A_1) + P(A_2) + P(A_3) \\leq 3\\epsilon\\)."
  },
  {
    "objectID": "week1/week1.html#selecting-k-items-from-n-options",
    "href": "week1/week1.html#selecting-k-items-from-n-options",
    "title": "Week 1",
    "section": "Selecting \\(k\\) items from \\(n\\) options",
    "text": "Selecting \\(k\\) items from \\(n\\) options\n\n\n\n\nwithout replacement\nwith replacement\n\n\n\n\nordered\n\\(\\frac{n!}{(n-k)!}\\)\n\\(n^k\\)\n\n\nunordered\n\\({n \\choose k}\\)\n\\({ n + k - 1 \\choose k }\\)"
  },
  {
    "objectID": "week1/week1.html#determining-the-leading-factor-in-stirlings-formula",
    "href": "week1/week1.html#determining-the-leading-factor-in-stirlings-formula",
    "title": "Week 1",
    "section": "Determining the Leading Factor in Stirling’s Formula",
    "text": "Determining the Leading Factor in Stirling’s Formula\nStirling’s formula is that for large values of \\(n\\), the following is a good approximation for the factorial function:\n\\[ n! \\approx \\frac{n^n}{e^n} \\sqrt{2 \\pi n}. \\]\nI’ll concern myself with, as an exercise, showing the leading factor \\(n^n e^{-n}\\) is correct.\nFirst, observe the relationship between \\(n!\\) and \\(n^n\\):\n\\[n! = \\underbrace{n \\cdot (n-1) \\cdots 1}_{n \\text{ terms}} < \\underbrace{n \\cdot n \\cdots n}_{n \\text{ times}} = n^n\\]"
  },
  {
    "objectID": "week2/week2.html",
    "href": "week2/week2.html",
    "title": "Week 2",
    "section": "",
    "text": "Conditional Probability and Independence\nIf we go back to the formula for sampling with replacement, the \\(-1\\) term comes from the fact that when converting from the number of bins we could put balls into to the number of dividers between the bins, there’s one less divider than there are bins.\nThat formula was \\[ {n + k - 1 \\choose k} \\] so we could think about how to put \\(k\\) balls into \\(n\\) bins, or instead \\(k\\) balls into \\(n-1\\) dividers, or we could think about there being \\(n-1\\) blue balls and \\(k\\) red balls and the blue balls represent the dividers, so now we’re describing choosing \\(k\\) balls to be red out of \\(n + k - 1\\) balls.\nWe have only been so far considering events and outcomes in the sample space \\(S\\). Often it’s most useful to work with functions of the outcome.\nFor instance, suppose a coin is tossed \\(N\\) times.\nA natural definition of the sample space would be \\(S = \\{ 0, 1 \\}^N\\), that is all sequences of \\(N\\) zeroes or ones.\nDefine \\(X\\) to be the number of times that heads comes up.\nIf we only want to evaluate whether the coin is biased, we may as well work with \\(X\\) rather than the whole sequence.\n\\(X\\) can be thought of as a function from the sample space \\(S\\) to the set of integers.\nDefinition. A random variable \\(X\\) is a function from the sample space equipped with sigma algebra \\(\\Omega\\) to the real numbers \\(X: S \\to \\mathbb R\\). Technically it must be a measurable function, that is \\(X^{-1}(A)\\) must be a measurable set for all measurable sets \\(A \\in \\mathcal B(\\mathbb R)\\). But we won’t worry about this so much in this course.\nIn other words, when the outcome is \\(s \\in S\\), the random variable takes the value \\(X(s)\\) which is some real number.\nThe probability that \\(X\\) takes value \\(x\\), denoted \\(P(X=x)\\), is \\[P(X=x) = P(\\{s \\in S \\colon X(s) = x \\})\\]\nIn the coin tossing example, if \\(s = (s_1, ..., s_N) \\in S = \\{0,1\\}^N\\), then the number of heads \\(X(s) = \\sum_{i=1}^N s_i\\).\nIf the probability of heads is \\(q \\in (0,1)\\), then\n\\[P(\\{s\\}) = \\prod_{i=1}^N q^{s_i}(1-q)^{1-s_i} = q^x (1-q)^{N-x}\\]\nwhere \\(x = \\sum_{i=1}^N s_i\\). Let \\(X(s) = \\sum_{i=1}^N s_i\\).\nThe probability of getting heads \\(x\\) times in \\(N\\) coin tosses is\n\\[P(X=x) = P(\\{ s \\in S \\colon X(s) = x \\}) = \\sum_{s \\in S} P(\\{ s\\})\n\\mathbb 1(X(s) = x)\\] \\[ = \\sum_{s \\in S} q^x (1-q)^{N-x} \\mathbb 1 (X(s)=x)\\] \\[ {N \\choose x} q^x (1-q)^{N-x}\\]\n\\(X\\) is said to follow the binomial distribution with parameters \\(N\\) and \\(q\\). This is denoted by writing \\(X \\sim \\text{Binomial}(N,q)\\).\nWe often think of \\(X\\) as a random quantity, but formally it is a function.\nExercise 1. Prove a generalized Bonferroni inequality:\n\\[ P \\left( \\cap_{i=1}^n A_i \\right) \\geq \\sum_{i=1}^n P(A_i) - (n-1) \\quad \\text{ for any events } A_1, ..., A_n.\\]\nProof 1. We proceed by induction. The base case is clear: \\(P(A_1) \\geq P(A_1) - (1-1)\\).\nAssume an inductive hypothesis: \\[P(\\cap_{i=1}^n A_i) \\geq \\sum_{i=1}^n P(A_i) - (n-1).\\]\nNow we can write that \\[P\\left(\\bigcap_{i=1}^{n+1} A_i \\right) = P\\left(A_{n+1} \\cap \\bigcap_{i=1}^n A_i \\right).\\]\nRecall Boole’s inequality: \\(P(B \\cap A^c) = P(B) - P(A \\cap B)\\), and then let \\(B = \\cap_{i=1}^n A_i\\) and \\(A^c = A_{n+1}\\). Then \\[\\begin{aligned}\nP\\left(A_{n+1} \\cap \\bigcap_{i=1}^n A_i \\right) & =\nP\\left(\\bigcap_{i=1}^n A_i\\right) - P\\left(A^c_{n+1} \\cap \\bigcap_{i=1}^n A_i\\right) \\\\\n& \\geq \\sum_{i=1}^n P(A_i) - (n-1) - P\\left(A^c_{n+1} \\cap \\bigcap_{i=1}^n A_i \\right). \\quad (\\star)\n\\end{aligned}\\]\nNow let’s apply Boole’s inequality again to the right-most probability, this time letting \\(B = A^c_{n+1}\\) and \\(A^c = \\bigcap_{i=1}^n A_i\\).\n\\[\nP\\left(A^c_{n+1} \\cap \\bigcap_{i=1}^n A_i \\right) =\nP(A^c_{n+1}) - P\\left(A^c_{n+1} \\cap \\left( \\bigcap_{i=1}^n A_i\\right)^c \\right).\n\\]\nApplying the fact that \\(P(A) = 1-P(A^c)\\) for any set \\(A\\), we then have that \\[P(A^c_{n+1}) = 1 - P(A_{n+1}) \\quad (\\star) \\]\nSubstituting that back in, we have that:\n\\[\nP\\left(A^c_{n+1} \\cap \\bigcap_{i=1}^n A_i \\right) =\n1 - P(A_{n+1}) - P\\left(A^c_{n+1} \\cap \\left( \\bigcap_{i=1}^n A_i\\right)^c \\right).\n\\]\nAnd then substituting that back into \\((\\star)\\):\n\\[P\\left( \\bigcap_{i=1}^{n+1} A_i \\right) \\geq\n\\sum_{i=1}^n P(A_i) - (n-1) -\n\\left[ 1 - P(A_{n+1}) - P\\left(A^c_{n+1} \\cap \\left( \\bigcap_{i=1}^n A_i\\right)^c \\right) \\right]\n\\] \\[\n= \\sum_{i=1}^n P(A_i) - (n-1) -\n1 + P(A_{n+1}) + P\\left(A^c_{n+1} \\cap \\left( \\bigcap_{i=1}^n A_i\\right)^c \\right)\n\\] \\[\n= \\sum_{i=1}^{n+1} P(A_i) - (n+1-1) + \\underbrace{P\\left(A^c_{n+1} \\cap \\left( \\bigcap_{i=1}^n A_i\\right)^c \\right)}_{\\geq 0 \\, \\text{ by the axioms of probability}}\n\\] \\[\n\\geq \\sum_{i=1}^{n+1} P(A_i) - (n+1-1)\n\\]\nThis concludes the proof.\nProof 2. We proceed by proof by contradiction: Assume that \\[P\\left( \\bigcap_{i=1}^n A_i \\right) < \\sum_{i=1}^n P(A_i) - (n-1)\\] \\[P\\left( \\bigcap_{i=1}^n A_i \\right) < 1 - n + \\sum_{i=1}^n P(A_i)\\] \\[P\\left( \\bigcap_{i=1}^n A_i \\right) < 1 - n + \\sum_{i=1}^n 1 - P(A_i^c)\\] \\[P\\left( \\bigcap_{i=1}^n A_i \\right) < 1 - n + n - \\sum_{i=1}^n P(A_i^c)\\] \\[P\\left( \\bigcap_{i=1}^n A_i \\right) < 1 - \\sum_{i=1}^n P(A_i^c)\\]\nBut this contradicts (the un-generalized version of) Bonferroni’s inequality. \\(\\rightarrow \\leftarrow\\). This concludes the proof.\nExercise 2. Given \\(B \\subset C\\) and \\(P(B) > 0\\), prove that \\[P(A | C) > P(A | B) \\Longleftrightarrow P(A | C \\cap B^c) > P(A | B).\\]\nExercise 3. Prove or disprove the following statement: For events \\(A\\) and \\(B\\) such that \\(0 < P(A) < 1\\) and \\(0 < P(B) < 1\\), \\[P(A|B) > P(A) \\Longleftrightarrow P(B | A) > P(B|A^c).\\]\n1-2pm Office Hours for Miller, Building 1 Room 245"
  },
  {
    "objectID": "week2/week2.html#jackpot-scenario",
    "href": "week2/week2.html#jackpot-scenario",
    "title": "Week 2",
    "section": "Jackpot Scenario",
    "text": "Jackpot Scenario\nWhen the pot reached $5M, a “rolldown” occurred in which the prizes for matching 3, 4, or 5 numbers were 10x higher.\n\n\n\nMatch Number\n6\n5\n4\n\n\n\n\nProbability\n1/13,983,815\n1/54,201\n1/1032\n\n\nAll prizes (in 2 years)\n15\n2158\n117685\n\n\nPrize (normal)\nJackpot\n$2,500\n$100\n\n\nPrize in Fall\nJackpot not hit\n$25,000\n$1,000\n\n\n\nIf there were a rolldown in the WINfall lottery, we’d have an expected return on a single ticket as:\n\n(1/54201)*25000 + (1/1032)*1000 + (1/57)*(50)\n\n[1] 2.307431"
  },
  {
    "objectID": "week2/week2.html#hypergeometric-distribution",
    "href": "week2/week2.html#hypergeometric-distribution",
    "title": "Week 2",
    "section": "Hypergeometric Distribution",
    "text": "Hypergeometric Distribution\nThe Hypergeometric\\((N,K,n)\\) distribution gives the probability of matching \\(k\\) numbers from a set of \\(K\\) winning numbers when selecting \\(n\\) from a set of \\(N\\) total.\nWe often say distribution instead of probability measure.\nIn the Winfall lottery, \\(N = 49\\), \\(K=6\\), and \\(n=6\\), and the outcome is the number of matches \\(k\\).\nLetting \\(P\\) denote the Hypergeometric\\((N,K,n)\\) distribution, \\[P(\\{k\\}) = \\frac{{K \\choose k}{N - K \\choose n - k}}{N \\choose n}.\\]\nPlugging \\(k=3, k=4, k=5, k=6\\) into this formula yields the probabilities in the above table."
  },
  {
    "objectID": "week2/week2.html#playing-cards",
    "href": "week2/week2.html#playing-cards",
    "title": "Week 2",
    "section": "Playing Cards",
    "text": "Playing Cards\nWhat’s the probability of drawing 4 cards from a deck and getting 4 aces?\nThe number of possible hands of 4 cards is \\({52 \\choose 4}\\).\nThus the probability of getting all 4 aces is\n\\[\\frac{1}{52 \\choose 4} = \\frac{4!48!}{52!}.\\]\nOr we could think about it sequentially: The probability of drawing an ace on the first card is \\(4/52\\), the next is \\(3/51\\), then \\(2/50\\), and \\(1/49\\) each conditioned on assuming we previously drew an ace card.\n\n\\[ \\frac{4}{52} \\times \\frac{3}{51} \\times \\frac{2}{50} \\times \\frac{1}{49} = \\frac{4!48!}{52!} = \\frac{4! \\cancel{48 \\cdot 47 \\cdots 1}}{52 \\cdot 51 \\cdot 50 \\cdot 49 \\cdot \\cancel{48 \\cdot 47 \\cdots 1}} \\]\nDefinition. The conditional probability of \\(A\\) given \\(B\\) denoted \\(P(A|B)\\) is\n\\[ P(A|B) = \\frac{P(A \\cap B)}{P(B)}\\]\nWe could think of \\(B\\) as the event where 1 ace has already been drawn (and thus represents the scenario where \\(B\\) consists of all the cards except 1 ace). Then,\n\\[P(A | B) =\n\\frac{P(\\{ \\text{Ace} \\clubsuit, \\text{Ace} \\diamondsuit, \\text{Ace} \\spadesuit, \\text{Ace} \\heartsuit\\} \\cap B\\})}{P(B)} = \\frac{3}{51} \\]"
  },
  {
    "objectID": "week2/week2.html#bayes-rule",
    "href": "week2/week2.html#bayes-rule",
    "title": "Week 2",
    "section": "Bayes’ Rule",
    "text": "Bayes’ Rule\n\\[P(A|B) = \\frac{P(A \\cap B)}{P(B)}.\\]\nMultiplying by \\(P(B)\\) yields:\n\\[P(A \\cap B) = P(A|B) P(B).\\]\nBy symmetry, \\[P(A \\cap B) = P(B|A) P(A).\\]\nIf \\(P(A) > 0\\) and \\(P(B) > 0\\), then \\[P(A|B) = \\frac{P(B|A) P(A)}{P(B)}.\\]\nIf \\(A_1, A_2, ...\\) form a partition of the sample space then\n\\[P(A_i |B) = \\frac{P(B|A_i) P(A_i)}{\\sum_{j=1}^\\infty P(B|A_j) P(A_j)}.\\]"
  },
  {
    "objectID": "week2/week2.html#the-monty-hall-problem",
    "href": "week2/week2.html#the-monty-hall-problem",
    "title": "Week 2",
    "section": "The Monty Hall Problem",
    "text": "The Monty Hall Problem\nOn a game show, there are 3 doors. Behind one door is a car, and behind the other two doors are goats. You win whatever is behind the door you select. First you pick door #1. The game show host then reveals that there is a goat behind door #3. The host then asks “Do you want to stay with #1 or switch to #2?” What would you do and why?\nIt turns out you should always switch. Let \\(D_1, D_2, D_3\\) denote the events that the car is behind door 1, 2, or 3. Let \\(M_1, M_2, M_3\\) denote the event that Monty opens door 1, 2, or 3, respectively.\nWe will assume that there’s equal probability of the car being behind 1, 2, or 3. Additionally, we will assume that he always opens a door that is not the one you picked and which does not have the car behind it.\nBy assumption \\[P(D_1) = P(D_2) = P(D_3) = 1/3.\\]\n\\[P(M_j|D_1) = (1/2) \\mathbb 1 (j \\in \\{2,3\\})\\] \\[P(M_j|D_2) = \\mathbb 1 (j = 3)\\] \\[P(M_j|D_3) = \\mathbb 1 (j = 2)\\]\nHere’s a table of the probability of each possible combination \\(i,j\\) of door that the car is behind \\((i)\\) and the door opened by Monty \\((j)\\):\n\n\n\n\nOpen #1\nOpen #2\nOpen #3\n\n\n\n\nCar in #1\n0\n1/6\n1/6\n\n\nCar in #2\n0\n0\n1/3\n\n\nCar in #3\n0\n1/3\n0\n\n\n\nBy the law of total probability, the probability that Monty opens door #3 is\n\\[P(M_3) = \\sum_{i=1}^3 P(M_3|D_i) P(D_i) = \\frac{1}{2} \\times \\frac{1}{3} +\n1 \\times \\frac{1}{3} + 0 \\times \\frac{1}{3} = \\frac{1}{2}.\\]\nSo by Bayes’ rule the conditional probability of the car being behind door #1, given Monty opened door #3, is\n\\[P(D_1|M_3) = \\frac{P(M_3|D_1) P(D_1)}{P(M_3)} = \\frac{(1/2) \\times (1/3)}{1/2} = \\frac{1}{3}\\]\nMeanwhile, the conditional probability of the car being behind door #2 given that Monty opened door #3 is\n\\[P(D_2 | M_3) = \\frac{P(M_3|D_2) P(D_2)}{P(M_2)} = \\frac{1 \\times (1/3)}{(1/2)} = \\frac{2}{3}\\]\n\nWhat’s wrong with the following reasoning?\nWe can think of the sample space as \\(\\{1,2,3\\}\\) and the outcome as the number that the door is behind, so \\(D_i = \\{i\\}.\\)\nBy conditioning on Monty opening door #3, we are conditioning on the event that the car is behind #1 or #2, that is \\(M_3 = D_3^c = \\{1,2\\}\\). Therefore\n\\[ P(D_1 | M_3) = \\frac{P(D_1 \\cap M_3)}{P(M_3)} \\] \\[ = \\frac{P(\\{1\\} \\cap \\{1,2\\})}{P(\\{1,2\\})} \\] \\[ = \\frac{1/3}{2/3} = \\frac{1}{2}.\\]\nFurther, \\(P(D_2|M_3) = 1 - P(D_1 | M_3) = 1/2\\) so we gain nothing by switching to door #2.\nThe problem is that this doesn’t condition on the fact that Monty will never choose the door with the car. In other words, the door Monty picks depends on your choice: when your door contains a goat, Monty only has one choice: the remaining door with a goat.\nIn essence, we need to define the joint-distribution of \\(D_i\\) and \\(M_j\\).\n\nThe intuition is that the probability gets squished into the other doors, so you get probabilities of \\(1/3\\) and \\(2/3\\)."
  },
  {
    "objectID": "week2/week2.html#independence",
    "href": "week2/week2.html#independence",
    "title": "Week 2",
    "section": "Independence",
    "text": "Independence\nIf we had a coin and we flipped it multiple times, where \\(A\\) represents the event where it comes up heads the first time and \\(B\\) is the event where it comes up heads the second time.\nWe would assume that \\(P(B|A) = P(B)\\).\nBy Bayes’ theorem, \\(P(B|A) = P(A \\cap B)/P(A)\\), that implies that \\(P(A \\cap B) = P(A)P(B)\\). When this holds, we say that \\(A\\) and \\(B\\) are independent.\nOne difference is that the second statement doesn’t require that \\(P(A)\\) is nonzero, but the definition using \\(P(B|A)\\) does.\nIf \\(A\\) and \\(B\\) are independent, then so are \\(A\\) and \\(B^c\\), \\(A^c\\) and \\(B\\), and \\(A^c\\) and \\(B^c\\).\nEvents \\(A_1, ..., A_n\\) are mutually independent if \\[P(\\cap_{i \\in I} A_i) = \\prod_{i \\in I} P(A_i)\\] for every subset \\(I \\subset \\{ 1, ..., n \\}.\\)\nYou might think that \\(P(A_1 \\cap \\cdots \\cap A_n) = P(A_1) \\cdots P(A_n)\\) would be a simpler definition of independence of multiple events, but this is not correct. See Casella & Berger (1.3.10)."
  },
  {
    "objectID": "week2/week2.html#continuing-on-random-variables",
    "href": "week2/week2.html#continuing-on-random-variables",
    "title": "Week 2",
    "section": "Continuing on Random Variables",
    "text": "Continuing on Random Variables\nIf the range of a random variable \\(\\{X(s) : s \\in S \\}\\) is countable then \\(X\\) is a discrete random variable.\nFor a discreet random variable, the function \\(f(x) = P(X = x)\\) is called a probability mass function.\nRecall how we define probability on random variables: \\[P(X = x)  = P(\\{ s \\in S : X(s) = x \\})\\]"
  },
  {
    "objectID": "week2/week2.html#cumulative-distribution-functions",
    "href": "week2/week2.html#cumulative-distribution-functions",
    "title": "Week 2",
    "section": "Cumulative Distribution Functions",
    "text": "Cumulative Distribution Functions\nThe cumulative distribution function (cdf) of \\(X\\) is defined to be \\[F(x) = P(X \\leq x).\\]\nDefinition. A function \\(F \\colon \\mathbb R \\to \\mathbb R\\) is a cdf if and only if\n\n\\(\\lim_{x \\to -\\infty} F(x) = 0\\) and \\(\\lim_{x \\to \\infty} F(x) = 1\\)\n\\(F(x)\\) is non-decreasing.\n\\(F(x)\\) is right-continuous, that is for all \\(x' \\in \\mathbb R\\) \\[ \\lim_{x \\downarrow x'} F(x) = F(x')\\]\n\nHere’s an example for a binomial distribution:\n\n\n\nBinomial distribution CDF\n\n\nFor any discrete probability distribution, the cdf will necessarily be discontinuous.\nAn example of a continuous cdf is \\[F(x) = (1- e^{-\\lambda x}) \\mathbb 1 (x > 0),\\] where \\(\\lambda\\) is a positive fixed number.\n\n\n\nExponential distribution CDF\n\n\n\nContinuous Random Variables\nA random variable is continuous if its cdf is a continuous function.\nIn other words, if \\(\\lim_{x \\to x'} F(x) = F(x')\\) for every \\(x' \\in \\mathbb R\\).\nA probability density function of a continuous random variable function \\(f: \\mathbb R \\to [0,\\infty)\\) such that for all \\(x \\in \\mathbb R\\), \\[F(x) = \\int_{-\\infty}^\\infty f(t) dt.\\]\nTherefore we need to integrate to get probabilities:\n\\[P(X \\in A) = \\int_A f(x) dx\\]"
  },
  {
    "objectID": "week2/week2.html#relationship-between-cdf-and-pdf",
    "href": "week2/week2.html#relationship-between-cdf-and-pdf",
    "title": "Week 2",
    "section": "Relationship between CDF and PDF",
    "text": "Relationship between CDF and PDF\n“Randavble” = “Random Variable” (Nice abbreviation, but a joke)\nSuppose we had a cdf \\(F\\), we could derive a pdf \\(f\\) by differentiating (usually).\nSimilarly, we can obtain \\(F\\) from \\(f\\).\nIf \\(f(x)\\) is continuous at \\(x\\), then \\[f(x) = \\frac{d}{dx} F(x),\\] by the fundamental theorem of calculus.\nA function \\(f: \\mathbb R \\to [0,\\infty)\\) is a probability density function if and only if \\(\\int f(x) dx = 1\\).\nIn fact any integrable non-negative function can be normalized to a pdf, by performing \\(f(x) = \\tilde{f}(x)/C\\) for a constant \\(C\\).\n\n\n\nNormalizing a pdf to have integral 1\n\n\nThere is not a unique pdf for a given distribution since the pdf can change arbitrarily on sets with Lebesgue measure 0. However, any two pdfs for the same distribution will agree “almost everywhere.”\nNot every continuous distribution has a pdf, since there are continuous cdfs that are nondifferentiable at uncountably many points, such as the Cantor function.\n\n\n\n\n\n\n\n\n\nRecall that the Exponential(\\(\\lambda\\)) distribution is defined by the cdf: \\[F(x) = (1-e^{-\\lambda x}) \\mathbb 1\\{x > 0\\}.\\]\nThe pdf can be obtained by differentiating:\n\\[f(x) = \\frac{d}{dx} F(x)= \\lambda e^{-\\lambda x} \\mathbb 1 \\{x > 0\\}.\\]\n\\(F(x)\\) is not differentiable at 0, but it doesn’t matter since \\(f(x)\\) can be defined arbitrarily on any countable set.\n\n\n\nThe exponential pdf\n\n\nFor measurable sets \\(A \\subset \\mathbb R\\), we define \\[X^{-1}(A) = \\{ s \\in S \\colon X(s) = x \\}\\]\nFor any random variable \\(X\\) not necessarily discrete or continuous, we define \\[P(X \\in A) = P(X^{-1}(A)).\\]"
  },
  {
    "objectID": "week2/week2.html#identically-distributed-random-variables",
    "href": "week2/week2.html#identically-distributed-random-variables",
    "title": "Week 2",
    "section": "Identically distributed random variables",
    "text": "Identically distributed random variables\nRandom variables \\(X\\) and \\(Y\\) are identically distributed if for every measurable set \\(A\\), \\[P(X \\in A) = P(Y \\in A).\\]\nThis is denoted by \\(P \\stackrel{d}{=} Y\\).\nIt turns out that \\(X \\stackrel{d}{=} Y\\) if and only if \\(F_X(x) = F_Y(x)\\) for all \\(x \\in \\mathbb R\\)."
  },
  {
    "objectID": "week2/week2.html#transformations-of-random-variables",
    "href": "week2/week2.html#transformations-of-random-variables",
    "title": "Week 2",
    "section": "Transformations of Random Variables",
    "text": "Transformations of Random Variables\nOften we are interested in functions \\(g(X)\\) of a random variable \\(X\\).\nIf \\(g\\) is measurable, then \\(g(X)\\) is a random variable.\nLetting \\(Y = g(X)\\), the distribution of \\(Y\\) is characterized by \\[P(Y \\in A) = P(g(X) \\in A) = P(X \\in g^{-1}(A)).\\]\nRecall that \\[g^{-1}(A) = \\{ x \\colon g(x) \\in A \\},\\] so this does not require that \\(g\\) be invertible.\nIn the discrete case,\n\\[f_Y(y) = P(Y = y) = P(g(X) = y) = \\sum_{x \\colon g(x) = y} f_X(x).\\]"
  },
  {
    "objectID": "week2/week2.html#binomial-variable-transformation-example",
    "href": "week2/week2.html#binomial-variable-transformation-example",
    "title": "Week 2",
    "section": "Binomial Variable Transformation Example",
    "text": "Binomial Variable Transformation Example\nSuppose \\(X \\sim \\text{Binomial}(N,q)\\) and \\(Y = N - X\\). Then \\[P(Y = k) = P(g(X) = k) = P(N - X = k) = P(X = N-k).\\]\nThus plugging in the pdf of \\(X\\):\n\\[ P(Y=k) = {N \\choose N-k} q^{N-k} (1-q)^{N-(N-k)}\\] \\[ = { N \\choose k} (1-q)^k q^{N-k}.\\]\nTherefore \\(Y \\sim \\text{Binomial}(N,1-q)\\)."
  },
  {
    "objectID": "week2/week2.html#transformations-of-continuous-random-variables",
    "href": "week2/week2.html#transformations-of-continuous-random-variables",
    "title": "Week 2",
    "section": "Transformations of continuous random variables",
    "text": "Transformations of continuous random variables\nFor a continuous r.v. \\(X\\) we have to take more care.\n\nIf \\(g\\) is invertible, then you might mistakenly think the pdf of \\(Y = g(X)\\) equals \\(f_X(g^{-1}(y))\\), but this is not true in general. The reason is that the pdf \\(f_Y\\) is a density, not a probability.\nThe correct formula accounts for the derivative.\nSuppose \\(X\\) is a continuous r.v. and let \\(\\mathcal X = \\{ x \\colon f_X(x) > 0 \\}\\). If \\(Y = g(X)\\) where \\(g \\colon \\mathcal X \\to \\mathbb R\\) is a strictly monotone function such that the inverse \\(g^{-1}(y)\\) has a continuous derivative then \\[f_Y(y) = f_X(g^{-1}(y)) \\left \\lvert \\frac{d}{dy}g^{-1}(y) \\right\\rvert\\]\nfor \\(y \\in \\mathcal Y \\coloneqq \\{ g(x) \\colon x \\in \\mathcal X \\}\\) and \\(f_Y(y) = 0\\) elsewhere.\n“This is a great source for exam problems!”\n\n\nMonotonicity\nA function is monotone increasing if \\[x < x' \\Longrightarrow g(x) \\leq g(x').\\] We also call this non-decreasing.\nSimilarly, a function is monotone decreasing (or non-increasing) if \\[ x < x' \\Longrightarrow g(x) \\geq g(x').\\]\nNote that Casella and Berger use “monotone” to imply strictly monotone, though the definitions above are the more conventional meaning."
  },
  {
    "objectID": "week2/week2.html#example-square-root-of-an-exponential-r.v.",
    "href": "week2/week2.html#example-square-root-of-an-exponential-r.v.",
    "title": "Week 2",
    "section": "Example: Square root of an Exponential r.v.",
    "text": "Example: Square root of an Exponential r.v.\nSuppose \\(X \\sim \\text{Exponential}(\\lambda)\\) and \\(Y=\\sqrt{X}\\).\nThen \\(f_X(x) = \\lambda \\exp(-\\lambda x) \\mathbb 1 \\{ x > 0 \\}\\), so \\(\\mathcal X = (0,\\infty).\\)\nWe write \\(Y = g(X)\\) where \\(g(x) = \\sqrt{x}\\) for \\(x \\in \\mathcal X\\).\nFor \\(y \\in \\mathcal Y = (0,\\infty)\\), the inverse of \\(g\\) is \\[g^{-1}(y) = y^2.\\]\n\\[\\frac{d}{dx} g^{-1}(y) = 2y.\\]\nThus by the change of variables formula above,\n\\[f_Y(y) = f_X(g^{-1}(y)) \\left\\lvert \\frac{d}{dx} g^{-1}(y) \\right\\rvert\\] \\[ = 2\\lambda y \\exp (-\\lambda y^2) \\mathbb 1 \\{ y > 0 \\}.\\]"
  },
  {
    "objectID": "week3/week3.html",
    "href": "week3/week3.html",
    "title": "Week 3",
    "section": "",
    "text": "Probability Distributions of Transformed Random Variables\nThe uniform distribution has a particularly important role in probability. We define the uniform distribution on \\((a,b)\\) where \\(a < b\\) as\n\\[f_U(u) = \\frac{1}{b-a} \\mathbb 1 ( a < u < b) \\]\nfor \\(u \\in \\mathbb R\\). This is denoted \\(U \\sim \\text{Uniform}(a,b)\\).\nThe cdf of \\(U \\sim \\text{Uniform}(a,b)\\) is\n\\[F_U(u) = \\left\\{ \\begin{array}{ll} 0 \\quad \\quad & \\text{ if } u \\leq a \\\\ (u-a)/(b-a) & \\text{ if } u \\in (a,b) \\\\ 1 & \\text{ if } u \\geq b \\end{array}\\right.\\]\nThe standard uniform distribution Uniform(0,1) has the special property that \\(CDF(u) = u\\).\nThe expected value denoted \\(\\mathbb E(X)\\) or \\(\\mathbb EX\\) is the average over all values that the random variable takes weighted according to their probability or probability density. It is also referred to as the expectation or mean of \\(X\\).\nLet \\(X\\) be a random variable and let \\(g(x)\\) be a measurable function. If \\(X\\) is discrete then the expected value is\n\\[\\mathbb Eg(X) = \\sum_{x \\in \\mathcal X} g(x) f(x)\\]\nWhere \\(\\mathcal X = \\{ X(s) : s \\in S \\}\\) is the range of \\(X\\) and \\(f(x)\\) is the pmf of \\(X\\).\nIf \\(X\\) is continuous then the expected value of \\(g(X)\\) is \\[\\mathbb Eg(X) = \\int_{-\\infty}^\\infty g(x) f(x) \\, dx\\] where \\(f(x)\\) is the pdf of \\(X\\)."
  },
  {
    "objectID": "week3/week3.html#change-of-variables-piecewise",
    "href": "week3/week3.html#change-of-variables-piecewise",
    "title": "Week 3",
    "section": "Change of variables piecewise",
    "text": "Change of variables piecewise\nOften \\(g(x)\\) is not strictly monotone, but is piecewise strictly monotone, e.g., \\(g(x) = x^2, \\, \\forall x \\in \\mathbb R\\).\nSuppose that \\(X\\) is a continuous random variable and let \\(\\mathcal X = \\{ x \\colon f_X(x) > 0 \\}\\).\nSuppose \\(g \\colon \\mathcal X \\to \\mathbb R\\) and \\(A_0, A_1, ..., A_k\\) is a partition of \\(\\mathcal X\\) such that\n\n\\(P(X \\in A_0) = 0\\),\n\\(g\\) is strictly monotone on \\(A_i\\) for \\(i = 1,...,k\\),\nthe inverse of \\(g\\) on \\(A_i\\), say \\(g_i^{-1}\\), has a continuous derivative on \\(g(A_i) = \\{ g(x) \\colon x \\in A_i \\}\\),\n\nthen\n\\[f_Y(y) = \\sum_{i = 1}^k f_X(g^{-1}_i(y)) \\left| \\frac{d}{dy} g_i^{-1}(y) \\right| \\mathbb 1(y \\in g(A_i)).\\]\n\n\n\n\n\nExample of partitioning a function into monotone sections\n\n\n\n\nWe were discussing in class whether or not the partitioning needs to be finite, and we think that one could use infinitely many partitions in some cases (say, for example, a distribution convolved with the \\(\\sin\\) function).\n\nExample: Normal to Chi-squared Transformation\nA random variable \\(X\\) has the standard normal distribution if\n\\[f_X(x) = \\frac{1}{\\sqrt{2\\pi}} \\exp(-\\frac{1}{2} x^2)\\] for \\(x \\in \\mathbb R\\). This is denoted \\(X \\sim \\mathcal N(0,1)\\).\nIf \\(X \\sim \\mathcal N(0,1)\\), then the random variable \\(Y = X^2\\) has the chi-squared distribution, denoted \\(Y \\sim \\chi^2\\).\n\\(g(x) = x^2\\) is strictly monotone on \\(A_1 = (-\\infty, 0)\\) and \\(A_2 = (0,\\infty)\\), wiith inverses \\(g_1^{-1}(y) = -\\sqrt{x}\\) and \\(g_2^{-1}(y) = \\sqrt{x}\\).\nBy the change of variables formula (in the piecewise case), for \\(y > 0\\) (remember we have to check when \\(y \\in g^{-1}(A_i)\\)),\n\\[f_Y(y) = \\frac{1}{\\sqrt{2\\pi}} \\exp(-\\frac{1}{2}(-\\sqrt{y})^2)\\left| \\frac{-1}{2\\sqrt{y}} \\right| +\n\\frac{1}{\\sqrt{2\\pi}} \\exp(-\\frac{1}{2}(\\sqrt{y})^2)\\left| \\frac{1}{2\\sqrt{y}} \\right| \\] \\[ = \\frac{1}{\\sqrt{2\\pi}} \\frac{1}{\\sqrt{y}} \\exp(-\\frac{1}{2} y).\\]\nAlong the way, we had to evaluate \\(\\frac{d}{dy} (-\\sqrt{y}) = \\frac{d}{dy} (-y^{1/2}) = \\frac{-1}{2}y^{-1/2}.\\)\nWe could write this ever-so-slightly more precisely:\n\\[ = \\left\\{ \\begin{array}{ll} \\frac{1}{\\sqrt{2\\pi}} \\frac{1}{\\sqrt{y}} \\exp(-\\frac{1}{2} y) \\quad & \\text{ if } y > 0 \\\\\n0 & \\text{ otherwise } \\end{array} \\right.\\]\n\nThese kinds of problems are great for exams!"
  },
  {
    "objectID": "week3/week3.html#probability-integral-transform",
    "href": "week3/week3.html#probability-integral-transform",
    "title": "Week 3",
    "section": "Probability Integral Transform",
    "text": "Probability Integral Transform\nLet \\(X\\) be a random variable with cdf \\(F\\). If \\(F\\) is a continuous function, then \\(F(X) \\sim \\text{Uniform}(0,1)\\).\nThis is called the probability integral transform.\nExample: Let \\(X \\sim \\text{Exponential}(\\lambda)\\). The cdf of \\(X\\) is \\[F(x) = (1-\\exp(-\\lambda x)) \\mathbb 1(x > 0),\\] which is a continuous function. Therefore, \\[1 - \\exp(-\\lambda X) \\sim \\text{Uniform}(0,1).\\]\nWhy can we drop the \\(\\mathbb 1 (X > 0)\\) factor? Because \\(X\\) is an exponential variable, so \\(X > 0\\) with probability 1. This is like asking \\[(1-e^{-\\lambda x}) \\mathbb 1 (X > 0) \\stackrel{d}{=} (1-e^{-\\lambda x})?\\]\nCan we simplify further? Yes! \\(\\text{Uniform}(0,1) = 1 - \\text{Uniform}(0,1)\\).\nSo we can rewrite this:\n\\[\\exp{-\\lambda x} = \\text{Uniform}(0,1).\\]"
  },
  {
    "objectID": "week3/week3.html#generalized-inverse-of-a-cdf",
    "href": "week3/week3.html#generalized-inverse-of-a-cdf",
    "title": "Week 3",
    "section": "Generalized Inverse of a cdf",
    "text": "Generalized Inverse of a cdf\nA cdf \\(F\\) can fail to be invertible in two ways:\n\n\\(F(x) = F(y)\\) for some \\(x \\neq y\\) (it is flat in some region), or\n\\(F\\) is discontinuous at some \\(x\\) (it has a jump at some point).\n\nThe generalized inverse of a cdf \\(F\\) is the function\n\\[G(u) = \\inf\\{x \\in \\mathbb R\\colon F(x) \\geq u \\}\\] for \\(u \\in (0,1).\\) We write \\(F_{-1}\\) to denote this function.\nWhen \\(F\\) is invertible, the generalized inverse equals the inverse, so there is no conflict in notation."
  },
  {
    "objectID": "week3/week3.html#inverse-probability-integral-transform",
    "href": "week3/week3.html#inverse-probability-integral-transform",
    "title": "Week 3",
    "section": "Inverse Probability Integral Transform",
    "text": "Inverse Probability Integral Transform\nLet \\(F\\) be any cdf. If \\(U \\sim \\text{Uniform}(0,1)\\) then \\(F^{-1}(U)\\) is a random variable with cdf \\(F\\).\nThis is called the inverse probability integral transform or the Smirnov transform.\nThe two transforms can be summarized as follows. Suppose \\(U \\sim \\text{Uniform}(0,1)\\) and \\(X\\) is a random variable with cdf \\(F\\). Then\n\n\\(F^{-1}(U) \\stackrel{d}{=} X.\\)\n\\(F(X) \\stackrel{d}{=} U\\) if \\(F\\) is continuous, but not otherwise.\n\n\nActivity: What is the distribution of \\(F(X)\\) in this example?\nMy thoughts:\nIf \\[F(x) = \\left\\{ \\begin{array}{ll}\n0 \\quad & \\text{ if } x \\leq 0 \\\\\nx \\quad & \\text{ if } 0 < x < .5 \\\\\n1 & \\text{ if } .5 \\leq x\n\\end{array}\\right.\\]\nSo if we take the derivative, \\[\\frac{d}{dx} F(x) = f_X(x) = \\left\\{\n\\begin{array}{ll}\n0 \\quad & \\text{ if } x \\leq 0 \\\\\n1 \\quad & \\text{ if } 0 < x < .5 \\\\\n0 & \\text{ if } .5 \\leq x\n\\end{array}\n\\right. \\]\nAnother student pointed out that looking at the height of the jump, there’s \\(Pr(X = .5) = .5\\),\nSo is it \\(f_X(x) = \\mathbb 1(0 < x < 0.5) + .5 \\times \\delta_{x = .5}(x)\\)?\nThere is a small mistake. The delta-distribution should be at 1, because when \\(x=1\\), \\(F(x) = 1\\).\nSo \\[F(x) = \\left\\{ \\begin{array}{ll}\n1 & \\text{ with probability }\\frac{1}{2} \\\\\n\\text{Uniform}(0,\\frac{1}{2}) & \\text{ with probability } \\frac{1}{2}\n\\end{array} \\right.\\]\n\nWhy should we care about this? Well it’s really useful computationally for generating random numbers that have different probability distributions from the uniform distribution. The Mersenne-Twister algorithm is the state of the art for generating uniform distribution random samples, and then often to get random numbers from other distributions one employs the above type of transformations under-the-hood."
  },
  {
    "objectID": "week3/week3.html#properties-of-expectation",
    "href": "week3/week3.html#properties-of-expectation",
    "title": "Week 3",
    "section": "Properties of Expectation",
    "text": "Properties of Expectation\nLet \\(X\\) be a random variable with range \\(\\mathcal X\\) and let \\(g,h \\colon \\mathcal X \\to \\mathbb R\\) be measurable functions such that \\(\\mathbb Eg(X)\\) and \\(\\mathbb Eh(X)\\) exist and are finite.\nThe following properties hold:\n\n\\(\\mathbb E(c\\, g(X)) = c \\mathbb Eg(X)\\) for any \\(c \\in \\mathbb R\\).\n\\(\\mathbb E(g(X) + h(X)) = \\mathbb Eg(X) + \\mathbb Eh(X)\\)\nIf \\(g(x) \\leq h(x)\\) for all \\(x \\in \\mathcal X\\), then \\(\\mathbb Eg(X) \\leq \\mathbb Eh(X)\\).\n\n\nExpectation minimizes the mean squared error\nSuppose we want to choose one point \\(a\\) that predicts \\(Y\\) as closely as possible (for instance, as in regression).\nIf we define “close” in terms of mean squared error, \\(\\mathbb E(|Y - a|^2)\\), then \\(\\mathbb EY\\) is the optimal choice of \\(a\\).\nTo see this, observe that \\[\\mathbb E(|Y-a|^2) = \\mathbb E(Y^2 - 2aY + a^2)\\] \\[ = \\mathbb EY^2 - 2a\\mathbb EY + a^2.\\]\nTo minimize, set the derivative equal to zero:\n\\[ 0 = \\frac{d}{da} \\mathbb E(|Y-a|^2) = -2\\mathbb EY + 2a\\]\nand solve for \\(a\\) to obtain \\(a = \\mathbb EY\\).\n\n\nExistence of Expected Values\nThe positive part of \\(X\\), denoted \\(X^+\\), is \\(X^+ = \\max(X,0)\\).\nThe negative part of \\(X\\), denoted \\(X^-\\), is \\(X^- = -\\min(X,0)\\).\nNote that \\(X = X^+ - X^-\\) and \\(|X| = X^+ + X^-\\).\n\n \n\n\nExistence vs. Finiteness\nWe say that the expected value of \\(X\\) exists if either \\(\\mathbb EX^+ < \\infty\\) or \\(\\mathbb EX^- < \\infty\\) or both. This definition differs from Casella & Berger’s definition.\nIf \\(\\mathbb E|X| < \\infty\\) then \\(\\mathbb EX\\) exists.\n\nThis is a sufficient (but not necessary) condition for existence.\nRecall that \\(|X| = X^+ + X^-\\), so \\(\\mathbb E|X| = \\mathbb EX^+ + \\mathbb EX^-\\).\nTherefore if \\(\\mathbb E|X| < \\infty\\), then \\(\\mathbb EX^+ < \\infty\\) and \\(\\mathbb EX^- < \\infty\\), since \\(X^+ \\geq 0\\) and \\(X^- \\geq 0\\).\n\nIn fact, if \\(\\mathbb E|X| < \\infty\\), then \\(\\mathbb EX\\) is finite. That is, \\(\\mathbb EX \\in \\mathbb R\\). This is because \\(\\mathbb EX = \\mathbb EX^+ - \\mathbb EX^-\\).\n\nWhy does \\(\\mathbb E|X|\\) always exist? Because \\(|X|^- = 0\\) and hence \\(\\mathbb E|X|^- < \\infty\\), and therefore we satisfy the definition of existence for an expectation that either the expectation of the positive part or the negative part are finite.\nThe only time the expectation doesn’t exist is when the expectation of the positive part and negative part are both infinite (resulting in \\(\\mathbb EX = \\mathbb EX^+ - \\mathbb EX^- = \\infty - \\infty = \\text{undefined}\\))."
  },
  {
    "objectID": "week4/week4.html",
    "href": "week4/week4.html",
    "title": "Week 4",
    "section": "",
    "text": "Families of Distributions\nIn statistics, families of distributions play a key role. Many statistical methods are based on assuming that the data are distributed according to some family of distributions. Estimation and inference then proceeds by finding the parameters of the family that could plausibly have generated the observed data.\nFor instance, if one assumes that data \\(X_1, ..., X_n\\) are \\(\\mathcal N(\\mu, \\sigma^2)\\) distributed, then we could use maximum likelihood to estimate \\(\\mu\\) and \\(\\sigma^2\\) as:\n\\[\\hat \\mu = \\frac{1}{n} \\sigma_{i=1}^n X_i \\quad \\quad \\hat \\sigma^2 = \\frac{1}{n} \\sum_{i=1}^n (X_i - \\bar X)^2.\\]\nMany commonly used distributions have special properties that make them well-justified in particular applications.\nExamples:\nSelecting, combining, and/or transforming distributions according to the “physics” of the data generating process is important for good statistical modeling.\nWe often write the name of the distribution itself to denote the pdf/pmf. For instance, \\(\\text{Uniform(x|a,b)\\) denotes the pdf of \\(\\text{Uniform(a,b)}\\).\nIf the distribution \\(X\\) has been defined, say as \\(X \\sim \\text{Uniform}(a,b)\\), another shorthand is to write \\(p(x|a,b)\\) for the pdf/pmf.\nWe will often denote pdfs or pmfs as \\(p(\\cdot)\\) instead of \\(f(\\cdot)\\).\nWhen dealing with multiple r.v.s, say \\(X\\) and \\(Y\\), it is common to simply write \\(p(x)\\) and \\(p(y)\\) for the pdf/pmf of \\(X\\) and \\(Y\\), respectively, instead of \\(p_X(x)\\) and \\(p_Y(y)\\). In other words, the letters used (\\(x\\) or \\(y\\)) indicates which random variable we’re talking about."
  },
  {
    "objectID": "week4/week4.html#recap-well-defined-vs.-undefined-expectations",
    "href": "week4/week4.html#recap-well-defined-vs.-undefined-expectations",
    "title": "Week 4",
    "section": "Recap: Well-Defined vs. Undefined Expectations",
    "text": "Recap: Well-Defined vs. Undefined Expectations\nThe positive part of \\(X\\) is well-defined if either \\(E X^+ < \\infty\\) or \\(E X^- < \\infty\\).\nA random variable \\(X\\) has the Zeta(\\(s\\)) distribution for \\(s > 1\\), if it has pmf\n\\[f_X(k) = P(X = k) = \\frac{1}{\\zeta(s)k^s} \\mathbb 1 (k \\in \\mathbb \\{ 1, 2, ... \\})\\]\nwhere \\(\\mathbb \\zeta (s)\\) is the Riemann zeta function. The use of the Riemann zeta function may seem scary, but it’s really just acting as the normalizing constant here so that this is a proper pmf.\nSince \\(EX^- = E(-\\min(X,0)) = 0\\), \\(EX\\) is well-defined.\nRecall that \\(\\zeta(s) = \\sum_{i=1}^\\infty \\frac{1}{k^s}\\).\nHowever, if \\(s \\leq 2\\) then the mean is infinite:\n\\[EX = \\sum_{i=1}^\\infty k f_X(k) = \\sum_{i=1}^\\infty\n\\frac{1}{\\zeta (s) k^{s-1}} = \\infty.\\]\nNow suppose that \\(Y\\) is a discrete random variable with pmf\n\\[f_Y(k) = P(Y=k) = \\frac{1}{2ck^2} \\mathbb 1 (|k| \\in \\{ 1, 2, ... \\})\\]\nwhere \\(c = \\zeta(2)\\).\nThen \\(EY^+ = \\infty\\) and \\(EY^- = \\infty\\). For example,\n\\[EY^+ = \\sum_{k=0}^\\infty kP(Y^+ = k) = \\sum_{k=1}^\\infty \\frac{1}{2ck} = \\infty.\\]\nSo the mean of \\(Y\\) is not well-defined (or, undefined).\n\nCauchy Distribution Example\nA random variable \\(X\\) has the Cauchy(0,1) distribution if it has the pmf\n\\[f_X(x) = \\frac{1}{p} \\frac{1}{1+x^2}\\]\nfor \\(x \\in \\mathbb R\\). If \\(X \\sim \\text{Cauchy}(0,1)\\) then \\(EX\\) is undefined.\n\n\n\n\n\n\n\n\n\nThe Cauchy distribution has heavy tails, meaning it can take very large values with non-negligible probability."
  },
  {
    "objectID": "week4/week4.html#moments",
    "href": "week4/week4.html#moments",
    "title": "Week 4",
    "section": "Moments",
    "text": "Moments\nLet \\(k\\) be a positive integer. The \\(k\\)th moment of \\(X\\) is \\(E(X^k)\\). The \\(k\\)th central moment of \\(X\\) is \\(E((X-EX)^k)\\).\nThe variance of a random variable is the 2nd central moment:\n\\[\\text{Var}(X) \\stackrel{def}{=} E((X-EX)^2).\\]\n\\(\\text{Var}(X)\\) is sometimes denoted \\(\\sigma^2(X)\\) or simply \\(\\sigma^2\\).\nThe standard deviation of \\(X\\) is \\(\\sqrt{\\text{Var}(X)}\\).\nBoth the variance \\(\\sigma^2\\) and the standard deviation \\(\\sigma\\) quantify how spread out a distribution is. However, \\(\\sigma\\) is more interpretable since it is in the same units as \\(X\\).\n\nIf the mean is undefined for a distribution, the variance and standard deviaion will also be undefined, as in the Cauchy distribution. How might we quantify the spread of the distribution? One might use quantiles. Median absolute deviation. A really simple approach might be the difference between the 95th and 5th percentiles.\n\n\nProperties of Variance\n\nIf \\(\\text{Var}(X) < \\infty\\), then for any \\(a,b \\in \\mathbb R\\),\n\n\\[\\text{Var}(aX + b) = a^2 \\text{Var}(X).\\]\n\nA useful formula for the variance is \\(\\text{Var}(X) = EX^2 - (EX)^2\\).\nSuppose \\(Y\\) is an estimator of some quantity \\(y_0\\). Then the mean squared error is \\[mse = E(|Y - y_0|^2) = (EY - y_0)^2 + E((Y-EY)^2).\\]\n\n\\[ = \\text{bias}^2 + \\text{variance}\\]\n\nProof of 2. \\[\\text{Var}(X) = E((X - E X)^2)\\] \\[ = E(X X - X E X - E X X + (E X)^2)\\] \\[ = E X^2 - 2E XE X + (E X)^2\\] \\[ = E (X^2) - (E X)^2\\]\nProof of 1 using 2. Now apply the 2nd to the first question:\n\\[\\text{Var}(aX+b) = E((aX+b - E(aX+b))^2)\\]\nor \\[\\text{Var}(aX+b) = E((aX+b)^2) - E(aX+b)^2 \\] \\[ = E(a^2X^2+2abE X + b^2) - (aE X+b)^2 \\]\n\\[ =  (a^2E(X^2)+\\cancel{2abE X} + \\cancel{b^2}) - (a^2(E X)^2+\\cancel{2abE X}+\\cancel{b^2})\\] \\[ = a^2((E X^2)^2 - E(X)^2) \\] \\[ = a^2\\text{Var}(X)\\]\nProof of 1 using definitions. Using the 2nd central moment formula:\n\\[\\text{Var}(aX+b) = E((aX+b - E(aX+b))^2)\\] \\[ = E((aX+b - (aE X+b))^2)\\] \\[ = E((aX - (aE X)^2)\\] \\[ = a^2E((X - (E X)^2)\\] \\[ = a^2\\text{Var}(X)\\]\nProof of 3. Mean squared error is defined as \\[mse = E((Y - y_0)^2)\\]\nA nice trick is to add and subtract by the same thing.\n\\[ \\text{mse} = E((Y - EY + EY - y_0)^2)\\] \\[ = E((Y-EY)^2 + 2(Y-EY)(EY-y_0) + (EY - y_0)^2)\\] \\[ = E((Y-EY)^2) + 2\\underbrace{(EY-EY)}_{=0}(EY-y_0) + (EY - y_0)^2\\] \\[ = \\underbrace{E((Y-EY)^2)}_{\\text{variance}} + \\underbrace{(EY - y_0)^2}_{\\text{bias}^2}\\]\n\n\nA good illustration of the bias-variance tradeoff is in estimating the sample variance of normally distributed values.\nSuppose that \\(X_1, X_2, ... \\sim \\mathcal N(\\mu, \\sigma^2)\\).\n\\[\\bar x = \\frac{1}{n} \\sum_{i=1}^n X_i\\]\n\\[\\hat \\sigma^2 = \\frac{1}{n-1} \\sum_{i=1}^n (X_i-\\bar X)^2\\]\nThe above is unbiased, but it’s not the estimator with lowest mse. One can get a better estimator with lower mean-squared-error by using either \\(1/n\\) or \\(1/(n+1)\\). For more details on the \\(1/(n+1)\\) correction, look at page 351 in Casella and Berger.\nThe usual \\(1/(n-1)\\) correction is known as Bessel’s correction.\nEven further, suppose that \\(X_1, X_2, ... \\sim \\mathcal N(\\mu_i, \\sigma^2)\\).\nNaively, one would think that the best estimates for \\(\\hat \\mu_i\\) is just \\(X_i\\), but the James-Stein estimator/paradox shows that by decreasing the variance we can come up with estimators that have lower mean-squared-error.\n\nA common misperception is that bias is always bad. In fact, allowing some bias usually improves performance by reducing variance. This is especially important when building a prediction model. Less flexible models tend to have greater bias, since they cannot fit the distributions as closely. More flexible models tend to have greater variance, since they have more parameters to estimate. Since \\(\\text{mse} = \\text{bias}^2 + variance\\), there is a trade-off, and mse is minimized by setting the flexibility equal to some critical point."
  },
  {
    "objectID": "week4/week4.html#moment-generating-functions",
    "href": "week4/week4.html#moment-generating-functions",
    "title": "Week 4",
    "section": "Moment Generating Functions",
    "text": "Moment Generating Functions\nThe moment generating function (mgf) of a random variable \\(X\\) is\n\\[M_X(t) = E[e^{tX}]\\] for \\(t \\in \\mathbb{R}\\).\nThe mgf is said to exist if \\(M_X(t)\\) is finite in a neighborhood of zero. In other words, if there is some \\(h > 0\\) such that \\(M_X(t) < \\infty\\) whenever \\(|t| < h\\).\nThis terminology is a little weird since the function always exists but might be infinite.\nWhy is it called the “moment generating function”?\nFor all \\(k \\in \\{ 1, 2, 3, ... \\}\\),\n\\[EX^k = \\frac{d^k}{dt^k} M_X(t) \\lvert_{t=0.}\\]\nThat is, the \\(k\\)th moment of \\(X\\) equals the \\(k\\)th derivative of \\(M_X(t)\\) evaluated at \\(t=0\\).\nSo \\(M_X(t)\\) is a function from which one can “generate” the moments simply by differentiating and evaluating at \\(t=0\\).\n\nExponential Example\nIf \\(X \\sim \\text{Exponential}(\\lambda)\\), then for \\(|t| < \\lambda\\),\n\\[M_X(t) = E[e^{tx}] = \\int_0^\\infty \\exp(tx) \\lambda \\exp(-\\lambda x) dx\\] \\[ = \\lambda \\int_0^\\infty \\exp(-(\\lambda - t)x) dx\\]\n\\[= \\frac{\\lambda}{\\lambda - 1} \\int_0^\\infty (\\lambda - t)\\exp(-(\\lambda - t)x)dx\\]\nIn the last step, we multiplied and divided by \\(\\lambda - t\\) so that the inside is an exponential pdf with parameter \\(\\lambda - t)\\) (and thus has integral 1).\n\\[ = \\frac{\\lambda}{\\lambda - 1} < \\infty\\]\nfor \\(|t| < \\lambda.\\)\nWe can easily compute the moments of \\(X\\) using the mgf.\nWithout using the mgf, we’d have to use integration by parts to solve:\n\\[EX^k = \\int_0^\\infty x^k \\lambda e^{-\\lambda x} dx,\\] which could be a bit painful for larger \\(k\\).\nSo instead, using the mgf, we get that the 1st and 2nd moments are:\n\\[E X = \\frac{d}{dt} \\frac{\\lambda}{\\lambda - t} \\big\\lvert_{t=0} = \\frac{\\lambda}{(\\lambda - t)^2} \\big\\lvert_{t=0} = \\frac{1}{\\lambda}\\]\n\\[EX^2 = \\frac{d^2}{dt^2} \\frac{\\lambda}{\\lambda-t} \\big\\lvert_{t=0} = \\frac{d}{dt} \\frac{\\lambda}{(\\lambda - t)^2} \\big\\lvert_{t=0} = \\frac{2\\lambda(\\lambda- t)}{(\\lambda-t)^4} \\big\\lvert_{t=0} = \\frac{2}{\\lambda^2}. \\]\nThus the variance of \\(X\\) is\n\\[\\text{Var}(X) = EX^2 - (EX)^2 = \\frac{2}{\\lambda^2} - \\frac{1}{\\lambda^2} = \\frac{1}{\\lambda^2}.\\]\n\nCould it be that the moments still exist even if the mgf does not take on a finite value?\nRecall that \\[e^{tX} = \\sum_{k=0}^\\infty \\frac{(tX)^k}{k!} \\geq \\frac{t^kX^k}{k!} \\quad (X \\geq 0)\\]\nSo it might be that the moment generating function doesn’t exist while the moments themselves do exist.\nWe’ll get to the characteristic function soon:\n\\[\\phi_X(t) = E(e^{itX}).\\]\nAnd \\(|e^{itX}| = 1\\).\n\n\n\nUniqueness of Moments\n\\(X\\) has bounded support if \\(P(|X| < c) = 1\\) for some \\(c \\in \\mathbb{R}\\).\nSuppose \\(X\\) and \\(Y\\) have bounded support. Then \\(X \\stackrel{d}{=} Y\\) if and only if \\(EX^k = EY^k\\) for all \\(k \\in \\{ 1, 2, ... \\}\\).\nIf \\(M_X(t)\\) and \\(M_Y(t)\\) exist and are equal on a neighborhood of zero, then \\(X \\stackrel{d}{=} Y\\).\n\nThis does not hold in general for unbounded distributions. There’s such an example in Casella and Berger."
  },
  {
    "objectID": "week4/week4.html#differentiating-under-the-integral-sign",
    "href": "week4/week4.html#differentiating-under-the-integral-sign",
    "title": "Week 4",
    "section": "Differentiating under the integral sign",
    "text": "Differentiating under the integral sign\nOften we want to interchange the order of differentiation and integration.\nFor example, for mgfs:\n\\[\\frac{d^k}{dt^k} M_X(t) \\lvert_{t=0} = \\frac{d^k}{dt^k} E\\big(\\exp(tX)\\big) \\lvert_{t=0}\\] \\[ = E\\big(\\frac{d^k}{dt^k} \\exp(tX)\\lvert_{t=0}\\big)\\] \\[ = E\\big( X^k \\exp(tX) \\lvert_{t=0}\\big)\\] \\[ = E(X^k).\\]\nThis is using the fact that \\(\\frac{d}{dt} e^{tx} = x e^{tx}\\), and hence \\(\\frac{d^k}{dt^x} e^{tx} = x^k e^{tx}\\).\nThe step where we swap the order of \\(\\frac{d^k}{dt^k}\\) and \\(E\\) is called differentiating under the integral sign.\nHowever, regularity conditions are needed for this to hold.\nSuppose that \\(f(x,t)\\) is differentiable with respect to \\(t\\) for each \\(x\\), and there exists a function \\(g(x,t)\\) such that\n\nfor all \\(x\\) and all \\(t'\\) in a neighborhood of \\(t\\) \\[\\left\\lvert \\frac{\\partial }{\\partial t} f(x,t) \\lvert_{t=t'} \\right\\rvert \\leq g(x,t)\\]\n\\(\\int_{-\\infty}^\\infty g(x,t) dx < \\infty\\). Then \\[\\frac{d}{dt} \\int_{-\\infty}^\\infty f(x,t) dx = \\int_{-\\infty}^\\infty \\frac{\\partial}{\\partial t} f(x,t) dx.\\]\n\nSee Casella & Berger (Theorem 2.4.3) for a slightly more general version. This proof uses one of the most important results in measure theory: the dominated convergence theorem.\nWe present a non-measure theoretic version of the dominated convergence theorem here, which is not fully general but gets the main idea across.\nSuppose that \\(f(x,t)\\) is continuous at \\(t_0\\) for each \\(x\\) and there exists \\(g(x)\\) such that\n\n\\(|f(x,t)| \\leq g(x)\\) for all \\(x\\) and all \\(t\\), and\n\\(\\int_{-\\infty}^\\infty g(x) dx < \\infty\\).\n\nThen \\[\\lim_{t\\to t_0} \\int_{-\\infty}^\\infty f(x,t) dx = \\int_{-\\infty}^{\\infty} \\lim_{t\\to t_0} f(x,t) dx.\\]\nThe dominated convergence theorem allows us to justify switching the order of limits and integrals.\nWe can think about the dominated convergence theorem as describing a situation where we have a sequence of functions:\n\\[f_1(x),\\, f_2(x),\\, f_3(x),\\, \\cdots\\]\nAnd what we’re saying is \\[\\lim_{n \\to \\infty} \\int f_n(x) dx = \\int \\left( \\lim_{n \\to \\infty} f_n(x))\\right) dx.\\]\n\n\n\n\n\n\n\n\n\n\nA counter-example would be \\(f_n(x) = 1/n\\), so \\(f_*(x) = 0\\), but \\(\\int f_n(x) dx > 0\\) for all \\(n\\).\nAnother counter-example is \\(f_n(x) = \\mathbb 1(n < x < n+1)\\). The limit \\(f_*(x) = 0\\) because for every \\(x\\), as \\(n\\to \\infty\\), there is an \\(N \\in \\mathbb N\\) such that for all \\(N' > N\\) \\(f_{N'}(x) = 0\\).\nThis is what the requirements around the existence of such a function \\(g(x)\\) are telling us (that \\(g(x)\\) is an envelope for all \\(f_n(x)\\) and \\(\\int_{-\\infty}^\\infty g(x) dx < \\infty\\)."
  },
  {
    "objectID": "week4/week4.html#independence-of-random-variables",
    "href": "week4/week4.html#independence-of-random-variables",
    "title": "Week 4",
    "section": "Independence of Random Variables",
    "text": "Independence of Random Variables\nRandom variables \\(X_1, ..., X_n\\) are independent if\n\\[P(X_1 \\in A_1, ..., X_n \\in A_n) = P(X_1 \\in A_1) \\cdots P(X_n \\in A_n)\\] for all measurable subsets \\(A_1, ..., A_n \\subset \\mathbb{R}\\).\nThe \\(\\text{Bernoulli}(q)\\) distribution is the special case of \\(\\text{Binomial}(N,q)\\) when \\(N=1\\).\nIf \\(X_1, ..., X_n \\sim \\text{Bernoulli}(q)\\) are independent, then\n\\(\\sum_{i=1}^n X_i \\sim \\text{Binomial}(N,q).\\)\nFrom this, it is easy to derive the mean of the Binomial distribution:\n\\[\\mathbb{E}\\left( \\sum_{i=1}^n X_i \\right) = \\sum_{i=1}^n \\mathbb{E}X_i = \\sum_{i=1}^n q = nq.\\]\n\nPoisson Distribution\nThe \\(\\text{Poisson}(\\lambda)\\) distribution has pmf\n\\[p(x|\\lambda) = e^{-\\lambda} \\frac{\\lambda^x}{x!} \\mathbb 1(x \\in \\mathcal X)\\]\nwhere \\(\\mathcal X = \\{ 0, 1, 2,...\\}\\). The parameter \\(\\lambda > 0\\) is referred to as the rate, for reasons that will become clear when we study Poisson processes.\nThe mean and variance of \\(X \\sim \\text{Poisson}(\\lambda)\\).\nThe Poisson model is often a good model for counting the occurrences of independent rare events.\nExamples:\n\nIn genomics, the number of reads covering a given locus is well-modeled as Poisson.\nIn physics, the number of photons hitting a detector during a given period of time is Poisson distributed.\nIn ecology, the number of organisms in a given region is often well-modeled as Poisson.\n\nThis is all due to a special property of the Poisson distribution jokingly referred to as the “law of small numbers”.\nThe Poisson is a limit of Binomials: if \\(q_N \\in (0,1)\\) is such that \\[N q_N \\to \\lambda\\] as \\(N \\to \\infty\\) for some \\(\\lambda > 0\\), then for all \\(x \\in \\{ 0, 1, 2, ...\\}\\),\n\\[\\text{Binomial}(x|N,q_N) \\longrightarrow \\text{Poisson}(x|\\lambda).\\]\n\n\nGeometric Distribution\nThe \\(\\text{Geometric}(q)\\) distribution has pmf\n\\[p(x|q) = (1-q)^{x-1}q \\mathbb 1(x \\in \\mathcal X).\\]\n\\[\\mathbb{E}X = 1/q\\]\n\\[\\text{Var}(X) = \\frac{1-q}{q^2}.\\]"
  },
  {
    "objectID": "week5/week5.html",
    "href": "week5/week5.html",
    "title": "Week 5",
    "section": "",
    "text": "Families of Distribution (cont. Discrete Distributions)"
  },
  {
    "objectID": "week5/week5.html#poisson-as-limit-of-binomials-law-of-small-numbers",
    "href": "week5/week5.html#poisson-as-limit-of-binomials-law-of-small-numbers",
    "title": "Week 5",
    "section": "Poisson as limit of Binomials (“Law of small numbers”)",
    "text": "Poisson as limit of Binomials (“Law of small numbers”)\n\nx <- rbinom(n = 10000, size = 100, prob = .05)\ny <- dpois(x = seq(0,20), lambda = 5)"
  },
  {
    "objectID": "week5/week5.html#geometric-distribution",
    "href": "week5/week5.html#geometric-distribution",
    "title": "Week 5",
    "section": "Geometric Distribution",
    "text": "Geometric Distribution\nThe Geometric(q) distribution has pmf\n\\[p(x|q) = (1-q)^{x-1} q \\mathbb 1(x \\in \\mathcal X),\\]\nwhere \\(\\mathcal X = \\{ 1, 2,... \\}\\). The parameter \\(q \\in (0,1)\\).\nThe mean and variance of \\(X \\sim \\text{Geometric}(q)\\) are:\n\\[EX = 1/q\\] \\[\\text{Var}(X) = \\frac{1-q}{q^2}.\\]\n\nGambler’s Fallacy\nSuppose you are playing a game with dice and someone notices that 1 hasn’t been rolled yet. Since the proportion of times that 1 is rolled must converge to 1/6, they think there is a high probability of rolling a 1 next.\nDo you think that the probability of rolling 1 next is (a) higher than 1/6, (b) equal to 1/6, or (c) lower than 1/6?\nA statistician might say we actually don’t know that the die is fair, and would need to come up with some empirical evidence describing our uncertainty around the fairness of the die.\nHowever, in general, if we assume that the die is fair, then the correct answer is that the probability of rolling 1 does not depend on the prior rolls, and hence would be 1/6.\n\n\nMemorylessness property\nThe gambler’s fallacy is related to a special property of the Geometric distribution.\nSuppose you have flipped tails \\(t\\) times. What is the probability that it will take \\(\\geq x\\) more flips to get heads? It doesn’t matter that we got tails \\(t\\) times already!\nMemorylessness property: If \\(X \\sim \\text{Geometric}(q)\\), then for all integers \\(t, x \\geq 0\\),\n\\[P(X > t + x | X > t) = P(X > x).\\]\nGeometric distributions are the only discrete distributions on \\(\\{1,2,...\\}\\) satisfying this property.\n\nWhy would we say that a normal distribution doesn’t have this memorylessness property? Well, if \\(X \\sim \\mathcal N(\\mu, \\sigma^2)\\) then it’s not true that \\(X | X > x_0\\) is a normal distribution."
  },
  {
    "objectID": "week5/week5.html#uniform-distribution",
    "href": "week5/week5.html#uniform-distribution",
    "title": "Week 5",
    "section": "Uniform Distribution",
    "text": "Uniform Distribution\nThe uniform distribution from \\(a\\) to \\(b\\) has pdf\n\\[p(x| a,b) = \\frac{1}{b-a} \\mathbb 1(x \\in \\mathcal X)\\]\nwhere \\(\\mathcal X = (a,b).\\) The parameters are \\(a,b \\in \\mathbb R\\) with \\(a < b\\).\nThe mean and variance of \\(X \\sim \\text{Uniform}(a,b)\\) are:\n\\[EX = (a+b)/2\\] \\[\\text{Var}(X) = \\frac{(b-a)^2}{12}.\\]"
  },
  {
    "objectID": "week5/week5.html#normal-gaussian-distribution",
    "href": "week5/week5.html#normal-gaussian-distribution",
    "title": "Week 5",
    "section": "Normal (Gaussian) Distribution",
    "text": "Normal (Gaussian) Distribution\nThe \\(\\mathcal N(\\mu, \\sigma^2)\\) distribution has pdf\n\\[p(x | \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp\n\\left(\n-\\frac{1}{2\\sigma^2} (x-\\mu)^2\n\\right)\\]\nfor all \\(x \\in \\mathbb R\\). The parameters are \\(\\mu \\in \\mathbb R\\) and \\(\\sigma^2 > 0.\\)\nThe mean and variance of \\(X \\sim \\mathcal N(\\mu, \\sigma^2)\\) are \\[EX = \\mu\\] \\[\\text{Var}(X) = \\sigma^2.\\]\nBe careful that some people write \\(\\mathcal N(\\mu, \\sigma)\\) to denote the normal distribution with mean \\(\\mu\\) and standard deviation \\(\\sigma\\). This comes up when there’s a formula in the second parameter position.\n\nSpecial Properties of the Normal Distribution\nThe normal distribution’s most special property relates to the central limit theorem (CLT).\n\nCLT tells us that the sum of a large number of independent random variables is approximately normal.\nConsequently, many real-world quantities tend to be normally distributed.\nWhen designing models, the CLT helps us understand when a normal model would be appropriate.\n\nWhy would human height be roughly normal? Because there are so many little factors (huge numbers of genetic loci, specific environmental factors, etc.) that add up together to form individual observations leading to the variability observed being reminiscent of the CLT.\nAnalytic tractability:\n\nCalculations can often be done in closed form, making normal models computationally convenient. Normal distributions can be combined to build complex models that are still tractable, such as Kalman filters.\n\nFor details on the analytic properties of the normal distribution and how its “niceness” led to its derivation:\n\nhttps://link.springer.com/chapter/10.1007/978-0-387-46409-1_7\nhttps://www3.nd.edu/~rwilliam/stats1/x21.pdf\nhttps://maa.org/sites/default/files/pdf/upload_library/22/Allendoerfer/stahl96.pdf\nhttps://math.stackexchange.com/questions/384893/how-was-the-normal-distribution-derived"
  },
  {
    "objectID": "week5/week5.html#chi-squared-distribution",
    "href": "week5/week5.html#chi-squared-distribution",
    "title": "Week 5",
    "section": "Chi-Squared Distribution",
    "text": "Chi-Squared Distribution\nSuppose that \\(X_1, ..., X_n \\sim \\mathcal N(0,1)\\) independently.\nThe distribution of \\(\\sum_{i=1}^n X_i^2\\) is called the chi-squared distribution with \\(n\\) degrees of freedom, denoted \\(\\chi^2(n)\\) or \\(\\chi_n^2\\).\nThe chi-squared distribution comes up a lot in statistical hypothesis testing, for instance when performing a \\(t\\) test.\nIf \\(X_1,...,X_n \\sim \\mathcal N(\\mu, \\sigma^2)\\) independently, then\n\\[(1/\\sigma^2) \\sum_{i=1}^n (X_i - \\bar X)^2 \\sim \\chi^2(n-1).\\]\nWhy the \\(n-1\\) instead of \\(n\\)? One already uses one degree of freedom to estimate the sample mean.\nIt turns out that \\(\\chi^2(n)\\) is a special case of the Gamma distribution."
  },
  {
    "objectID": "week5/week5.html#gamma-distribution",
    "href": "week5/week5.html#gamma-distribution",
    "title": "Week 5",
    "section": "Gamma Distribution",
    "text": "Gamma Distribution\nThe Gamma(a,b) distribution with shape \\(a>0\\) and rate \\(b>0\\) has pdf\n\\[p(x | a,b) = \\frac{b^a}{\\Gamma(a)} x^{a-1}\\exp(-bx) \\mathbb 1(x \\in \\mathcal X)\\]\nwhere \\(\\mathcal X = (0,\\infty)\\). Here, \\(\\Gamma(a) = \\int_0^\\infty t^{a-1}e^{-t} dt.\\)$\nThe mean and variance of \\(X \\sim \\Gamma(a,b)\\) are:\n\\[EX = a/b\\]\n\\[\\text{Var}(X) = a/b^2.\\]\nBe careful that there is another parameterization of the Gamma distribution that is often used. (In fact, Casella & Berger seem to prefer this parameterization.)\n\\(\\text{Gamma}(a, \\theta)\\) with shape \\(a > 0\\) and scale \\(\\theta > 0\\) (where \\(1/\\theta\\) is the same as the rate parameter from before) has pdf:\n\\[p(x | a, \\theta) = \\frac{(1/\\theta)^a}{\\Gamma(a)} x^{a-1} \\exp(-x / \\theta) \\mathbb 1 (x \\in \\mathcal X)\\]\nwhere \\(\\mathcal X = (0,\\infty)\\). When coding and reading books, make sure one knows which is being used. In what follows, unless otherwise specified, we’ll use the shape and rate parameterization.\n\nGamma relationships\n\n\\(\\text{Gamma}(1,\\lambda) = \\text{Exponential}(\\lambda).\\)\nIf \\(X \\sim \\text{Gamma}(a,b)\\) then \\(cX \\sim \\text{Gamma}(a,b/c)\\).\nIf \\(X_i \\sim \\text{Gamma}(a,b)\\) independently for \\(i = 1,...,n\\), then \\[\\sum_{i=1}^n X_i \\sim \\text{Gamma}(a_1 + ... + a_n, b).\\]\n\\(\\text{Gamma}(n/2,1/2) = \\chi^2(n)\\), the chi-squared distribution with \\(n\\) degrees of freedom.\n\n\n\nExponential Distribution: Memorylessness Property\nThe exponential distribution is a special case of the Gamma distribution.\nThe exponential distribution has the special property that it is the only continuous distribution on \\((0,\\infty)\\) with this property.\nIf \\(X \\sim \\text{Exponential}(\\lambda)\\), then for all \\(x, t > 0\\), \\[P(X > t+x | X>t) = P(X>x).\\]\nThis is the same as the memorylessness property of the Geometric distribution, but in the continuous case.\nMemorylessness will come up again when we study stochastic processes, particularly Poisson processes."
  },
  {
    "objectID": "week5/week5.html#the-log-normal-distribution",
    "href": "week5/week5.html#the-log-normal-distribution",
    "title": "Week 5",
    "section": "The Log-Normal Distribution",
    "text": "The Log-Normal Distribution\nThe \\(\\text{LogNormal}(\\mu, \\sigma^2)\\) distribution has pdf\n\\[p(x | \\mu, \\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\frac{1}{x}\n\\exp\\left(-\\frac{1}{2\\sigma^2}(\\log(x) - \\mu)^2\\right) \\mathbb 1(x \\in \\mathcal X)\\] where \\(\\mathcal X = (0, \\infty)\\). The parameters are \\(\\mu \\in \\mathbb R\\) and \\(\\sigma^2 > 0\\).\nThe mean and variance of \\(X \\sim \\text{LogNormal}(\\mu, \\sigma^2)\\) are \\[EX = \\exp(\\mu + \\frac{1}{2}\\sigma^2)\\] \\[\\text{Var}(X) = \\exp(2\\mu + 2\\sigma^2) - \\exp(2\\mu + \\sigma^2).\\]\n\nLog-Normal Relationships\nIf \\(X \\sim \\text{LogNormal}(\\mu, \\sigma^2)\\), then \\(\\log(X) \\sim \\mathcal N(\\mu, \\sigma^2)\\).\nThe log-normal is very useful for regression models of nonnegative continuous outcomes.\nIn such applications, it is often preferable to parameterize in terms of \\(\\theta = \\log(EX)\\) instead of \\(\\mu = E\\log(X)\\):\n\\[\\theta = \\log(EX) = \\mu + \\frac{1}{2}\\sigma^2.\\]\nWhile the log-normal looks similar to the Gamma distribution, the log-normal tends to be better behaved for regression.\nSuppose \\(X \\sim \\text{Gamma}(a,b)\\) and \\(Y = \\log(X)\\). Then the pdf of \\(Y\\) is asymmetric and has a factor of \\(\\exp(-be^y)\\), which makes it strongly disfavor larger values of \\(y\\).\nJensens Inequality tells us that if we have a convex function like \\(g(X) = -\\log (X)\\), we have that \\[g(EX) \\leq Eg(X).\\]"
  },
  {
    "objectID": "week5/week5.html#beta-distribution",
    "href": "week5/week5.html#beta-distribution",
    "title": "Week 5",
    "section": "Beta distribution",
    "text": "Beta distribution\nThe \\(\\text{Beta}(a,b)\\) distribution with parameters \\(a,b > 0\\) has pdf\n\\[p(x | a,b) = \\frac{1}{B(a,b)}x^{a-1}(1-x)^{b-1} \\mathbb 1(x \\in \\mathcal X),\\]\nwhere \\(\\mathcal X = (0,1)\\) and \\(B(a,b)\\) is the beta function,\n\\[B(a,b) = \\frac{\\Gamma(a)\\Gamma(b)}{\\Gamma(a+b)},\\]\nrecalling that \\(\\Gamma(a) = \\int_0^\\infty t^{a-1}e^{-t} dt\\)$ is the gamma function.\nThe mean and variance of \\(X \\sim \\text{Beta}(a,b)\\) are:\n\\[EX = \\frac{a}{a+b}\\]\n\\[\\text{Var}(X) = \\frac{ab}{(a+b)^2(a+b+1)}.\\]\n\\(\\text{Beta}(a,b)\\) often arises as the distribution of a random variable that is the probability of some event.\nIn Bayesian statistics, \\(\\text{Beta}(a,b)\\) is often used as a prior on probabilities \\(q\\), such as the parameters of Bernoulli, Binomial, Geometric, or Negative Binomial distributions.\nIf \\(X_1 \\sim \\text{Gamma}(a_1, b)\\) and \\(X_2 \\sim \\text{Gamma}(a_2, b)\\) are independent then \\(X_1 / (X_1 + X_2) \\sim \\text{Beta}(a_1, a_2)\\)."
  },
  {
    "objectID": "week5/week5.html#location-scale-families",
    "href": "week5/week5.html#location-scale-families",
    "title": "Week 5",
    "section": "Location-Scale Families",
    "text": "Location-Scale Families\nLocation-scale families are formed by starting from a single standard pdf \\(f(x)\\) and considering all pdfs of the form\n\\[f(x | m, s) = \\frac{1}{s} f\\left( \\frac{x - m}{s} \\right)\\]\nfor \\(m \\in \\mathbb{R}\\) and \\(s > 0\\). The location is \\(m\\) and the scale is \\(s\\).\nIf \\(X\\) has pdf \\(f(x)\\) then \\(sX + m\\) has pdf \\(f(x | m, s)\\) by the change of variable formula.\nIf \\(g(x) = sx + m\\), then \\(g^{-1}(y) = \\frac{y-m}{s}\\) and \\(\\frac{d}{dy}g^{-1} (y) = \\frac{1}{s}.\\)\n\\(\\mathcal N(\\mu, \\sigma^2)\\) is a location scale family with standard pdf \\[f(x) = \\mathcal N(x | 0,1) = \\frac{1}{\\sqrt{2\\pi}} \\exp(-\\frac{1}{2} x^2).\\]\nIf \\(X \\sim \\mathcal N(0,1)\\) then \\(\\sigma X + \\mu \\sim \\mathcal N(\\mu, \\sigma^2)\\)."
  },
  {
    "objectID": "week5/week5.html#laplace-distribution-aka-the-double-exponential",
    "href": "week5/week5.html#laplace-distribution-aka-the-double-exponential",
    "title": "Week 5",
    "section": "Laplace Distribution (aka the Double Exponential)",
    "text": "Laplace Distribution (aka the Double Exponential)\nThe \\(\\text{Laplace}(\\mu, s)\\) distribution has pdf\n\\[p(x | \\mu, s) = \\frac{1}{2s} \\exp\\left( -\\frac{1}{s} |x-\\mu|\\right),\\]\nfor all \\(x \\in \\mathbb R\\). The location is \\(\\mu \\in \\mathbb{R}\\) and the scale is \\(s > 0\\).\n\nlibrary(ggplot2)\nggplot(data.frame(x=seq(-4,4,.1)), aes(x)) + \n  stat_function(fun=\\(x) { (1/2)*exp(-abs(x)) }) + \n  theme_bw() + \n  ylab(\"f(x)\") + \n  ggtitle(\"Laplace Distribution\") \n\n\n\n\n\n\n\n\nThe mean and variance of \\(X \\sim \\text{Laplace}(\\mu, s)\\) are\n\\[EX = \\mu\\]\n\\[\\text{Var}(X) = 2s^2.\\]\nIf \\(X \\sim \\text{Laplace}(0,1)\\) then \\(sX+\\mu \\sim \\text{Laplace}(\\mu, s)\\).\nThe standard Laplace pdf is \\(f(x) = \\frac{1}{2} \\exp(-|x|).\\)\nThe Laplace distribution has heavier tails than the Gaussian but still has finite moments of all order, that is \\(E|X|^k < \\infty\\) for all \\(k \\in \\{ 1, 2, ... \\}\\).\nIt is called the “double exponential” because the Laplace(0,1) pdf is proportional to an Exponential(1) pdf reflected around 0.\nIf \\(X \\sim \\text{Laplace}(\\mu, s)\\) then \\(\\left \\lvert \\frac{X-\\mu}{s}\\right\\rvert \\sim \\text{Exponential}(1)\\)."
  },
  {
    "objectID": "week5/week5.html#cauchy-distribution",
    "href": "week5/week5.html#cauchy-distribution",
    "title": "Week 5",
    "section": "Cauchy Distribution",
    "text": "Cauchy Distribution\nThe Cauchy(m,s) distribution has pdf\n\\[p(x | m,s) = \\frac{1}{\\pi s\\left( 1 + \\left( \\frac{x-m}{s} \\right)^2 \\right)}\\]\nfor all \\(x \\in \\mathbb{R}\\). The location is \\(m \\in \\mathbb{R}\\) and the scale is \\(s > 0\\).\nNo moments of the Cauchy distribution are well-defined.\n\nWhy is it that the tails of the Cauchy distribution are so much larger than the normal distribution?\nThe density of the normal distribution are decaying like \\(\\mathcal O(e^{-x^2})\\), whereas the Cauchy distributions tails are decaying like \\(\\mathcal O(\\frac{1}{x^2})\\). Even worse is the log-Gamma distribution which decays like \\(e^{-e^x}\\).\n\nIf \\(X \\sim \\text{Cauchy}(0,1)\\) then $sX + m (m,s).\nEven though the Cauchy distribution looks roughly similar to a Gaussian, the Cauchy distributions have much heavier tails than Gaussian or Laplace distributions.\nWhat happens if you try to estimate \\(m\\) via the sample mean?\nIf \\(X_1,...,X_n \\sim \\text{Cauchy}(m,s)\\) independently, then\n\\[\\frac{1}{n} \\sum_{i=1}^n X_i \\sim \\text{Cauchy}(m,s).\\]\nIf \\(X,Y \\sim \\mathcal N(0,1)\\) independently, then \\(X/Y \\sim \\text{Cauchy}(0,1)\\)."
  },
  {
    "objectID": "week5/week5.html#exponential-families",
    "href": "week5/week5.html#exponential-families",
    "title": "Week 5",
    "section": "Exponential Families",
    "text": "Exponential Families\nExponential families are a unifying generalization of many common distributions and possess many nice properties.\nExamples include:\n\nBernoulli, Binomial, Poisson, Exponential, Beta, Gamma, Inverse-Gamma, Normal (Gaussian), Multivariate Gaussian, Log-Normal, Inverse Gaussian, Multinomial, Dirichlet\n\nNon-examples include:\n\nUniform, Cauchy, Students’ t-Distribution\n\nA one parameter exponential family is a collection of distributions indexed by \\(\\theta \\in \\Theta\\) with pdfs/pmfs of the form\n\\[p(x \\mid \\theta) = \\exp (\\phi(\\theta) t(x) - \\kappa(\\theta)) h(x)\\]\nfor some real-valued functions \\(\\phi(\\theta), t(x), \\kappa(\\theta),\\) and \\(h(x)\\).\n\\(h(x)\\) has to be non-negative.\n\\(k(\\theta)\\) is a log-normalization constant: since \\(\\int p(x | \\theta) dx = 1\\),\n\\[\\kappa(\\theta) = \\log \\int \\exp (\\phi(\\theta) t(x)) h(x) dx.\\]\n\\(t(x)\\) is called the sufficient statistic.\n\nExamples of One-parameter Exponential Families\n\nThe Exponential Distribution Family\nThe simplest example is the \\(\\text{Exponential}(\\theta)\\) distribution family:\nsince the pdfs are:\n\\[p(x \\mid \\theta) = \\theta e^{-\\theta sx} \\mathbb 1 (x > 0) =\n\\exp (\\phi(\\theta) t(x) - \\kappa(\\theta)) h(x),\\] for \\(\\theta \\in \\Theta = (0,\\infty)\\), where \\(t(x)= -x\\), \\(\\phi(\\theta) = \\theta\\), \\(\\kappa(\\theta) = -\\log(\\theta)\\), and \\(h(x) = \\mathbb 1 (x > 0)\\).\nWe could just as easily move the \\(-\\) sign on \\(t(x) = -x\\) to \\(\\phi(\\theta)\\), so there’s not necessarily a unique choice of \\(\\phi\\) and \\(t\\). There are multiple equivalent formulations. Sometimes people use different notation, such as \\[h(x) c(\\theta) e^{\\phi(\\theta)t(x)},\\]\nwhere \\(c(\\theta) = e^{-\\kappa(\\theta)}\\).\nIn this case we’re just defining \\(t(x)\\) to be the sufficient statistic, but there’s another sense in which a sufficient statistic is information that tells one all the information about a parameter in question — these turn out to be equivalent.\n\n\nThe Poisson Distribution Family\nThe \\(\\text{Poisson}(\\theta)\\) distributions form an exponential family since the pmfs are\n\\[p(x \\mid \\theta) = \\frac{\\theta^x e^{-\\theta}}{x!} \\mathbb 1 \\left(x \\in \\mathcal X\\right) = \\exp (x \\log (\\theta) - \\theta) \\frac{1}{x!} \\mathbb 1\\left(x \\in \\mathcal X\\right) = \\exp (\\phi(\\theta) t(x) - \\kappa(\\theta)) h(x)\\]\nfor \\(\\theta \\in \\Theta = (0,\\infty)\\), where \\(\\mathcal X = \\{0,1,2,...\\}\\), \\(t(x) = x\\), \\(\\phi(\\theta) = \\log(\\theta)\\), \\(\\kappa(\\theta) = \\theta\\), and \\(h(x) = \\frac{1}{x!} \\mathbb 1(x \\in \\mathcal X)\\)."
  },
  {
    "objectID": "week5/week5.html#multi-parameter-exponential-families",
    "href": "week5/week5.html#multi-parameter-exponential-families",
    "title": "Week 5",
    "section": "Multi-Parameter Exponential Families",
    "text": "Multi-Parameter Exponential Families\nAn exponential family is a collection of distributions indexed by \\(\\theta \\in \\Theta\\) with pdfs/pmfs of the form\n\\[p(x \\mid \\theta) = \\exp( \\phi(\\theta) t(x) - \\kappa(\\theta)) h(x)\n\\]\n\nGamma Example\nThe \\(\\text{Gamma}(a,b)\\) distributions, with \\(a, b > 0\\) are an exponential family:\n\\[p(x \\mid \\theta) = \\exp ( \\phi(\\theta)^T t(x) - \\kappa(\\theta)) h(x),\\]\nfor some vector-valued functions\n\\[\\phi(\\theta) = \\begin{pmatrix} \\phi_1(x) \\\\ \\vdots \\\\ \\phi_k(\\theta) \\end{pmatrix} \\quad \\text{ and } \\quad\n\\text{(}x) = \\begin{pmatrix} t_1(x) \\\\ \\vdots \\\\ t_k(x) \\end{pmatrix}\\]\n\\[\\begin{aligned}\n\\text{Gamma}(x \\mid a,b) & = \\frac{b^a}{\\Gamma(a)}x^{a-1} \\exp (-bx) \\mathbb 1(x > 0)  \\\\\n& = \\exp(\\phi(\\theta)^Tt(x) - \\kappa(\\theta)) h(x)\n\\end{aligned}\\]\nwhere \\(\\theta = (a,b)^T\\), \\(\\phi(\\theta) = (-b, a-1)^T\\), \\(t(x) = (x,\\log x)^T\\), and \\(h(x) = \\mathbb 1(x > 0)\\), and since \\(\\kappa(\\theta)\\) is always the normalizing constant, that’s where the \\(\\Gamma(a)\\) term goes.\nSo we’d have to get that \\[\\kappa(\\theta) = -a \\log b + \\log \\Gamma(a),\\] \\[\\text{since } e^{-\\kappa(\\theta)} = \\frac{b^a}{\\Gamma(a)}.\\]\n\n\nExponential Families: Special Properties\nThe entropy of a random variable \\(X\\) is\n\\[H(X) = -\\sum_{x \\in \\mathcal X} p(x) \\log(p(x)) \\quad \\text{ if $X$ is discrete},\\] \\[h(X) = -\\int p(x) \\log (p(x)) dx \\quad \\text{ if $X$ is continuous.}\\]\nExponential families have maximum entropy subject to a linear constraint. More precisely, among all distributions satisfying the constraint that \\(\\mathbb{E}t(X) = \\tau\\) for some \\(\\tau \\in \\mathbb{R}^k\\), an exponential family distribution with sufficient statistic \\(t(x)\\) has maximum entropy.\n\nCan we use this framework to show how the discrete uniform distribution is the maximum entropy distribution on a discrete set?\nYes — in that case, the idea is that there is no constraint \\(t(x)\\), or \\(t(x)\\) could be said to be a zero-dimensional vector.\n\n\n\nOne might be interested in the fact that entropy represents the average minimum number of bits needed to encode a message.\nhttps://en.wikipedia.org/wiki/Entropy_(information_theory)\nhttps://machinelearningmastery.com/what-is-information-entropy/\nhttps://math.stackexchange.com/questions/2299145/why-does-the-average-number-of-questions-bits-needed-for-storage-in-shannons-en\n\nThe rough interpretation is that if all you know is that \\(\\mathbb E t(X) = \\tau\\) then an exponential family distribution makes the fewest assumptions about the rest of that distribution.\nIn Bayesian statistics, exponential families are (incredibly) useful because they admit conjugate priors, facilitating posterior computation."
  },
  {
    "objectID": "week5/week5.html#random-vectors",
    "href": "week5/week5.html#random-vectors",
    "title": "Week 5",
    "section": "Random Vectors",
    "text": "Random Vectors\nIt’s often important to consider multiple random variables at a time, like clinical measurements on height, weight, age, sex, blood pressure, etc. on a set of subjects.\nA random vector is a function from the sample space to a \\(d\\)-dimensional real space.\nSometimes we denote a random vector by a single letter like \\(X\\), like \\[X(s) = (X_1(s), ..., X_d(s)) \\in \\mathbb{R}^d.\\]\nOther times we’ll use different letters for each entry, like \\((X,Y,Z)\\) that takes values \\((X(s), Y(s), Z(s)) \\in \\mathbb{R}^3\\).\nWe’ll return now to the Monty Hall problem to illustrate multiple random variables.\nLet \\(X\\) be the door that the car is behind, and \\(Y\\) is the door that Monty opens.\nThe pair \\((X,Y)\\) is a random vector taking values in \\(\\mathbb{R}^2\\) (or more specifically \\(\\{1,2,3\\} \\times \\{1,2,3\\}\\)).\nSince we chose door #1 at first, our assumptions imply that\n\\[X \\sim \\text{Uniform}(\\{1,2,3\\})\\] \\[P(Y = 2 | X = 1) = P(Y = 3 | X= 1) = 1/2\\] \\[P(Y = 3 | X = 2) = 1\\] \\[P(Y = 2 | X = 3) = 1\\]\nRecall that we wrote down the joint distribution in a table:\n\n\n\n\nMonty Opens Door #1\nOpen #2\nOpen #3\n\n\n\n\nCar is behind #1\n0\n1/6\n1/6\n\n\n… behind #2\n0\n0\n1/3\n\n\n… behind #3\n0\n1/3\n0\n\n\n\nThis table is implied by the assumptions written down above.\nThe probability of the event that \\(X = x, Y = y\\) is denoted \\(P(X = x, Y = y)\\)."
  },
  {
    "objectID": "week5/week5.html#joint-probability-mass-functions",
    "href": "week5/week5.html#joint-probability-mass-functions",
    "title": "Week 5",
    "section": "Joint Probability Mass Functions",
    "text": "Joint Probability Mass Functions\nA random vector is discrete if its range \\(X(S) \\subset \\mathbb{R}^d\\) is countable.\nThe joint pmf of a discrete random vector \\(X = (X_1, ..., X_d)\\) is\n\\[f_X(x) = f_X(x_1, ..., x_d) = P(X_1 = x_1, ..., X_d = x_d)\\]\nfor \\(x = (x_1, ..., x_d) \\in \\mathbb{R}^d\\).\nAs before, it’s common to drop subscripts and or use \\(p\\) instead of \\(f\\), as in:\n\\[p(x,y) = p_{X,Y}(x,y) = f(x,y) = f_{X,Y}(x,y).\\]\nIf \\(X \\in \\mathbb{R}^d\\) is a discrete random vector with range \\(\\mathcal X\\), then\n\\[P(X \\in A) = \\sum_{x \\in \\mathcal X \\cap A} p(x)\\]\n\\[E h(X) = \\sum_{x \\in \\mathcal X} h(x) p(x).\\]\nHere, though, \\(x = (x_1, ..., x_d)\\), so writing out the formulas more explicitly, \\[P((X_1, ..., X_d) \\in A) = \\sum_{(x_1, ..., x_d) \\in \\mathcal X \\cap A} p(x_1, ..., x_d),\\] \\[E h(X_1, ..., X_d) = \\sum_{(x_1, ..., x_d) \\in \\mathcal X} h(x_1, ..., x_d)p(x_1, ..., x_d).\\]\n\nExpected winnings in the Monty Hall problem\nSay the car is worth $30,000 and a goat is worth $150.\nIf you stick with door #1, then your winnings are\n\\[h(x,y) = 30000 \\times \\mathbb 1(x = 1) + 150 \\times \\mathbb 1(x \\neq 1),\\]\nso your expected winnings are\n\\[\\mathbb{E}h(X,Y) = 30000 \\times \\frac{1}{3} + 150 \\times \\frac{2}{3} = 10100.\\]\nIf you always switch doors, then your winnings are\n\\[h(x,y) = 30000 \\times \\mathbb 1 (x \\neq 1) + 150 \\times \\mathbb 1(x=1),\\]\nso your expected winnings are\n\\[\\mathbb{E}h(X,Y) = 30000 \\times \\frac{2}{3} + 150 \\times \\frac{1}{3} = 20050.\\]\nIt’s interesting to note that these expected values only depend on what door the car is behind (\\(X\\)), not what Monty does (\\(Y\\)).\n\nOne might say we’ve “conditioned” on our strategy in these two calculations. One could introduce another random variable \\(Z\\) to indicate our strategic choice, which we would be conditioning on.\n\n\n\nThe expected value of an indicator with respect to a random variable is always the probability of that event."
  },
  {
    "objectID": "week5/week5.html#marginal-probability-mass-functions",
    "href": "week5/week5.html#marginal-probability-mass-functions",
    "title": "Week 5",
    "section": "Marginal Probability Mass Functions",
    "text": "Marginal Probability Mass Functions\nFor a discrete random vector \\((X_1, ..., X_d)\\), the marginal pmf of \\(X_i\\), denoted \\(f_{X_i}(x_i)\\) or \\(p(x_i)\\), is just the pmf of the random variable \\(X_i\\).\nThat is \\[f_{X_i}(x_i) = p(x_i) = P(X_i = x_i).\\]\nWe call it a “marginal pmf” in the context of a joint distribution on multiple variables.\nThe marginal pmfs can be represented in terms of the joint pmf:\n\\[p(x) = \\sum_{y \\in \\mathcal Y} p(x,y)\\]\n\\[p(y) = \\sum_{x \\in \\mathcal X} p(x,y)\\]\nwhere \\(\\mathcal X\\) and \\(\\mathcal Y\\) are the ranges of \\(X\\) and \\(Y\\), respectively.\nIn general for a random vector, \\((X_1, ..., X_d)\\),\n\\[p(x_i) = \\sum_{x_{-i}} p(x_1, ..., x_d)\\]\nwhere the sum is over all values of\n\\[x_{-i} = (x_1, ..., x_{i-1}, x_{i+1}, ..., x_d).\\]\nReturning to the Monty Hall probability table:\n\n\n\n\nMonty Opens Door #1\nOpen #2\nOpen #3\n\n\n\n\nCar is behind #1\n0\n1/6\n1/6\n\n\n… behind #2\n0\n0\n1/3\n\n\n… behind #3\n0\n1/3\n0\n\n\n\nWe can read off the marginal pmfs from the table by summing across rows or columns. This is why they’re called marginal pmfs. And the joint probability for specific combinations of \\((X=x, Y=y)\\) are the individual cell entries."
  },
  {
    "objectID": "week5/week5.html#conditional-probability-mass-functions",
    "href": "week5/week5.html#conditional-probability-mass-functions",
    "title": "Week 5",
    "section": "Conditional Probability Mass Functions",
    "text": "Conditional Probability Mass Functions\nConsider a discrete random vector \\((X,Y)\\). For any \\(y\\) such that \\(p(y) > 0\\), the conditional pmf of \\(X\\) given \\(Y\\) is \\[f_{X|Y}(x \\mid y) = p(x \\mid y) = P(X = x \\mid Y = y) = \\frac{p(x,y)}{p(y)}.\\]\nLikewise for any \\(x\\) such that \\(p(x) > 0\\),\n\\[f_{Y|X}(y \\mid x) = P(Y = y \\mid X = x) = \\frac{p(x,y)}{p(x)}.\\]\nFor a discrete random vector \\((X_1, ..., X_d),\\)\n\\[p(x_i | x_{-i}) = \\frac{p(x_1, ..., x_d)}{p(x_{-i})}.\\]\nThe conditional pmf can be seen in a tabular fashion if we take the table and (un-) normalize it so that each row sums to 1 if we’re interested in conditioning on the row-variables, and vice-versa for the columns.\nIn other words, the conditional pmf \\(p(y|x) = p(x,y) / p(x)\\) can be seen as\n\n\n\n\n\n\n\n\n\n\nMonty Opens Door #1 (\\(Y = 1 | X=x\\))\nOpen #2 (\\(Y = 2 | X = x\\))\nOpen #3 (\\(Y = 3 | X = x\\))\n\n\n\n\nCar is behind #1 (\\(X = 1\\))\n0\n1/2\n1/2\n\n\n… behind #2 (\\(X = 2\\))\n0\n0\n1\n\n\n… behind #3 (\\(X = 3\\))\n0\n1\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\nMonty Opens Door #1 (\\(Y = 1\\))\nOpen #2 (\\(Y = 2\\))\nOpen #3 (\\(Y = 3\\))\n\n\n\n\nCar is behind #1 (\\(X = 1 | Y = y\\))\nundefined\n1/3\n1/3\n\n\n… behind #2 (\\(X = 2 | Y = y\\))\nundefined\n0\n2/3\n\n\n… behind #3 (\\(X = 3 | Y = y\\))\nundefined\n2/3\n0"
  },
  {
    "objectID": "week5/week5.html#joint-marginal-and-conditional-probability-density-functions",
    "href": "week5/week5.html#joint-marginal-and-conditional-probability-density-functions",
    "title": "Week 5",
    "section": "Joint, marginal, and conditional probability density functions",
    "text": "Joint, marginal, and conditional probability density functions\nBasically all the math translates over directly, as long as one isn’t worried about measure-theoretic questions.\nSuppose \\((X,Y)\\) is a bivariate random vector and \\(f(x,y) \\geq 0\\) is a function such that for all measurable \\(A \\subset \\mathbb{R}^2\\),\n\\[P((X,Y) \\in A) = \\int_{-\\infty}^\\infty \\int_{-\\infty}^\\infty\nf(x,y) \\mathbb 1((x,y) \\in A) \\text{d}x \\text{d}y.\\]\nThen \\(f(x,y)\\) is the joint pdf of \\((X,Y)\\).\nMore generally, \\((X_1, ..., X_d)\\) has joint pdf \\(f(x_1, ..., x_d)\\) if \\[P((X_1, ..., X_d) \\in A) = \\int_A f(x_1, ..., x_d) \\text{d}x_1 \\cdots \\text{d}x_d\\]\nfor all measurable \\(A \\subset \\mathbb{R}^d\\). Here \\(\\int_A\\) denotes integration over the set \\(A\\).\nIf such a function \\(f\\) exists, then the random vector is continuous.\nFor a random vector \\((X_1, ..., X_d)\\) with pdf \\(p(x_1, ..., x_d)\\),\n\\[\\mathbb{E}h(X_1, ..., X_d) = \\int h(x_1, ..., x_d) p(x_1, ..., x_d) \\text{d}x_1 \\cdots \\text{d}x_d.\\]\nUsually we write this more compactly as\n\\[\\mathbb{E}h(X) = \\int h(x) p(x) \\text{d}x,\\]\nwhere \\(X = (X_1, ..., X_d)\\) and \\(x = (x_1, ... x_d)\\).\nIf \\((X,Y)\\) is continuous, then \\(X\\) and \\(Y\\) are continuous random variables and their marginal pdfs are just their pdfs as random variables.\nMarginal pdfs can be expressed in terms of the joint pdf:\n\\[f_X(x) = p(x) = \\int p(x,y) \\text{d}y\\] \\[f_Y(y) = p(y) = \\int p(x,y) \\text{d}x\\]\nFor \\(y\\) such that \\(p(y) > 0\\), the conditional pdf of \\(X\\) given \\(Y\\) is defined as\n\\[f_{X|Y}(x|y) = p(x|y) = \\frac{p(x,y)}{p(y)}.\\]\nLikewise, for \\(x\\) such that \\(p(x) > 0\\),\n\\[f_{Y|X}(y|x) = p(y|x) = \\frac{p(x,y)}{p(x)}.\\]\n\nThere is something really subtle going on here: we’re not quite “conditioning” on the probability of the conditioned-variable taking on a particular value, since we’re using the density of the random variable in the denominator."
  },
  {
    "objectID": "week6/week6.html",
    "href": "week6/week6.html",
    "title": "Week 6",
    "section": "",
    "text": "Recap\nMarginal and Conditional pdfs\nIf \\((X,Y)\\) is continuous, then \\(X\\) and \\(Y\\) are continuous random variables and their marginal pfs are just their pdfs as random variables.\nMarginal pdfs can be expressed in terms of the joint pdf:\n\\[\nf_X(x) = p(x) = \\int p(x,y) dy\n\\]\n\\[\nf_Y(y) = p(y) = \\int p(x,y) dx\n\\]\nThis is similar to the discrete case, but with integrals.\nFor \\(y\\) such that \\(p(y) > 0\\), the conditional pdf of \\(X\\) given \\(Y\\) is defined as:\n\\[\nf_{X|Y} (x | y) = p(x|y) = \\frac{p(x,y)}{p(y)}.\n\\]\nLikewise, for \\(x\\) such that \\(p(x) > 0\\),\n\\[\nf_{Y|X}(y|x) = p(y|x) = \\frac{p(x,y)}{p(x)}.\n\\]\nThus, technically the conditional pdf is only defined when the marginal pdf is \\(>0.\\)\nRandom variables \\(X_1, …, X_n\\) are independent if\n\\[\nP(X_1 \\in A_1, ..., X_n \\in A_n) = P(X_1 \\in A_1) \\cdots P(X_n \\in A_n)\n\\]\nfor all (measurable) subsets \\(A_1, …, A_n \\subset \\mathbb R\\).\nIntuitively, independence captures the idea that \\(X_1, …, X_n\\) contain no information about one another.\nIn terms of the pdf/pmf, \\(X_1, …, X_n\\) are independent if\n\\[\nf(x_1, ..., x_d) = f_{X_1}(x_1) \\cdots f_{X_n}(x_d)\n\\]\nfor all \\(x_1, …, x_d\\).\nThis isn’t strictly an if-and-only-if statement. For pdfs, we won’t always be able to factor the pdf doesn’t mean that \\(X_1, …, X_n\\) are independent.\nLet \\(X, Y \\sim \\text{Uniform}(0,1)\\) be independent. Then we have that\n\\[f(x,y) = \\mathbb 1(0 < x,y< 1)\\]\n\\[\nf(x) = \\mathbb 1(0 < x < 1)\n\\]\n\\[\nf(y) = \\mathbb 1(0 < y < 1)\n\\]\n\\[\n\\tilde{f}(x) = \\mathbb 1(0 < x < 1) + \\underline{\\mathbb 1 \\left(x = \\frac{1}{2}\\right)}\n\\]\nThe underlined part adds nothing to the pdf since it occurs with measure zero. We couldn’t necessarily factor \\(f(x,y) = \\tilde{f}(x) f(y)\\).\nThe opposite of the statement above would require an additional caveat of “if there exists \\(f_{X_1}, … f_{X_d}\\).”"
  },
  {
    "objectID": "week6/week6.html#conditional-expectations",
    "href": "week6/week6.html#conditional-expectations",
    "title": "Week 6",
    "section": "Conditional Expectations",
    "text": "Conditional Expectations\nSuppose \\((X,Y)\\) is a random vector and \\(g(x)\\) is a measurable function. The conditional expectation of \\(g(X)\\) given that \\(Y = y\\) is\n\\[\n\\mathbb E(g(X) | Y = y) = \\sum_{x \\in \\mathcal X} g(x) f_{X|Y} (x|y)\n\\]\nin the discrete case and \\[\n\\mathbb E(g(X) | Y=y) = \\int g(x) f_{X|Y} (x|y) dx\n\\]\nin the continuous case.\nOften we abbreviate $\\(\\mathbb E (g(X) | Y = y)\\) as \\(\\mathbb E(g(X)|y)\\).\nAs before \\(\\mathbb{E}(g(X)|y)\\) is defined as long as \\(f(y) \\neq 0\\).\n\nCaveats, Interpretation\nWhen \\(Y\\) is a discrete r.v. the conditional expectation is equivalent to conditioning on \\(Y=y\\) as the notation suggests. However, if \\(Y\\) is a continuous random variable, the interpretation is much more subtle since it requires measure theoretic considerations."
  },
  {
    "objectID": "week6/week6.html#conditional-distributions",
    "href": "week6/week6.html#conditional-distributions",
    "title": "Week 6",
    "section": "Conditional Distributions",
    "text": "Conditional Distributions\nSuppose \\((X,Y)\\) is a random vector.\nThe conditional distribution of \\(X\\) given that \\(Y= y\\) is the probability measure \\(Q\\) such that\n\\[\nQ(A) = \\mathbb{E}\\left( \\mathbb 1 (X \\in A) \\mid Y = y \\right)\n\\]\nfor all measurable sets \\(A\\).\nThe conditional expectation \\(\\mathbb{E}(g(X) | Y = y)\\) can be thought of as the expected value of \\(g(X)\\) under the conditional distribution of \\(X\\) given \\(Y = y\\).\nThat is \\[\n\\mathbb{E}(g(X) | Y = y) = \\mathbb{E}g(\\tilde{X}) \\text{ where } \\tilde{X} \\sim Q.\n\\]\nThis is a very useful way to think of conditional expectations.\n\nBasic Properties\nIn particular,\n\n\\(\\mathbb{E}cg(X) | y) = c \\mathbb{E}(g(X) | y)\\) for any \\(c \\in \\mathbb{R}\\).\n\\(\\mathbb{E}(g(X) + h(X) | y) = \\mathbb{E}(g(X) | y) + \\mathbb{E}(h(X)|y)\\)\n\\(g(x) \\leq h(x) \\to \\mathbb{E}(g(X) | y) \\leq \\mathbb{E}(h(X)|y)\\).\n\n\n\nConditional expectations as random variables\nIt’s often useful to leave the condition as a random variable.\nConsider the function \\(h(y) = \\mathbb{E}(X|Y = y)\\). Then \\(h(Y)\\) is a random variable, denoted \\(\\mathbb{E}(X|Y)\\).\nA key property of conditional expectations is that\n\\[\n\\mathbb{E}(g(Y) X | Y) = g(Y) \\mathbb{E}(X|Y)\n\\]\nThe basic idea here is that if we were to condition on a particular value of \\(Y\\), then \\(g(Y)\\) would just be constant on the left-hand-side of the expectation.\n\n\nLaw of Total Expectation & Law of Total Variance\nConditional expectations follow two very useful properties:\nThe law of total expectation\n\\[\n\\mathbb{E}X = \\mathbb{E}(\\mathbb{E}(X \\mid Y))\\]\nAnd the law of total variance:\n\\[\n\\text{Var}X = \\mathbb{E}( \\text{Var}X \\mid Y ) + \\text{Var}( \\mathbb{E}(X \\mid Y ))\\]\n\nExercise: Derive these two properties.\nNote that \\[ \\mathbb{E}(\\mathbb{E}(X \\mid Y)) = \\mathbb{E}\\left(\\sum x f_{X|Y} (x|y)\\right)\n\\]\n\\[ \\mathbb{E}(\\mathbb{E}(X \\mid Y)) = \\sum_{y \\in \\mathcal Y} \\left(\\sum_{x \\in \\mathcal X} x f_{X|Y} (x|y)\\right) \\cdot P(Y = y)\n\\]\nIf I recall correctly, \\(P(X|Y) = P(X,Y)/P(Y)\\).\n\\[ \\mathbb{E}(\\mathbb{E}(X \\mid Y)) = \\sum_{y \\in \\mathcal Y} \\left(\\sum_{x \\in \\mathcal X} x P(X = x, Y = y)\\right)\n\\]\nInterchange ordering of summation:\n\\[ \\mathbb{E}(\\mathbb{E}(X \\mid Y)) = \\sum_{x \\in \\mathcal X} \\left(\\sum_{y \\in \\mathcal Y} x P(X = x, Y = y)\\right)\n\\]\nPull out the \\(x\\) from the inner sum:\n\\[ \\mathbb{E}(\\mathbb{E}(X \\mid Y)) = \\sum_{x \\in \\mathcal X} x \\left(\\sum_{y \\in \\mathcal Y} P(X = x, Y = y)\\right)\n\\]\n\\[ \\mathbb{E}(\\mathbb{E}(X \\mid Y)) = \\sum_{x \\in \\mathcal X} \\left( x P(X = x) \\right)\n\\]\n\\[ \\mathbb{E}(\\mathbb{E}(X \\mid Y)) = \\mathbb{E}X\n\\]\n\nIs there a way to apply\n\\[\n\\text{Var}X = \\mathbb{E}X^2 - (\\mathbb{E}X)^2 \\quad ?\n\\]\n\\[\n\\text{Var}X = \\mathbb{E}X^2 - (\\mathbb{E}(\\mathbb{E}(X \\mid Y)))^2\n\\]\n\n\n\nAnswers. Recall our definition:\n\\[\n\\mathbb{E}(X \\mid Y) = h(Y) \\quad \\text{where} \\quad h(y) = \\mathbb{E}(X | Y = y).\n\\]\nInsert them into the problem stem:\n\\[\n\\begin{aligned}\n\\mathbb{E}( \\mathbb{E}(X \\mid Y)) & = \\mathbb{E}h(Y) = \\sum_{y} h(y) p(y)  \\\\\n& = \\sum_y \\mathbb{E}(X \\mid Y = y) p(y) = \\sum_y \\sum_x x p(x \\mid y) p(y) \\\\\n& = \\sum_y \\sum_x x p(x,y) = \\sum_x x \\sum_y p(x,y) \\\\\n& = \\sum_x x p(x) = \\mathbb{E}X.\n\\end{aligned}\n\\]\nAside: How would one know what the expectation is with respect to in the outer expectations? The expectation is always with respect to whatever is random in the argument.\nAside #2: Isn’t the \\(h(Y)\\) notation a little bit confusing? What’s the intuition? Well, one can call \\(h(Y)\\) the “average value of \\(X\\) at a given level of \\(Y\\)”.\nMaybe a more intuitive way to look at it is to think about the usual setting where we’re regressing \\(Y\\) on \\(X\\)."
  },
  {
    "objectID": "week7/week7.html",
    "href": "week7/week7.html",
    "title": "Week 7",
    "section": "",
    "text": "Recap\nRecall that if \\((X,Y)\\) is a random vector and \\(g(x)\\) is a measurable function. The conditional expectation of \\(g(X)\\) given that \\(Y=y\\) is\n\\[\n\\mathbb E(g(X) | Y=y) = \\int_{x \\in \\mathcal X} g(x) f_{X|Y}(x|y) dx\n\\]\nWe have to be a bit careful with the notation \\(\\mathbb E[X|Y=y]\\) which might lead you to believe we’re conditioning on the probability that \\(Y=y\\), but the probability \\(Y=y\\) is vanishingly small (0), though there is a density for the probability distribution of \\(Y\\) at \\(y\\).\nSuppose the heights of adult women and men are \\(\\mathcal N(\\mu_1, \\sigma_1^2)\\) and \\(\\mathcal N(\\mu_2, \\sigma_2^2)\\), respectively.\nThen the pdf of heights for women and men combined is\n\\[\\pi \\mathcal N(\\mu_1, \\sigma_1^2) + (1-\\pi) \\mathcal N(\\mu_2, \\sigma_2^2)\n\\]\nwhere \\(\\pi\\) is the proportion of women in the population.\nThis is called a mixture of Gaussians.\nMore generally, mixture distributions are obtained by taking a weighted sum or integral of pdfs/pmfs.\nMixtures are a useful way of enrichiing a simple family of distributions."
  },
  {
    "objectID": "week7/week7.html#independence-and-dependence",
    "href": "week7/week7.html#independence-and-dependence",
    "title": "Week 7",
    "section": "Independence and Dependence",
    "text": "Independence and Dependence\n\\(X\\) and \\(Y\\) are independent if and only if for all (measurable) functions \\(g(x)\\) and \\(h(y)\\),\n\\[\n\\mathbb{E}(g(X)h(Y)) = \\mathbb{E}(g(X))\\mathbb{E}(h(Y))\n\\]"
  },
  {
    "objectID": "week7/week7.html#sums-of-gaussians",
    "href": "week7/week7.html#sums-of-gaussians",
    "title": "Week 7",
    "section": "Sums of Gaussians",
    "text": "Sums of Gaussians\nThis property of mgfs can simplify derivations of the distribution of a sum of independent random variables.\nThe mgf of \\(X \\sim \\mathcal N(\\mu, \\sigma^2)\\) is\n\\[\nM_X(t) = \\exp(\\mu t + \\frac{1}{s} \\sigma^2 t^2).\n\\]\nSuppose \\(X_1 \\sim \\mathcal N(\\mu_1, \\sigma^2_1)\\) and \\(X_2 \\sim \\mathcal N(\\mu_2, \\sigma^2_2)\\) independently. Then\n\\[\nM_{X_1 + X_2}(t) = M_{X_1}(t)M_{X_2}(t)\n\\]\n\\[\n=\\exp\\left( (\\mu_1 + \\mu_2) t + \\frac{1}{2}(\\sigma_1^2 + \\sigma^2_2) t^2\\right),\n\\]\nwhich is the mgf of \\(\\mathcal N(\\mu_1 + \\mu_2, \\sigma^2_1 + \\sigma^2_2)\\) for all \\(t\\).\nRecall that if two variables have the mgfs that are equal and finite around some interval of the origin, then they are the variables are equal in distribution. (It turns out that if two mgfs are finite and equal on a neighborhood around zero, they must be equal everywhere).\nTherefore we’re allowed to make the jump that \\[\nY \\sim \\mathcal N(\\mu_1 + \\mu_2, \\sigma_1^2 + \\sigma_2^2) \\Rightarrow X_1 + X_2 \\stackrel{d}{=} Y.\n\\]"
  },
  {
    "objectID": "week7/week7.html#law-of-total-expectation-law-of-total-variance",
    "href": "week7/week7.html#law-of-total-expectation-law-of-total-variance",
    "title": "Week 7",
    "section": "Law of Total Expectation & Law of Total Variance",
    "text": "Law of Total Expectation & Law of Total Variance\n\n\\(\\mathbb{E}X = \\mathbb{E}(\\mathbb{E}(X \\mid Y))\\)\n\\(\\text{Var}X = \\mathbb{E}(\\text{Var}(X \\mid Y)) = \\text{Var}(\\mathbb{E}(X\\mid Y))\\)\n\n\nLooking at claim 2, let’s expand the two terms on the right:\n\\[\n\\begin{aligned}\n\\mathbb{E}(\\text{Var}(X \\mid Y)) & = \\mathbb{E}(\\mathbb{E}(X^2 \\mid Y ) - \\mathbb{E}(X \\mid Y)^2) \\\\\n& = \\mathbb{E}(\\mathbb{E}(X^2 \\mid Y)) - \\mathbb{E}(\\mathbb{E}(X \\mid Y)^2) \\\\\n& = \\mathbb{E}(X^2) - \\mathbb{E}(\\mathbb{E}(X \\mid Y)^2) \\quad \\text{ by Law of Total Expectation}\n\\end{aligned}\n\\]\n\\[\n\\begin{aligned}\n\\text{Var}(\\mathbb{E}(X \\mid Y)) & = \\mathbb{E}(\\mathbb{E}(X \\mid Y)^2) - \\mathbb{E}(\\mathbb{E}(X \\mid Y))^2 \\\\\n& = \\mathbb{E}(\\mathbb{E}(X \\mid Y)^2) - (\\mathbb{E}X)^2 \\quad \\text{ by Law of Total Expectation}\n\\end{aligned}\n\\]\nTherefore,\n\\[\n\\begin{aligned}\n\\mathbb{E}(\\text{Var}(X \\mid Y)) + \\text{Var}(\\mathbb{E}(X \\mid Y)) & = \\mathbb{E}X^2 - (\\mathbb{E}X)^2 \\\\\n& = \\text{Var}X.\n\\end{aligned}\n\\]\n\n\n\nWe define conditional expectation before we define conditional probability. Why? It seems almost backwards?\nThere is measure-theoretic difficulty with continuous distributions. There are paradoxes that can come up if this is done too naively. This is called the Borel paradox.\n\nWikipedia article\nPhilosophical ramifications\nSlides\nWriteup\nPreprint\n\nThere was one study in the 1990s on tracking populations of fish, but it was based on a faulty construction of a conditional distribution due to the Borel paradox.\n\n\nExample: Compound Distributions\nSay you roll a die and it comes up \\(Y\\). Then you roll the die \\(Y\\) times, getting outcomes \\(X_1,...,X_Y\\). What is \\(\\mathbb{E}(\\sum_{i=1}^Y X_i)?\\).\nThis is two-stage procedure leads to something we call a compound distribution, which are also called mixture models or hierarchical model.\n\\[\n\\begin{aligned}\n\\mathbb{E}\\left( \\sum_{i=1}^Y X_i \\right) & = \\mathbb{E}\\left( \\mathbb{E}\\left( \\sum_{i=1}^Y X_i \\mid Y \\right) \\right) \\\\\n& = \\mathbb{E}\\left( \\sum_{i=1}^Y \\mathbb{E}(X_i \\mid Y ) \\right)  \\\\\n& = \\mathbb{E}(3.5 Y) = 3.5^2.\n\\end{aligned}\n\\]\n\\(Y\\) itself is a discrete uniform on \\(\\{1,...,6\\}\\) and each \\(X_i\\) is also a discrete uniform on \\(\\{1,...,6\\}\\), but the composition of the two is not.\nThe sum of a random number of random variables is said to follow a compound distribution.\n\n\nExample 2\nSuppose \\(N \\sim \\text{Poisson}(\\lambda)\\) and \\(X_1,...,X_N \\sim \\text{Geometric}(q)\\) independently given \\(N\\). Let \\(S = \\sum_{i=1}^N X_i\\).\nBy the law of total expectation\n\\[\\mathbb{E}S = \\mathbb{E}(\\mathbb{E}(S \\mid N)) = \\mathbb{E}( N / q) = \\lambda / q.\n\\]\nRemember that if two variables are independent, then \\(\\text{Var}(X + Y) = \\text{Var}(X) + \\text{Var}(Y)\\), which is also true of conditional variances.\nSo we can say that\n\\[\n\\text{Var}(S \\mid N) = \\sum_{i=1}^N \\text{Var}(X_i \\mid N) = N \\frac{1-q}{q^2}.\n\\]\nTherefore, by the law of total variance,\n\\[\n\\begin{aligned}\n\\text{Var}S & = \\mathbb{E}(\\text{Var}(S \\mid N)) + \\text{Var}(\\mathbb{E}(S \\mid N)) \\\\\n& = \\mathbb{E}(N(1-q)/q^2) + \\text{Var}(N/q) \\\\\n& = \\lambda(1-q)/q^2 + \\lambda/q^2 \\\\\n& = \\lambda(2-q)/q^2.\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "week7/week7.html#the-exchange-paradox",
    "href": "week7/week7.html#the-exchange-paradox",
    "title": "Week 7",
    "section": "The Exchange Paradox",
    "text": "The Exchange Paradox\nAt a carnival, there is a mysterious man with two envelopes on the table in front of him. He tells you that one of the envelopes has twice as much money as the other, and you can pick one. You randomly pick one envelope and find \\(x\\) dollars inside. Now, he says, you can keep that money or take the money in the other envelope.\nYour friend says you should switch, because you will either get \\(x/2\\) or \\(2x\\) dollars, each with probability 1/2, so the expected value of switching is\n\\[\n(1/2)(x/2) + (1/2)(2x) = 5x/4\n\\]\nWhat would you do?\n\nThe problem is that what’s in the envelope is not a random quantity.\nWe want to evaluate\n\\[\n\\mathbb{E}(\\text{How Much Money You'd Get} \\mid \\text{You Switch})\n\\]\nAnd we want to see if this is the same, larger, or smaller than:\n\\[\n\\mathbb{E}(\\text{How Much Money You'd Get} \\mid \\text{You Don't Switch})\n\\]\nLet \\(M\\) be the amount of money in the lower value envelope.\nIf you first pick the envelope with the higher amount, and you switch you’d get \\(M\\) money, and if you pick the envelope with the lower amount and switch, you’d get \\(2M\\) money.\n\\[\n\\mathbb{E}(\\text{How Much Money You'd Get} \\mid \\text{You Switch}) = (1/2)M + (1/2)2M\n\\]\nIf you pick the envelope with the higher amount and you don’t switch, then you get \\(2M\\) and you don’t …\n\\[\n\\mathbb{E}(\\text{How Much Money You'd Get} \\mid \\text{You Don't Switch}) = (1/2)M + (1/2)2M\n\\]\n\n\n\nLet \\(X =\\) the amount in the envelope selected and \\(Y =\\) amount in the other envelope.\nLet \\(m\\) and \\(2m\\) denote the (unknown) amount of money in the two envelopes.\nThen \\[P(Y = 2m \\mid X = m) = 1\n\\] \\[P(Y = m \\mid X = 2m) = 1\n\\]\n\\[P(X = m) = P(X = 2m) = 1/2.\\]\nThus by law of total expectation\n\\[\n\\begin{aligned}\n\\mathbb{E}Y & = \\mathbb{E}(\\mathbb{E}(Y \\mid X))  \\\\\n& = \\mathbb{E}(Y \\mid X=m)P(X=m) + \\mathbb{E}(Y \\mid X = 2m) P(X = 2m) \\\\\n& = (2m)(1/2) + m(1/2) = 3m/2.\n\\end{aligned}\n\\]\nMeanwhile,\n\\[\\mathbb{E}X = mP(X = m) + 2m P(X = 2m) = 3m/2.\n\\]\nThere is no advantage to switching.\nMorals: Write down your assumptions carefully, and realize that your intuition can lead you astray."
  },
  {
    "objectID": "week7/week7.html#discrete-case",
    "href": "week7/week7.html#discrete-case",
    "title": "Week 7",
    "section": "Discrete Case",
    "text": "Discrete Case\nSuppose \\((U,V) = g(X,Y)\\) for some function \\(g\\).\nIf \\((X,Y)\\) is discrete, then the joint pmf of \\((U,V)\\) is\n\\[f_{U,V}(u,v) = sum_{x,y} f_{X,Y} (x,y) \\mathbb 1(g(x,y) = (u,v)).\n\\]\nThis is really just the same as the univariate case, except that we are considering bivariate rather than univariate elements."
  },
  {
    "objectID": "week7/week7.html#continuous-case",
    "href": "week7/week7.html#continuous-case",
    "title": "Week 7",
    "section": "Continuous Case",
    "text": "Continuous Case\nIt is less obvious how to handle the case where \\((X,Y)\\) is continuous. Fortunately, however, there is still a nice formula.\nSuppose \\((X,Y)\\) is a continuous random vector, and \\((U,V) = g(X,Y)\\) for some function \\(g\\) such that\n\n\\(g\\) is one-to-one, with inverse \\(h(u,v) = (x,y)\\) on its range,\nthe partial derivatives of \\(g(x,y)\\) exist and are continuous\nthe Jacobian matrix \\(Dh\\) is nonsingular, where\n\n\\[Dh = \\begin{bmatrix}\n  \\frac{\\partial x}{\\partial u} & \\frac{\\partial x}{\\partial v} \\\\\n  \\frac{\\partial y}{\\partial u} & \\frac{\\partial y}{\\partial v}\n  \\end{bmatrix} =\n  \\begin{bmatrix}\n  \\frac{\\partial h_1}{\\partial u} & \\frac{\\partial h_1}{\\partial v} \\\\\n  \\frac{\\partial h_2}{\\partial u} & \\frac{\\partial h_2}{\\partial v}\n  \\end{bmatrix}.\n\\]\nThe joint pdf of \\((U,V)\\) is\n\\[f_{U,V}(u,v) = f_{X,Y}(h_1(u,v), h_2(u,v)) \\lvert \\det (Dh) \\rvert\n\\]\nfor \\((u,v)\\) in the range of \\(g(X,Y)\\) and is zero elsewhere.\n\\(g(x,y)\\) being one-to-one means that if \\((x,y) \\neq (x',y')\\) then \\(g(x,y) \\neq g(x',y')\\). A one-to-one function has a inverse from its range back to its domain.\nThe determinant factor is\n\\[\\lvert \\det (Dh) \\rvert = \\lvert \\frac{\\partial h_1}{\\partial u} \\frac{\\partial h_2}{\\partial v} - \\frac{\\partial h_1}{\\partial v} \\frac{\\partial h_2}{\\partial u} \\rvert.\n\\]\nSometimes \\(\\det (Dh)\\) is referred to as the Jacobian of \\(h\\), often denoted \\(J_h\\) or simply \\(J\\). The notation is not totally standard though, and sometimes \\(J_h\\) denotes the matrix \\(Dh\\)."
  },
  {
    "objectID": "week8/week8.html",
    "href": "week8/week8.html",
    "title": "Week 8",
    "section": "",
    "text": "Recap\nSuppose \\((X,Y)\\) is a random vector, and \\((U,V) = g(X,Y)\\) for some function \\(g\\).\nThat is \\(U = g_1(X,Y)\\) and \\(V = g_2(X,Y)\\).\nHow can we derive the joint pdf/pmf of \\((U,V)\\) from the joint pdf/pmf of \\((X,Y)\\)?\nThe discrete case is pretty straightforward, but in the continuous case we need to get comfortable with the Jacobian matrix.\nSuppose \\((X,Y)\\) is a continuous random vector and \\((U,V) = g(X,Y)\\) for some function \\(g\\) such that\n\\[Dh = \\begin{bmatrix}\n  \\frac{\\partial x}{\\parital u} & \\frac{ \\partial x}{\\partial v} \\\\\n  \\frac{\\partial y}{\\partial u} & \\frac{\\partial y}{\\partial v}\n  \\end{bmatrix} =\n  \\begin{bmatrix}\n  \\frac{\\partial h_1}{\\parital u} & \\frac{ \\partial h_1}{\\partial v} \\\\\n  \\frac{\\partial h_2}{\\partial u} & \\frac{\\partial h_2}{\\partial v}\n  \\end{bmatrix}\n  \\]\nThe determinant factor is \\[|\\det Dh| = \\lvert \\frac{\\partial h_1 }{\\partial u} \\frac{\\partial h_2}{\\partial v} - \\frac{\\partial h_1}{\\partial v} \\frac{\\partial h_2}{\\partial u } \\rvert.\n\\]\nSuppose \\(X,Y \\sim \\mathcal N(0,1)\\) independently and \\(\\rho \\in (-1, 1)\\).\nDefine \\(U = X\\) and \\(V = \\rho X + \\sqrt{1 - \\rho^2}Y\\). That is\n\\[u = g_1(x,y) = x\n\\]\n\\[v = g_2(x,y) = \\rho x + \\sqrt{1 - \\rho^2} y.\n\\]\nThe inverse is defined by\n\\[x = h_1(u,v) = u\n\\] \\[y = h_2(u,v) = \\frac{v - \\rho u }{\\sqrt{1-\\rho^2}}.\n\\]\nThus the Jacobian matrix is\n\\[\nDh =\n  \\begin{bmatrix}\n  \\frac{\\partial h_1}{\\partial u} & \\frac{ \\partial h_1}{\\partial v} \\\\\n  \\frac{\\partial h_2}{\\partial u} & \\frac{\\partial h_2}{\\partial v}\n  \\end{bmatrix} =\n  \\begin{bmatrix}\n  1 & 0 \\\\\n  -\\frac{\\rho}{\\sqrt{1 - \\rho^2}} & \\frac{1}{\\sqrt{1 - \\rho^2}}\n  \\end{bmatrix}.\n\\]\n\\[\\det Dh = \\frac{1}{\\sqrt{1 - \\rho^2}}.\n\\]\n\\[\n\\begin{aligned}\nf_{X,Y}(x,y) & = \\frac{1}{\\sqrt{2\\pi}}\\exp \\biggr ( -\\frac{1}{2} y^2\\biggr ) \\\\\n& = \\frac{1}{\\sqrt{2\\pi}}\\exp \\biggr ( -\\frac{1}{2} u^2\\biggr ) \\exp \\biggr ( - \\frac{1}{2} \\left( \\frac{v - \\rho u}{\\sqrt{1-\\rho^2}} \\right)^2 \\biggr )\n\\biggr \\lvert \\det Dh \\biggr \\rvert \\\\\n& = \\frac{1}{2\\pi \\sqrt{1-\\rho^2}}\\exp \\biggr ( -\\frac{1}{2} u^2 - \\frac{1}{2} \\left( \\frac{v^2 - 2v \\rho u + \\rho^2 u^2}{1-\\rho^2} \\right) \\biggr ) \\\\\n& = \\frac{1}{2\\pi \\sqrt{1-\\rho^2}} \\exp \\biggr ( -\\frac{1}{2 (1 - \\rho^2)} \\left( u^2 - 2\\rho uv + v^2 \\right) \\biggr ).\n\\end{aligned}\n\\]\nThe bivariate normal distribution with means \\(\\mu_X\\) and \\(\\mu_Y \\in \\mathbb R\\), variances \\(\\sigma_X^2, \\sigma_Y^2 > 0\\), and correlation \\(\\rho \\in (-1,1)\\) has pdf\n\\[f(x,y) = \\frac{1}{2 \\pi \\sigma_X \\sigma_Y \\sqrt{1-\\rho^2}} \\exp \\left(\n-\\frac{1}{2 (1-\\rho^2)} ( \\tilde x^2 - 2 \\rho \\tilde x\\tilde y + \\tilde y ^2)\n  \\right),\n\\]\nwhere\n\\[\\tilde x = \\frac{x - \\mu_X}{\\sigma_X} \\quad \\quad \\text{ and } \\quad \\quad \\tilde y = \\frac{y - \\mu_Y}{\\sigma_Y}\n\\]\nIf \\((X,Y)\\) have this bivariate normal distribution, then\n\\[\n\\begin{aligned}\n\\mathbb{E}(Y | X = x) & = \\mu_Y + \\rho \\sigma_Y \\frac{x - \\mu_X}{\\sigma X} \\\\\n\\text{Var}(Y | X = x ) & = (1 - \\rho^2) \\sigma_Y^2.\n\\end{aligned}\n\\]\nIn other words, the conditional pdf of \\(Y\\) given \\(X =x\\) is\n\\[p(y\\mid x) = \\mathcal N \\left( y \\mid \\mu_y + \\rho \\sigma_Y \\frac{x - \\mu_X}{\\sigma_X}, (1-\\rho^2) \\sigma_Y^2 \\right).\n\\]\nIf \\(X\\) and \\(Y\\) are each normally distributed, then \\((X,Y)\\) is not necessarily bivariate normal.\nOne example is where \\(\\rho = 1\\) or \\(\\rho = -1\\) since this violates the constraint that \\(\\rho \\in (-1,1)\\).\nBut what about a more satisfying counterexample?\nLet’s say \\(X \\sim \\mathcal N(0,1)\\) and\n\\[Y = \\left\\{ \\begin{array}{ll}\nX \\quad & \\text{ if } |X| \\leq 1 \\\\\n-X \\quad & \\text{ if } |X| > 1\n\\end{array} \\right. .\n\\]\nWe defined the bivariate normal distribution in term of its pdf, but there is a more general definition that we will use.\nDefinition: We will say that \\((X,Y)\\) is bivariate normal if \\(aX + bY\\) is normally distributed for \\(a,b \\in \\mathbb R\\).\nThis definition applies in the degenerate scenario where \\(\\rho = 1\\) or -1."
  },
  {
    "objectID": "week8/week8.html#correlation-and-covariance",
    "href": "week8/week8.html#correlation-and-covariance",
    "title": "Week 8",
    "section": "Correlation and Covariance",
    "text": "Correlation and Covariance\nWhen random variables \\(X\\) and \\(Y\\) are not independent, they are dependent. However, the dependence may be weak, or it may be strong. Correlation is an important way of quantifying the dependence between random variables. Covariance is a related concept that also depends on the scales of \\(X\\) and \\(Y\\).\nTo declutter the notation, let’s denote\n\\[\n\\begin{aligned}\n\\mu_X = \\mathbb{E}X \\quad & \\quad \\sigma_x = \\sqrt{\\text{Var}X } \\\\\n\\mu_Y = \\mathbb{E}Y \\quad & \\quad \\sigma_y = \\sqrt{\\text{Var}Y }\n\\end{aligned}\n\\]\nThe covariance of \\(X\\) and \\(Y\\) is\n\\[\n\\text{Cov}(X,Y) = \\mathbb{E}\\bigg( (X - \\mu_X) ( Y - \\mu_Y )\\bigg).\n\\]\nThe correlation of \\(X\\) and \\(Y\\) is\n\\[\n\\begin{aligned}\n\\rho_{X,Y} = \\text{Cor}(X,Y) & = \\frac{\\text{Cov}(X,Y)}{\\sigma_X \\sigma_Y} \\\\\n& = \\mathbb{E}\\left( \\left(\\frac{X - \\mu_X }{\\sigma_X}\\right) \\left(\\frac{Y - \\mu_Y}{\\sigma_Y}\\right) \\right).\n\\end{aligned}\n\\]\nThis is sometimes called the correlation coefficient, or Pearson’s correlation coefficient.\n\n\n\n\nExample of correlations from Wikimedia\n\n\nNote that in the center figure, division by zero (with \\(\\sigma_Y = 0\\)) implies that the correlation is undefined. Notably, that scenario would have covariance.\nIt’s important to acknowledge that correlation is not going to relate necessarily to the slope of a regression line because we are standardizing by \\(\\sigma_Y\\).\nProperties of \\(-1 \\leq \\rho_{X,Y} \\leq 1\\).\n\\(\\rho_{X,Y} > 0\\) implies a positive association (direct relationship); \\(\\rho_{X,Y} < 0\\) implies a negative association.\nIf \\(\\sigma_X = 0\\) or \\(\\sigma_Y = 0\\), then \\(\\rho\\) is undefiend.\n\\(|\\rho_{X,Y}| = 1\\) if and only if there exists \\(a \\neq 0\\) and \\(b \\in \\mathbb R\\) such that \\(P(Y = aX+b) = 1\\). The sign of \\(\\rho\\) equals the sign of \\(a\\).\nCorrelation captures the strength of the association in terms of how close to linear the relationship is, but not the magnitude of the slope.\nIf \\(X\\) and \\(Y\\) are independent, then \\(\\rho_{X,Y} = 0\\). However, if \\(\\rho_{X,Y} = 0\\) then \\(X\\) and \\(Y\\) are not necessarily independent.\n\nIf we want to show that independence \\(\\to \\rho = 0\\), we can see that by writing that we want to show:\n\\[\\text{Cov}(X,Y) = \\mathbb{E}\\biggr( (X - \\mu_X) (Y - \\mu_Y) \\biggr) = 0.\n\\]\nBecause \\(X\\) and \\(Y\\) are independent, and the expectation is over a product of a piece that’s just a function of \\(X\\) and another piece that’s just a function of \\(Y\\), we can write that\n\\[\n\\text{Cov}(X,Y) = \\mathbb{E}\\biggr( X - \\mu_X \\biggr)\\mathbb{E}\\biggr( Y - \\mu_Y \\biggr) = 0 \\times 0 = 0.\n\\]\n\\[\n\\Longrightarrow \\text{Cor}(X,Y) = \\rho_{X,Y} = 0\n\\]\n\nProperties of covariance\n\n\\(\\text{Cov}(X,Y) = \\rho_{X,Y} \\sigma_X \\sigma_Y\\)\n\\(\\text{Cov}(X,Y) = \\mathbb{E}XY - (EX)(EY)\\)\n\\(\\text{Cov}(X,X) = \\text{Var}X\\)\n\\(\\text{Cov}(aX + b, cY+d) = ac \\text{Cov}(X,Y)\\)\n\\(\\text{Var}(X + Y) = \\text{Var}(X) + \\text{Var}(Y) + 2 \\text{Cov}(X,Y)\\)\n\n\nShowing 1:\nObserve that \\(\\rho_{X,Y} = \\text{Cov}(X,Y) / (\\sigma_X \\sigma_Y)\\) implies this.\nShowing 2: We can write that\n\\[\n\\begin{aligned}\n\\text{Cov}(X,Y) & = \\mathbb{E}(XY - \\mu_X Y - \\mu_Y X + \\mu_X \\mu_Y) \\\\\n& = \\mathbb{E}(XY) - \\mathbb{E}(X) \\mathbb{E}(Y) - \\mathbb{E}(Y) \\mathbb{E}(X) + \\mathbb{E}(X)\\mathbb{E}(Y) \\\\\n& = \\mathbb{E}(XY) - \\mathbb{E}(X) \\mathbb{E}(Y).\n\\end{aligned}\n\\]\nShowing 3:\n\\[\\text{Cov}(X,X) = \\mathbb{E}((X - \\mu_X)(X-\\mu_X)) = \\mathbb{E}((X-\\mu_X)^2) = \\text{Var}(X).\n\\]\nShowing 5:\n\n\\[\n\\begin{aligned}\n\\text{Var}(X+Y) & = \\mathbb{E}((X+Y)^2) - \\mathbb{E}(X+Y)^2 \\\\\n& = \\mathbb{E}(X^2 + 2XY + Y^2) - (\\mathbb{E} X^2 + 2\\mathbb{E} X\\mathbb{E} Y+ \\mathbb{E} Y^2) \\\\\n& = \\mathbb{E}(X^2) + 2\\mathbb{E}(XY) + \\mathbb{E}(Y^2) - \\mathbb{E} X^2 - 2\\mathbb{E} X\\mathbb{E} Y- \\mathbb{E} Y^2 \\\\\n& = \\underbrace{\\mathbb{E}(X^2) - \\mathbb{E} X^2}_{\\text{Var}(X)} + \\underbrace{\\mathbb{E}(Y^2) - \\mathbb{E} Y^2}_{\\text{Var}(Y)} + \\underbrace{2\\mathbb{E}(XY) - 2\\mathbb{E} X\\mathbb{E} Y}_{=\\text{Cov}(X,Y) \\text{, by property 2}}\n\\end{aligned}\n\\]\n\n\n\nThe simpler way to derive 5 is to first consider a simpler case where \\(\\mu_X = 0\\) and \\(\\mu_Y = 0\\).\n\\[\n\\begin{aligned}\n\\text{Var}(X + Y) & = \\mathbb{E}((X+Y)^2) \\\\\n& = \\mathbb{E} X^2 + 2\\mathbb{E} X\\mathbb{E} Y+ \\mathbb{E} Y^2 \\\\\n& = \\text{Var}(X) + \\text{Var}(Y) + 2 \\text{Cov}(X,Y)\n\\end{aligned}\n\\]\nAnd if the means are not zero, we can apply that \\(\\text{Var}(X + b) = \\text{Var}(X)\\) and \\(\\text{Var}(Y + d) = \\text{Var}(d)\\), and by property 4, \\(\\text{Cov}(X + b, Y + d) = \\text{Cov}(X,Y)\\).\n\n\n\nAs a side-note, if one wants to speak the formula\n\\[\\text{Var}(X) = \\mathbb{E}(X^2) - \\mathbb{E}(X)^2\n\\]\naloud, then the best way to say it seems to be\n“the variance of \\(X\\) is the second moment of \\(X\\) minus the mean of \\(X\\) squared.”\n\nIf we’re doing linear regression with a single covariate, then the formula is \\(Y = \\alpha + \\beta x + \\varepsilon\\). If we assume that \\(x \\perp\\!\\!\\!\\perp\\varepsilon\\), then\n\\[\n\\text{Cor}(X,Y) = \\rho_{X,Y} = \\beta \\frac{\\sigma_X}{\\sigma_Y}\n\\]"
  },
  {
    "objectID": "week8/week8.html#transforming-two-variables-into-one-variable",
    "href": "week8/week8.html#transforming-two-variables-into-one-variable",
    "title": "Week 8",
    "section": "Transforming two variables into one variable",
    "text": "Transforming two variables into one variable\nOften we want to know the distribution of a single random variable \\(U = g_1(X,Y)\\) that is a function of \\((X,Y)\\). However, this is hardly ever an invertible transformation. Fortunately, it turns out that we can still use the bivariate transformation technique as follows.\nIntroduce a new “auxiliary” variable \\(V = g_2(X,Y)\\), chosen to make calculations as easy as possible.\nCompute \\(f_{U,V}(u,v)\\) from \\(f_{X,Y}(x,y)\\) using the bivariate transformation formula.\nThen, integrate to get the marginal density of \\(U\\):\n\\[f_U(u) = \\int f_{U,V} (u,v) \\mathrm d v.\n\\]\n\nExample: Transformation of Uniforms\nLet \\(X,Y \\sim \\text{Uniform}(-1,1)\\) independently and suppose \\(U = (X+Y)/2\\) and \\(V = X-Y\\).\nWhat is the joint pdf of \\((U,V)\\)? Also draw a picture of the joint pdf.\n\nSince we’ll need this down the line, why don’t we go ahead and evaluate the elements of the Jacobian matrix:\nWe first need to invert these functions:\n\\(2X = 2U + V \\Longrightarrow X = U + V/2,\\)\nand \\(2Y = 2U - V \\Longrightarrow Y = U - V/2\\).\n\\[\nDh = \\begin{bmatrix}\n\\frac{\\partial x}{\\partial u} & \\frac{\\partial x}{\\partial v} \\\\\n\\frac{\\partial y}{\\partial u} & \\frac{\\partial y}{\\partial v} \\\\\n\\end{bmatrix} =\n\\begin{bmatrix}\n1 & 1/2 \\\\\n1 & -1/2 \\\\\n\\end{bmatrix}.\n\\]\nSo therefore we have that \\(| \\det Dh | = |-1/2 - 1/2| = |-1| = 1.\\)\nSo the pdf will be\n\\[f_{U,V}(u,v) = f_{X,Y}(x(u,v), y(u,v)) | \\det Dh |\n\\]\nTherefore\n\\[f_{U,V}(u,v) = \\frac{1}{4} \\mathbb 1 (x(u,v) \\in (-1,1)) \\mathbb 1(y(u,v) \\in (-1,1))\n\\]\n\\[f_{U,V}(u,v) = \\frac{1}{4} \\mathbb 1 (-1 < u + v/2 < 1) \\mathbb 1(-1 < u - v/2 < 1)\n\\]\n\\[f_{U,V}(u,v) = \\frac{1}{4} \\mathbb 1 (-2 < 2u + v < 2) \\mathbb 1(-2 < 2u - v < 2)\n\\]\nOne way to think through what these boundary conditions are is to treat these inequalities as four linear constraints.\nReplacing \\(u\\) with \\(x\\) and \\(v\\) with \\(y\\), we would get linear constraints like \\(x + y/2 = 1 \\Longrightarrow y = 2 - 2x\\).\n\n\nu <- seq(-1,1,length.out=100)\nv <- seq(-2,2, length.out=100)\n\nuv_mat <- expand.grid(u,v)\ncolnames(uv_mat) <- c('u', 'v')\n\nuv_mat$density <- with(uv_mat, 1/4 * (u + v/2 < 1 & u + v/2 > -1 & u-v/2 < 1 & u-v/2 > -1))\n\nsuppressMessages(library(tidyverse))\n\nuv_mat |> \n  ggplot(aes(x = u, y = v, fill = as.factor(density),\n  alpha = as.factor(density))) + \n  geom_tile() + \n  scale_fill_manual(values = c('0' = 'white', '0.25' = 'cornflowerblue')) + \n  scale_alpha_manual(values = c('0' = 0, '0.25' = 1)) + \n  labs(fill = \"Density\") + \n  theme_bw() + \n  guides(alpha = guide_none()) + \n  xlim(c(-2,2)) + \n  ggtitle(expression(paste(\"Probability density function of f\"[\"U,V\"], \"(u,v)\")))\n\n\n\n\n\n\nExample: Ratio of Standard Normals\nTo illustrate this technique, suppose \\(X,Y \\sim \\mathcal N(0,1)\\) independently. What is the distribution of \\(X/Y\\)?\nDefine \\(U = X / Y\\) and \\(V = Y\\). That is\n\\[u = g_1(x,y) = x/y\n\\]\n\\[v = g_2(x,y) = y.\n\\]\nIntroducing \\(V = Y\\) makes \\(g\\) invertible, so we can use the bivariate transformation formula.\nThe inverse is \\(x = h_1(u,v) = uv\\) and \\(y = h_2(u,v) = v.\\)\nThus the Jacobian matrix is\n\\[Dh = \\begin{bmatrix}\n\\frac{\\partial h_1}{\\partial u} & \\frac{ \\partial h_1}{\\partial v} \\\\\n  \\frac{\\partial h_2}{\\partial u} & \\frac{\\partial h_2}{\\partial v}\n\\end{bmatrix} = \\begin{bmatrix}\nv & u \\\\ 0 & 1\n\\end{bmatrix}.\n\\]\nThe punchline (which we’ll see soon) is Cauchy.\nTherefore the joint pdf of \\((U,V)\\) is\n\\[\n\\begin{aligned}\nf_{U,V}(u,v) & = f_{X,Y}(h_1(u,v), h_2(u,v)) \\lvert \\det Dh \\rvert \\\\\n& = \\frac{1}{\\sqrt{2\\pi}} \\exp \\left( - \\frac{1}{2} (uv)^2 \\right) \\frac{1}{\\sqrt{2\\pi}} \\exp \\left( -\\frac{1}{2} v^2 \\right) \\lvert v \\rvert \\\\\n\\frac{1}{2\\pi} |v| \\exp \\left( - \\frac{1}{2} v^2 (1 + u^2 )\\right).\n\\end{aligned}\n\\]\nTo find the marginal of \\(U\\), we need to integrate out \\(V\\). Making the change of variable \\(t = v^2, \\, \\, \\mathrm dt = 2v\\mathrm dv\\),\n\\[\n\\begin{aligned}\n\\int_0^\\infty v \\exp \\left( -\\frac{1}{2} v^2(1+u^2) \\right) \\mathrm dv & = \\frac{1}{2} \\int_0^\\infty \\exp \\left( -\\frac{1}{2} t (1+u^2) \\right) \\mathrm d t \\\\\n& = \\frac{1}{1 + u^2}.\n\\end{aligned}\n\\]\nTherefore since \\(f_{U,V}(u,v) = f_{U,V}(u,-v),\\)\n\\[f_U(u) = \\int_{-\\infty}^\\infty f_{U,V}(u,v) \\mathrm dv = 2 \\int_0^\\infty f_{U,V}(u,v) \\mathrm dv = \\frac{1}{\\pi} \\frac{1}{1+u^2}.\n\\]\nWe can recognize this as a Cauchy distribution."
  },
  {
    "objectID": "week8/week8.html#mean-and-covariance-of-a-random-vector",
    "href": "week8/week8.html#mean-and-covariance-of-a-random-vector",
    "title": "Week 8",
    "section": "Mean and Covariance of a Random Vector",
    "text": "Mean and Covariance of a Random Vector\nThe covariance matrix of a random vector \\((X,Y)^T = \\begin{bmatrix} X \\\\ Y \\end{bmatrix}\\) is\n\\[\n\\begin{aligned}\n\\text{Cov}\\left( \\begin{bmatrix} X \\\\ Y \\end{bmatrix} \\right) & =\n\\begin{bmatrix}\n\\text{Cov}(X,X) & \\text{Cov}(X,Y) \\\\\n\\text{Cov}(Y,X) & \\text{Cov}(Y,Y)\n\\end{bmatrix} \\\\\n& =\n\\begin{bmatrix}\n\\sigma_X^2 & \\rho_{X,Y} \\sigma_X \\sigma_Y \\\\\n\\rho_{X,Y} \\sigma_X \\sigma_Y & \\sigma_Y^2\n\\end{bmatrix}.\n\\end{aligned}\n\\]\nThe mean of a random vector \\((X,Y)^T\\) is defined to be the vector of the means of its entries:\n\\[\\mathbb{E}\\left( \\begin{bmatrix} X \\\\ Y \\end{bmatrix} \\right) = \\begin{bmatrix} \\mathbb{E}X \\\\ \\mathbb{E}Y \\end{bmatrix} .\n\\]\nFor any random vector \\(X = \\begin{bmatrix} X_1 \\\\ X_2 \\end{bmatrix}\\) and any \\(2 \\times 2\\) matrix \\(A\\),\n\\[\\text{Cov}(AX) = A \\text{Cov}(X) A^T.\n\\]\n(Actually this works for any dimensional \\(A\\) as long as the number of rows of \\(A\\) is the same as \\(\\text{length}(X)\\).)\nIt is common to parameterize bivariate (and more generally, multivariate) normal distributions in terms of the mean vector and covariance matrix. We write\n\\[\n\\begin{bmatrix}\nX \\\\ Y\n\\end{bmatrix} \\sim \\mathcal N(\\mu, \\Sigma)\n\\]\nto denote that \\((X,Y)^T\\) is bivariate normal such that\n\\[\\mu = \\mathbb{E}\\left( \\begin{bmatrix} X \\\\ Y \\end{bmatrix} \\right) \\quad \\quad \\text{ and } \\quad \\quad \\Sigma = \\text{Cov}\\left( \\begin{bmatrix} X \\\\ Y \\end{bmatrix} \\right).\n\\]\nHowever, not just any \\(2 \\times 2\\) matrix \\(\\Sigma\\) can be used. \\(\\Sigma\\) must be a symmetric positive semi-definite matrix, that is\n\n\\(\\Sigma = \\Sigma^T\\) (symmetric) and\n\\(t^T \\Sigma t \\geq 0\\) for all \\(t \\in \\mathbb R^2\\) (positive semi-definiteness)o\n\nWe can write \\(t^T \\Sigma t = t_1^2 \\Sigma_{11} + t_1 t_2 \\Sigma_{12} + t_2 t_1 \\Sigma_{21} + t_2^2 \\Sigma{22}.\\)\nIf \\(\\rho = 1\\), then \\[\n\\begin{aligned}\nt^T \\Sigma t & = t_1^2 \\sigma_1^2 + 2t_1t_2 \\sigma_1 \\sigma_2 + t_2^2 \\sigma_2^2 \\\\\n& = (t_1 \\sigma_1 + t_2 \\sigma_2)^2\n\\end{aligned}.\n\\]\nThis leads to a useful way of constructing bivariate normal distributions. Let \\(s_1 \\geq s_2 \\geq 0\\) and \\(\\theta \\in [0, 2\\pi )\\). Let \\(Z_1, Z_2 \\sim \\mathcal N(0,1)\\) independently and define\n\\[\n\\begin{bmatrix}\nX_1 \\\\ X_2\n\\end{bmatrix} =\n\\underbrace{\\begin{bmatrix}\n\\cos \\theta & - \\sin \\theta \\\\\n\\sin \\theta & \\cos \\theta\n\\end{bmatrix}}_{\\text{rotation}}\n\\underbrace{\\begin{bmatrix}\ns_1 & 0 \\\\\n0 & s_2\n\\end{bmatrix}}_{\\text{scaling}}\n\\begin{bmatrix}\nZ_1 \\\\ Z_2\n\\end{bmatrix}.\n\\]\nThen \\((X_1,X_2)^T\\) is bivariate normal such that the line along which \\(X_1\\) and \\(X_2\\) are correlated at angle \\(\\theta\\), the scale along this line is \\(s_1\\) and the scale orthogonal to the line is \\(s_2\\).\nConversely, given \\(\\Sigma\\) we can recover the scaling and rotation.\nCompute the eigendecomposition \\(\\Sigma = U \\Lambda U^T\\) where \\(U\\) is an orthogonal matrix and \\(\\Lambda = \\text{diag}(\\lambda_1, \\lambda_2)\\), with \\(\\lambda_1 \\geq \\lambda_2 \\geq 0\\).\nRecall that a matrix is orthogonal if \\(U^TU = I\\) and \\(UU^T = I\\).\nThen \\(\\lambda_1 = s_1^2\\), \\(\\lambda_2 = s_2^2\\), and \\(U\\) is the rotation matrix.\nThen we can represent \\(\\begin{bmatrix} X_1 \\\\ X_2 \\end{bmatrix} \\sim \\mathcal N(0,\\Sigma)\\) as\n\\[\n\\begin{bmatrix}\nX_1 \\\\ X_2\n\\end{bmatrix} =\n\\underbrace{\\begin{bmatrix}\nu_{11} & u_{12} \\\\\nu_{21} & u_{22} \\\\\n\\end{bmatrix}}_{U \\,\\text{ (rotation)}}\n\\underbrace{\\begin{bmatrix}\n\\sqrt{\\lambda_1} & 0 \\\\\n0 & \\sqrt{\\lambda_2}\n\\end{bmatrix}}_{\\Lambda^{1/2} \\,\\text{ (scaling)}}\n\\begin{bmatrix}\nZ_1 \\\\ Z_2\n\\end{bmatrix},\n\\]\nwhere \\(Z_1, Z_2 \\sim \\mathcal N(0,1)\\) independently.\nOr, more succinctly, \\(X = U \\Lambda^{1/2} Z\\).\n\nInstead of a location-scale family, this is like a rotation-scale+location family!\n\n\nSo the above show two methods:\n\nStarting from angles and scales to produce a bivariate matrix;\nOr starting from a covariance matrix using eigendecomposition to determine the rotation and scales necessary to yield that covariance matrix.\n\n\nIf this looks scary, brush up on your linear algebra:\nDiagonalization of symmetric matrices; eigendecomposition."
  },
  {
    "objectID": "week9/week9.html",
    "href": "week9/week9.html",
    "title": "Week 9",
    "section": "",
    "text": "Recap\nWe left off talking about bivariate normal distributions. We characterized them using their covariances, and showed that we could use matrix decomposition on the covariance matrix to give it an interpretation in terms of rotation and scaling matrices.\nSome handy definitions:\nThe eigendecomposition of \\(\\Sigma\\) is \\(\\Sigma = U \\Lambda U^T\\).\nA matrix is orthogonal if \\(U^TU = I\\) and \\(UU^T = I\\), a generalization of the unit-length vector.\nPrincipal components analysis can be done by applying this decomposition to the sample covariance \\(\\hat \\Sigma\\) estimated from data \\(x_1, ..., x_n \\in \\mathbb R^2\\).\nThe columns of \\(U\\) are PC directions, \\(s_1\\), \\(s_2\\) are the PC scales, \\(U^T x_i\\) are the PC scores.\nThe mgf of a bivariate random vector \\(X = (X_1, X_2)^T\\) is defined to be\n\\[M_X(t) = \\mathbb{E}\\exp (t_1 X_1 + t_2 X_2) = \\mathbb{E}\\exp (t^T X),\\]\nfor \\(t = \\begin{bmatrix} t_1 \\\\ t_1 \\end{bmatrix} \\in \\mathbb R^2\\).\nIf \\(X\\) is bivariate normal then\n\\[M_X(t) = \\exp \\left(\nt_1 \\mu_{x_1} + t_2 \\mu_{x_2} + \\frac{1}{2} \\left(t_1^2 \\sigma^2_{x_1} + 2t_1 t_2 \\rho \\sigma_{x_1} \\sigma_{x_2} + t_2^2 \\sigma_{x_2}^2 \\right)\n\\right).\n\\]\nIn matrix/vector notation, if \\(\\begin{bmatrix} X_1 \\\\ X_2 \\end{bmatrix} \\sim \\mathcal N(\\mu, \\Sigma)\\), then\n\\[M_X(t) = \\exp \\left( t^T \\mu + \\frac{1}{2} t^T \\Sigma t \\right)\n\\]\nEarlier we saw Boole’s Inequality and Bonferroni’s inequality. Inequalities are quite useful in probability because it’s often easier to bound some quantity of interest than to characterize it exactly. Often a decent bound is all that is needed.\nFor example, suppose you are manufacturing widgets. Each widget can be defective in three different ways, \\(A_1, A_2, A_3\\). You have data on the probability of each type of defect \\(P(A_k)\\), but you don’t have any data on the joint probability of these events.\nFortunately the probability of any defect can be bounded using Boole’s inequality:\n\\[P(A_1 \\cup A_2 \\cup A_3) \\leq P(A_1) + P(A_2) + P(A_3)\n\\]\nIf each \\(P(A_k)\\) is small, then we can guarantee that the probability of any defect occuring is small.\nif the number of possible defects is large, then their pairwise possible combinations are quite large, so we can save on data collection quite a bit.\nOne of the simplest but most useful inequalities in probability theory:\nMarkov’s inequality: if \\(X\\) is nonnegative and \\(a > 0\\), then\n\\[P(X \\geq a) \\leq \\frac{ \\mathbb{E}X }{a}.\n\\]\nProof:\nSince \\(1 \\geq \\mathbb 1(X \\geq a),\\)$\n\\[\\begin{aligned}\n\\mathbb{E}X & \\geq \\mathbb{E}X \\mathbb 1 (X \\geq a) \\\\\n& \\geq \\mathbb{E}a \\mathbb 1(X \\geq a) \\\\\n& = a P(X \\geq a).\n\\end{aligned}\n\\]\nDividing both sides by \\(a\\) yields the result.\nThis is a surprisingly powerful corollary of Markov’s inequality. It yields an exponentially decaying bound as \\(a\\) grows, compared to the \\(1/a\\) in Markov’s inequality.\nChernoff’s bound\n\\[P(X \\geq a) \\leq \\inf_{t > 0} e^{-ta} \\mathbb{E}\\exp (tX)\n\\]\nProof: For all \\(t > 0\\),\n\\[\n\\begin{aligned}\nP(X \\geq a) & = P(tX \\geq ta) \\\\\n& = P(\\exp (tX) \\geq \\exp (ta)) \\\\\n& \\leq \\frac{\\mathbb{E}\\exp (tX)}{\\exp (ta)} \\\\\n= e^{-ta} \\mathbb{E}\\exp (tX).\n\\end{aligned}\n\\]\nSince the left-hand side doesn’t depend on \\(t\\), the inequality holds when taking the infimum of the right-hand side over \\(t\\)."
  },
  {
    "objectID": "week9/week9.html#investing-returns-example",
    "href": "week9/week9.html#investing-returns-example",
    "title": "Week 9",
    "section": "Investing Returns Example",
    "text": "Investing Returns Example\nYou invest $1000 dollars in a holding where the annual returns are \\(\\mathrm{Pareto}(\\alpha, c)\\) distributed with \\(\\alpha = 2\\) and \\(c = 1/4\\).\nMore precisely, after \\(n\\) years, your investment is worth\n\\[Y_n = 1000 X_1 X_2 \\cdots X_n\\]\ndollars, where \\(X_1, ..., X_n \\sim \\mathrm{Pareto}(\\alpha, c)\\) independently.\nRecall that the pdf of \\(\\mathrm{Pareto}(\\alpha, c)\\) is\n\\[p(x) = \\frac{\\alpha c^{\\alpha}}{x^{\\alpha+1}} \\mathbb 1(x > c).\\]\n\nIs this a good investment?\nFirst, guess using your intuition. Then try to show something formally.\n\nf <- function(x) {2 * .25^(2) / x^(2 + 1) * (x > .25)}\ncurve(f, from = 0, to = 2)\n\n\n\n\nSince so much of the pdf appears to be concentrated in (0.25,1), we were wondering, given that the Pareto distrbution has heavy tails, what’s the probability of getting an outcome \\(\\geq 1\\)?\n\ncdf_f <- function(x) { 1 - (.25/x)^2 }\n1 - cdf_f(1)\n\n[1] 0.0625\n\n\nIt looks like that might happen 6.25% of the time, but we don’t really have a sense of how large those values might be.\n\n\n\nApplying Markov’s theorem:\nWe’re interested in \\(P(Y_n \\geq a) \\leq \\frac{\\mathbb{E}Y_n}{a}\\).\n\\[\n\\begin{aligned}\nP(Y_n \\geq a) & \\leq \\frac{\\mathbb{E}Y_n}{a} \\\\\n& = \\frac{\\mathbb{E}10000 X_1 \\cdots X_n}{a} \\\\\n& = \\frac{1000 (\\mathbb{E}X_1) \\cdots (\\mathbb{E}X_n)}{a} \\\\\n& = \\frac{1000}{a} \\left( \\frac{\\alpha c}{\\alpha - 1} \\right)^n \\\\\n& = \\frac{1000}{a} \\left( \\frac{1}{2} \\right)^n \\stackrel{n \\to \\infty }{\\Longrightarrow} 0 \\\\\n\\Longrightarrow & P(Y_n \\geq a) \\longrightarrow 0 \\quad \\text{ for all } a > 0.\n\\end{aligned}\n\\]\n\n\n\nIf one sees a probability of some large event that they want to bound, usually the first thing one should try is applying Markov’s theorem."
  },
  {
    "objectID": "week9/week9.html#corollaries-of-markovs-inequality",
    "href": "week9/week9.html#corollaries-of-markovs-inequality",
    "title": "Week 9",
    "section": "Corollaries of Markov’s Inequality",
    "text": "Corollaries of Markov’s Inequality\n\nFor any random variable \\(X\\) and any \\(a > 0\\),\n\n\\[P(|X| \\geq a) \\leq \\frac{\\mathbb{E}|X|}{a}.\\]\n\nFor any random variable \\(X\\), any \\(a \\in \\mathbb R\\), and any monotone increasing function \\(g(x) \\geq 0\\),\n\n\\[P(X \\geq a) \\leq \\frac{\\mathbb{E}g(X)}{g(a)}.\\]\n\nChebyshev’s inequality: For any random variable \\(X\\) and any \\(a > 0\\),\n\n\\[P(|X - \\mathbb{E}X| \\geq a) \\leq \\frac{\\text{Var}(X)}{a^2}.\\]\nChebyshev’s inequality allows us to bound the probability that a random variable is a certain distance from its mean.\n\nTry to show 2 and 3 using Markov’s inequality.\nBecause \\(x \\geq a \\to g(x) \\geq g(a)\\), \\[P(X \\geq a) \\leq P(g(X) \\geq g(a)) \\quad \\text{ since $g$ is monotonically increasing}.\\]\n(If the statement said strictly monotone, the first inequality would be an equality.)\nNote that \\(g(a)\\) needs to be positive.\nAnd then apply Markov’s inequality to get that\n\\[P(X \\geq a) = P(g(X) \\geq g(a)) = \\frac{\\mathbb{E}g(X)}{g(a)}.\\]\nThen for 3, we want to set \\(g(x) = x^2\\), which is monotonically increasing on \\(X \\geq 0\\).\n\\[\\begin{aligned}\nP(|X-\\mathbb{E}X| \\geq a) & = P(| X - \\mathbb{E}X |^2 \\geq a^2) \\\\\n& \\leq \\frac{ \\mathbb{E}|X-\\mathbb{E}X|^2 }{a^2} \\\\\n& = \\frac{\\text{Var}(X)}{a^2},\n\\end{aligned}\n\\]\nnoting that \\(|X - \\mathbb{E}X|\\) and \\(a^2\\) being non-negative is what makes this work."
  },
  {
    "objectID": "week9/week9.html#example-tail-bounds-for-normal-distributions",
    "href": "week9/week9.html#example-tail-bounds-for-normal-distributions",
    "title": "Week 9",
    "section": "Example: Tail Bounds for Normal Distributions",
    "text": "Example: Tail Bounds for Normal Distributions\nSuppose \\(X \\sim \\mathcal N(\\mu, \\sigma^2)\\) and we want to bound the probability that \\(X\\) is far from its mean. The exact expression involves the standard normal CDF \\(\\Psi(x)\\):\n\\[P(|X - \\mu| \\geq a) = P\\left(\n\\lvert\\frac{X - \\mu}{\\sigma} \\rvert \\geq\na / \\sigma\n\\right) = 2 \\Psi(-a/\\sigma)\n\\] for \\(a > 0\\). However, \\(\\Psi(x)\\) does not have a simple closed form.\nMeanwhile, Chebyshev’s inequality easily yields \\[P(|X-\\mu| \\geq a) \\leq \\frac{\\text{Var}(X)}{a^2}\n= \\frac{\\sigma^2}{a^2}\n\\]"
  },
  {
    "objectID": "week9/week9.html#example-tail-bounds-for-normal-distributions-1",
    "href": "week9/week9.html#example-tail-bounds-for-normal-distributions-1",
    "title": "Week 9",
    "section": "Example: Tail Bounds for Normal Distributions",
    "text": "Example: Tail Bounds for Normal Distributions\nSuppose \\(X \\sim \\mathcal N(\\mu, \\sigma^2)\\). By Chernoff’s bound,\n\\[\\begin{aligned}\nP(X - \\mu \\geq a) & \\leq \\inf_{t > 0} e^{-ta} \\mathbb{E}\\exp \\left( t(X - \\mu) \\right) \\\\\n& = \\inf_{t > 0} e^{-ta} \\exp \\left( \\frac{1}{2} \\sigma^2 t^t \\right) \\\\\n& = \\inf_{t > 0} \\exp \\left( -ta + \\frac{1}{2} \\sigma^2 t^2)\n\\end{aligned}\\]\nusing the formula for the mgf of \\(X - \\mu \\sim \\mathcal N(0, \\sigma^2)\\).\nTo minimize \\(f(t) = -ta + \\frac{1}{2} \\sigma^2 t^2\\) we set\n\\[ 0 = f'(t) = -a + \\sigma^2 t\n\\]\nand solve to get \\(t = a/\\sigma^2\\). Plugging this in yields \\[P(X - \\mu \\geq a) \\leq \\exp (- a^2/\\sigma^2 + \\frac{1}{2} a^2 / \\sigma^2) = \\exp(-\\frac{1}{2} a^2 / \\sigma^2).\\]\nBy symmetry \\(P(-(X-\\mu) \\geq a) \\leq \\exp( - \\frac{1}{2} a^2 / \\sigma^2)\\). Thus,\n\\[P(|X - \\mu | \\geq a) \\leq 2 \\exp ( -\\frac{1}{2} a^2 / \\sigma^2).\\]\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\ndf <- tibble(\n  a = seq(1, 6, length.out = 1000),\n  exact_value = pnorm(q = -a) + (1-pnorm(q = a)),\n  chebyshev = 1/a^2,\n  chernoff = 2*exp(-.5*a^2)\n)\n\nplt1 <- df |> \ntidyr::pivot_longer(\n  cols = c(exact_value, chebyshev, chernoff),\n  names_to = 'bound',\n  values_to = 'pr'\n) |> \nggplot(aes(x = a, y = pr, color = bound)) + \ngeom_line() + \ntheme_bw() + \ntheme(legend.position = c(.85,.75)) + \nggtitle(expression(paste(\"Bound on P(|X - \", mu, \") ≥ a)\")))\n\nplt2 <- plt1 + \nscale_y_log10(labels = scales::number_format()) + \ntheme(legend.position = c(.15, .25))\n\nplt1 \n\n\n\nplt2 \n\n\n\n\nA tighter bound can be achieved using\n\\[\nP(|X - \\mu| \\geq a) \\leq\n\\sqrt{\\frac{2 \\sigma^2}{\\pi a^2}} \\exp (\n  -\\frac{1}{2} a^2 / \\sigma^2\n).\n\\]"
  },
  {
    "objectID": "week9/week9.html#convex-functions",
    "href": "week9/week9.html#convex-functions",
    "title": "Week 9",
    "section": "Convex Functions",
    "text": "Convex Functions\nA function \\(g : \\mathcal X \\to \\mathbb R\\) is convex if\n\\[g(tx + (1-t)y) \\leq tg(x) + (1-t)g(y)\n\\]\nA function \\(g: \\mathcal X \\to \\mathbb R\\) is concave if \\(-g\\) is convex.\nIntuition: Conex functions curve upwards, concave functions curve downwards.\n\ntibble(\n  x = seq(-2,2,length.out=1000),\n  x2 = x^2,\n  exp = exp(x),\n  neg_log = -log(x),\n  x_inv = 1/x\n) |> tidyr::pivot_longer(\n  cols = 2:5,\n  names_to = 'f',\n  values_to = 'g(x)'\n) |> \n# mutate(\n#   f = case_when(\n#     f == 'x2' ~ paste(expression(x^2)),\n#     f == 'exp' ~ paste(expression(e^x)),\n#     f == 'neg_log' ~ paste(expression(log(x))),\n#     f == 'x_inv' ~ paste(expression(1/x))\n#   )\n# ) |> \nggplot(aes(x = x, y = `g(x)`, color = f)) + \ngeom_line()\n\nWarning in log(x): NaNs produced\n\n\nWarning: Removed 500 rows containing missing values (`geom_line()`).\n\n\n\n\n\nSuppose \\(g : \\mathcal X \\to \\mathbb R\\) is twice-differentiable at all \\(x \\in \\mathcal X\\). Then \\(g\\) is convex if and only if\n\\[\\frac{\\partial^2}{\\partial x^2 } g(x) \\geq 0\\]\nfor all \\(x \\in \\mathcal X\\).\nSuppose \\(g : \\mathcal X \\to \\mathbb R\\) is a convex function. For any \\(x_0 \\in \\mathcal X\\), there exist \\(a,b \\in \\mathcal R\\) such that \\[ax + b \\leq g(x)\\] for all \\(x \\in \\mathcal X\\) and\n\\[a x_0 + b = g(x_0)\\]\nJensen’s Inequality: If \\(X\\) is a random variable with range \\(\\mathcal X\\) and \\(g: \\mathcal X \\to \\mathbb R\\) is a convex function, then\n\\[g(\\mathbb{E}X) \\leq \\mathbb{E}g(X).\\]\nProof: Define \\(x_0 = \\mathbb{E}X\\). Since \\(g\\) is convex, there exist \\(a, b \\in \\mathbb R\\) such that \\(ax + b \\leq g(x)\\) for all \\(x \\in \\mathcal X\\) and \\(ax_0 + b = g(x_0).\\) Therefore,\n\\[g(\\mathbb{E}X) = g(x_0) = ax_0 + b = \\mathbb{E}(aX + b) \\leq \\mathbb{E}g(X).\\]"
  },
  {
    "objectID": "week9/week9.html#weighted-am-gm-inequality",
    "href": "week9/week9.html#weighted-am-gm-inequality",
    "title": "Week 9",
    "section": "Weighted AM-GM Inequality",
    "text": "Weighted AM-GM Inequality\nThe inequality of arithmetic means and geometric means is a classic result that can be easily proved using Jensen’s inequality.\nWeighted AM-GM Inequality: For any \\(x_1, ... x_n \\geq 0\\) and \\(w_1, ..., w_n \\geq 0\\) such that \\(\\sum_{i=1}^n w_i = 1\\),\n\\[w_1x_1 + ... + w_n x_n \\geq x_1^{1_2} \\cdots x_{n}^{w_n}.\\]\n\nShow this using Jensen’s inequality"
  },
  {
    "objectID": "week9/week9.html#hoeffdings-inequality",
    "href": "week9/week9.html#hoeffdings-inequality",
    "title": "Week 9",
    "section": "Hoeffding’s Inequality",
    "text": "Hoeffding’s Inequality\nSuppose \\(X_1, ..., X_n\\) are independent random variables such that \\(r_i \\leq X_i \\leq s_i\\) and denote \\(\\bar X = \\frac{1}{n} \\sum_{i=1}^n X_i\\). Then for all \\(a > 0\\)\n\\[P(| \\bar X - \\mathbb{E}\\bar X > a) \\leq 2 \\exp \\left( - \\frac{2a^n}{\\frac{1}{n} \\sum_{i=1}^n (s_i - r_i)^2} \\right).\\]\nHoeffding’s shows that for bounded independent random variables, the sample mean converges exponentially quickly to the mea.\nLike Chernoff’s bound, Hoeffding’s provides an exponentially decaying bound as \\(a\\) grows. An advantage of Hoeffding’s is that the mgf doesn’t need to be known to get an explicit bound. On the other hand, the random variables need to be bounded.\nFor instance, if \\(X_1, ..., X_n \\sim \\text{Bernoulli}(q),\\) then\n\\[P(| \\bar X - q | > a) \\leq 2 \\exp (-2a^n).\\]"
  },
  {
    "objectID": "week11/week11.html",
    "href": "week11/week11.html",
    "title": "Week 11",
    "section": "",
    "text": "Recap — and Continuing Multivariate Normality\nWe introduced several properties of the multivariate normal distributions. We introduced eigendecomposition of the covariance matrix; the precision matrix; independence and covariance; transformation to a \\(\\chi^2(k)\\) distribution, etc.\nOutline:\nIn practice, we usually have multiple data points, \\(x_1, ..., x_n\\).\nHere, each \\(x_i\\) may be univariate or multivariate.\nIn many applications, it is natural to think of \\(x_1, ..., x_n\\) as samples from a common distribution.\nThe most common setting is to model the data as independent and identically distributed (i.i.d.) random variables or random vectors \\(X_1, ..., X_n\\) meaning that:\nWe usually write “\\(X_1, ..., X_n\\) are i.i.d.” to denote this.\nCasella & Berger refer to this as a “random sample of size \\(n\\),” but Jeff does not like this phrase as he finds it vague and thinks it should be avoided.\nSometimes people refer to each \\(X_i\\) as a sample, and sometimes they refer to the whole collection \\(X_1, ..., X_n\\) as a sample. Which one is meant should be clear from context.\nSuppose you conduct an experiment to measure the speed of sound, and the measured outcome is \\(X\\). Since there is randomness in \\(X\\), you decide to repeat the experiment \\(n\\) times. This yields \\(n\\) outcomes, \\(X_1, ..., X_n\\). Since the experimental conditions are approximately the same in each iteration, we may assume the \\(X_i\\) are i.i.d.\nSuppose the pdf/pmf of \\(X_1\\) is \\(q(x)\\).\nWe often denote this by writing “$X_1, …, X_n q $ i.i.d.”\nThe joint pdf/pmf is:\n\\[f(x_1, ..., x_n) = q(x_1) \\cdots q(x_n) = \\prod_{i=1}^n q(x_i).\\]\nIf \\(q(x) = p(x \\mid \\theta)\\) for some parametric family, then\n\\[f(x_1, ..., x_n) = p(x_1 \\mid \\theta) \\cdots p(x_n \\mid \\theta) = \\prod_{i=1}^n p(x_i \\mid \\theta).\\]\nViewing this as a function of \\(\\theta\\), it is called the likelihood function, and is usually denoted \\(L(\\theta)\\):\n\\[L(\\theta) = \\prod_{i=1}^n p(x_i \\mid \\theta).\\]\nSuppose \\(X_1, ..., X_n \\stackrel{i.i.d.}{\\sim} \\text{Exponential}(\\lambda)\\).\nThen the joint pdf is\n\\[L(\\lambda) = p(x_1, ..., x_n \\mid \\lambda) = \\prod_{i=1}^n p(x_i \\mid \\lambda)\\]\n\\[ = \\prod_{i=1}^n \\lambda \\exp(-\\lambda x_i)\\]\n\\[= \\lambda^n \\exp \\left( -\\lambda \\sum_{i=1}^n x_i \\right).\\]"
  },
  {
    "objectID": "week11/week11.html#proportionality",
    "href": "week11/week11.html#proportionality",
    "title": "Week 11",
    "section": "Proportionality",
    "text": "Proportionality\n\\(f(x)\\) is proportional to \\(g(x)\\) if there exists \\(c \\neq 0\\) such that \\(f(x) = cg(x)\\) for all \\(x\\). (This can be generalized to allow \\(c=0\\) but the definition is more complicated and we won’t need it here.)\nWe write \\(f \\propto g\\) to denote that \\(f\\) is proportional to \\(g\\).\nProperties:\n\nIf \\(f \\propto g\\) then \\(g \\propto f\\) (Symmetry).\nIf \\(f \\propto g\\) and \\(g \\propto h\\), then \\(f \\propto h\\) (Transitivity).\nIf \\(f\\) and \\(g\\) are pdfs or pmfs and \\(f \\propto g\\), then \\(f = g\\).\n\n\nSymmetry because one can take \\(c_2 = 1/c_1\\).\nTransitivity because one can multiply \\(c_1 c_2\\).\nFor the third, the two properties to apply is that the integral/sum over all values of a pdf/pmf is 1 and proportionality applies at all individual values.\nSo \\(\\sum f = \\sum g = 1\\), and \\(f(x) = cg(x) \\quad \\forall x\\).\nObserve that \\(\\sum f(x) = \\sum c g(x) = c \\sum g(x) = c \\times 1 = 1\\).\n\\(\\therefore c = 1\\)"
  },
  {
    "objectID": "week11/week11.html#proportionality-of-the-multivariate-normal",
    "href": "week11/week11.html#proportionality-of-the-multivariate-normal",
    "title": "Week 11",
    "section": "Proportionality of the Multivariate Normal",
    "text": "Proportionality of the Multivariate Normal\nThe pdf of \\(X \\sim \\mathcal N(\\mu, \\Sigma),\\) when \\(\\Lambda = \\Sigma^{-1}\\) exists is\n\\[\\begin{aligned}\\mathcal N(x \\mid \\mu, \\Lambda^{-1}) & = \\frac{|\\det \\Lambda|^{1/2}}{(2\\pi)^{k/2}}\n\\exp \\left( -\\frac{1}{2} (x- \\mu)^\\mathtt{T}\\Lambda (x-\\mu)\\right)  \\\\\n& \\propto \\exp \\left( -\\frac{1}{2} (x-\\mu)^\\mathtt{T}\\Lambda (x-\\mu)\\right) \\\\\n& = \\exp \\left( -\\frac{1}{2} ( x^\\mathtt{T}\\Lambda x - x^T \\Lambda \\mu - \\mu^T \\Lambda x + \\overbrace{\\mu^T \\Lambda \\mu}^{\\text{constant}} \\right) \\\\\n& \\propto \\exp \\left( -\\frac{1}{2} (x^\\mathtt{T}\\Lambda x - 2 \\mu^T \\Lambda x \\right).\n\\end{aligned}\\]\nSuppose \\(f(x) \\propto \\exp(-\\frac{1}{2} (x^\\mathtt{T}A x - 2c^T x))\\).\nSet \\(\\Lambda = A\\) and \\(\\mu^\\mathtt{T}\\Lambda = c^T\\) and solve for \\(\\mu\\) and \\(\\Sigma\\).\n\\(\\Sigma = A^{-1}\\) and \\(\\mu = A^{-1} c\\) because \\(c = \\Lambda^\\mathtt{T}\\mu = \\Lambda \\mu\\).\nTherefore \\(f(x) = \\mathcal N(x \\mid A^{-1} c, A^{-1})\\)."
  },
  {
    "objectID": "week11/week11.html#conditional-distributions",
    "href": "week11/week11.html#conditional-distributions",
    "title": "Week 11",
    "section": "Conditional Distributions",
    "text": "Conditional Distributions\nTo find the distribution of \\(X_a \\mid X_b = x_b\\), letting \\(\\Lambda = \\Sigma^{-1}\\),\n\\[\\begin{aligned} p(x_a \\mid x_b) \\propto p(x_a, x_b) & = p(x) = \\mathcal N(x \\mid \\mu, \\Sigma)  \\\\\n& \\propto \\exp \\left( -\\frac{1}{2} (x^\\mathtt{T}\\Lambda x - 2 \\mu^\\mathtt{T}\\Lambda x \\right)\n\\end{aligned}.\\]\nMultiplying out and keeping only terms depending on \\(x_a\\),\n\\[\\begin{aligned}\nx^\\mathtt{T}& \\Lambda x - 2 \\mu^\\mathtt{T}\\Lambda x = \\begin{bmatrix} x_a \\\\ x_b \\end{bmatrix}^\\mathtt{T}\\begin{bmatrix} \\Lambda_{aa} & \\Lambda_{ab} \\\\ \\Lambda_{ba} & \\Lambda_{bb} \\end{bmatrix} \\begin{bmatrix} x_a \\\\ x_b \\end{bmatrix}\n- 2 \\begin{bmatrix} \\mu_a \\\\ \\mu_b \\end{bmatrix}^\\mathtt{T}\\begin{bmatrix} \\Lambda_{aa} & \\Lambda_{ab} \\\\ \\Lambda_{ba} & \\Lambda_{bb} \\end{bmatrix} \\begin{bmatrix} x_a \\\\ x_b \\end{bmatrix} \\\\\n& = \\begin{bmatrix} x_a^\\mathtt{T}\\Lambda_{aa} + x_b^\\mathtt{T}\\Lambda_{ba} & x_a^\\mathtt{T}\\Lambda_{ab} + x_b^\\mathtt{T}\\Lambda_{bb} \\end{bmatrix} \\begin{bmatrix} x_a \\\\ x_b \\end{bmatrix} -\n2 \\begin{bmatrix} \\mu_a^\\mathtt{T}\\Lambda_{aa} + \\mu_b^\\mathtt{T}\\Lambda_{ba} & \\mu_a^T \\Lambda_{ab} + \\mu_b^\\mathtt{T}\\Lambda_{bb} \\end{bmatrix} \\begin{bmatrix} x_a \\\\ x_b \\end{bmatrix} \\\\\n& = (x_a^\\mathtt{T}\\Lambda_{aa} + x_b^\\mathtt{T}\\Lambda_{ba}) x_a + (x_a^T \\Lambda_{ab} +\\cancel{ x_b^\\mathtt{T}\\Lambda_{bb}}) x_b - 2((\\mu_a^T \\Lambda_{aa} + \\mu_b^\\mathtt{T}\\Lambda_{ba})x_a + \\cancel{(\\mu_a^\\mathtt{T}\\Lambda{ab} + \\mu_b^\\mathtt{T}\\Lambda_{bb}) x_b}) \\\\\n& \\text{we can cancel terms that don't depend on $x_a$} \\\\\n& = x_a^\\mathtt{T}\\Lambda_{aa} x_a + x_b^\\mathtt{T}\\Lambda_{ba} x_a + x_a^\\mathtt{T}\\Lambda_{ab} x_b - 2(\\mu_a^\\mathtt{T}\\Lambda_{aa} + \\mu_b^\\mathtt{T}\\Lambda_{ba})x_a + \\text{a constant} \\\\\n& = x_a^\\mathtt{T}\\Lambda_{aa} x_a - 2(\\mu_a^\\mathtt{T}\\Lambda_{aa} + \\mu_b^\\mathtt{T}\\Lambda_{ba} - x_b^T \\Lambda_{ba}) x_a + \\text{ a constant} \\\\\n& = x_a^\\mathtt{T}\\Lambda_{aa} x_a - 2(\\mu_a^\\mathtt{T}\\Lambda_{aa} - (x_b-\\mu_b)^T \\Lambda_{ba})x_a + \\text{ a constant}.\n% & = x_a^\\T \\Lambda_{aa} x_a + 2x_\n\\end{aligned}\\]\nTo express \\(p(x_a \\mid x_b)\\) in terms of \\(\\Sigma\\) instead of \\(\\Lambda\\), we can use the following handy formula for inversion of a \\(2 \\times 2\\) block matrix:\n\\[\\begin{bmatrix} A & B \\\\ C & D \\end{bmatrix}^{-1} = \\begin{bmatrix} M & -MBD^{-1} \\\\ -D^{-1}CM & D^{-1} + D^{-1}CMBD^{-1} \\end{bmatrix},\\]\nwhere\n\\[M = (A - BD^{-1}C)^{-1}.\\]\nApplying this to \\(\\Sigma^{-1}\\) we have in particular that\n\\[\\Lambda_{aa} = (\\Sigma_{aa} - \\Sigma_{ab}\\Sigma_{bb}^{-1} \\Sigma_{ba})^{-1}\\] \\[\\Lambda_{aa}^{-1} \\Lambda_{ab} = -\\Sigma_{ab}\\Sigma_{bb}^{-1}.\\]\nPlugging into the prior result,\n\\[\\begin{aligned} p(x_a \\mid x_b) & = \\mathcal N \\left( x_a \\mid \\mu_a - \\Lambda_{aa}^{-1} \\Lambda_{ab} (x_b - \\mu_b), \\Lambda_{aa}^{-1}\\right) \\\\\n& = \\mathcal N \\left( x_a \\mid \\mu_a + \\Sigma_{ab}\\Sigma_{bb}^{-1} (x_b - \\mu_b), \\Sigma_{aa} - \\Sigma_{ab} \\Sigma_{bb}^{-1} \\Sigma_{ba} \\right). \\end{aligned}\\]"
  },
  {
    "objectID": "week11/week11.html#linear-gaussian-models",
    "href": "week11/week11.html#linear-gaussian-models",
    "title": "Week 11",
    "section": "Linear-Gaussian Models",
    "text": "Linear-Gaussian Models\nLinear-Gaussian models are a unifying generalization of many commonly used models.\nThey are often used implicitly or explicitly in the following:\n\nBayesian linear regression\nPrincipal components analysis\nFactor analysis\nClustering with mixtures of Gaussians\nVector quantization\nKalman filter models\nHidden Markov models with Gaussian noise\n\n\nDetails\nSuppose \\(X\\) and \\(Y\\) have a joint distribution such that\n\\[X \\sim \\mathcal N(\\mu, \\Lambda^{-1})\\] \\[Y \\mid X = x \\sim \\mathcal N(Ax + b, L^{-1}),\\]\nfor some matrix \\(A\\), vectors \\(\\mu\\) and \\(b\\), and symmetric positive definite matrices \\(\\Lambda\\) and \\(L\\).\nThen the marginal distribution of \\(X\\) and the conditional distribution of \\(X \\mid Y = y\\) are\n\\[Y \\sim \\mathcal N(A\\mu + b, L^{-1} + A\\Lambda^{-1}A^\\mathtt{T})\\] \\[X \\mid Y = y \\sim \\mathcal N(C(A^\\mathtt{T}L(y-b) + \\Lambda \\mu), C)\\]\nwhere\n\\[C = (\\Lambda + A^\\mathtt{T}L A)^{-1}.\\]"
  },
  {
    "objectID": "week11/week11.html#bayesian-linear-regression",
    "href": "week11/week11.html#bayesian-linear-regression",
    "title": "Week 11",
    "section": "Bayesian Linear Regression",
    "text": "Bayesian Linear Regression\nConsider a linear regression model:\n\\[Y \\mid \\beta \\sim \\mathcal N (A\\beta, \\sigma^2 I)\\]\nwhere \\(Y = (Y_1, ..., Y_n)^\\mathtt{T}\\) is the vector of outcomes and \\(A\\) is the design matrix, \\(A = \\begin{bmatrix} x_1 & \\cdots & x_n \\end{bmatrix}^\\mathtt{T}\\).\nSuppose we place a normal prior distribution on \\(\\beta \\in \\mathbb{R}^{k}\\), the vector of coefficients\n\\[\\beta \\sim \\mathcal N(0, \\Lambda^{-1}).\\]\nThis is a linear-Gaussian model with \\(X = \\beta\\), \\(\\mu = 0\\), \\(b = 0\\), \\(L^{-1} = \\sigma^2 I\\).\nThus the conditional distribution of \\(\\beta \\mid Y = y\\) is:\n\\[\\beta \\mid Y \\sim \\mathcal N(CA^\\mathtt{T}y/\\sigma^2, C).\\]\n\\[C = (\\Lambda + A^\\mathtt{T}A / \\sigma^2)^{-1}.\\]\n\nWriting out the linear model:\n\\[Y_i = x_i^\\mathtt{T}\\beta + \\varepsilon_i\\]\n\\[\\varepsilon \\sim \\mathcal N(0, \\sigma^2)\\]\n\\[Y = \\begin{bmatrix} Y_1 \\\\ \\vdots \\\\ Y_n \\end{bmatrix} \\sim \\mathcal N(A \\beta, \\sigma^2 I)\\]\n\\[A = \\begin{bmatrix} —x_1^\\mathtt{T}— \\\\ \\vdots \\\\ —x_n^\\mathtt{T}—  \\end{bmatrix}.\\]\nA is called the “design matrix.”\n\nLet’s write out \\(CA^Ty/\\sigma^2\\)\n\\[ = (\\Lambda + A^\\mathtt{T}A / \\sigma^2)^{-1} A^T y / \\sigma^2\\] \\[ = (\\sigma^2 \\Lambda + A^T A)^{-1} A^T y\\] \\[ \\lim_{\\Lambda \\rightarrow 0} (A^\\mathtt{T}A)^{-1} A^\\mathtt{T}y,\\]\nwhich is the OLS estimator.\n\nWhy do we believe the normal distribution is so important?\nIt comes up a lot in practice as a reasonable model because of the central limit theorem.\nIt’s also because it’s computationally, analytically tractable. You can actually derive all these conditional distributions, which is kind of remarkable. That’s not usually the case for many other multivariate distributions.\nKalman filters are big time-series models where you have sensors and you have to keep track in real-time where is the rocket, what is its position, where is it going, and do that based on noisy measurements. It’s taking in lots of data and doing multivariate-normal calculations that you can do analytically.\nYou get a decent amount of flexibility in the multivariate Gaussian: you can not only adjust the means and the variances, but you also have all the covariance parameters which gives you a lot of parameters to work with providing flexibility."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Probability (BST 230) Notes",
    "section": "",
    "text": "Welcome to my notes on (Mostly Non-Measure Theoretic) Probability!\n\n\n\n\n\nI hope you will enjoy them, but you do have to be prepared: the notation is pretty all over the place.\nI’ve been honing my \\(\\LaTeX\\) tikz and pgfplots skills throughout this course, so I hope you enjoy the diagrams I have diligently been creating throughout.\nBesides the above, here are a couple more figures I’m so thrilled to have been able to learn how to make in \\(\\LaTeX\\)."
  },
  {
    "objectID": "week11/week11.html#definition-of-a-statistic",
    "href": "week11/week11.html#definition-of-a-statistic",
    "title": "Week 11",
    "section": "Definition of a Statistic",
    "text": "Definition of a Statistic\nOften we are interested in some function of the sample.\nFor instances, a sample mean, sample variance, maximum, minimum, some percentile, etc.\nA statistic is some function \\(T(X_1, ..., X_n)\\).\nIn the context of samples from a model \\(p(x \\mid \\theta)\\), a statistic is not permitted to depend on the parameter \\(\\theta\\).\nOtherwise, it can be any measurable function of the sample.\nOften, we write \\(T\\) to denote the random variable or random vector \\(T(X_1, ..., X_n)\\).\nBasic examples of statistics:\n\nSample mean: \\(\\bar X = \\frac{1}{n} \\sum_{i=1}^n X_i\\)$\nSample variance: \\(S^2 = \\frac{1}{n-1} \\sum_{i=1}^n (X_i - \\bar X)^2\\)$\nSample standard deviation: \\(S = \\sqrt{S^2}\\)\nMin and max: \\(\\min\\{ X_1, ..., X_n\\}\\) and \\(\\max\\{ X_1, ..., X_n\\}\\)\nOrder statistics \\(X_{(1)}, ..., X_{(n)}\\) are defined by sorting \\(X_1, ..., X_n\\) from least to greatest.\n\nFor example, the median or different percentiles depend on order statistics.\nNote that order statistics introduce dependency among the \\(X_{(1)}, ..., X_{(n)}\\).\nSuppose the \\(\\{X_i\\}\\) are i.i.d. with mean \\(\\mu = \\mathbb{E}X_1\\) and \\(\\sigma^2 = \\text{Var}(X_1) < \\infty\\) (which is the case as a result of one of the inequalities).\n\n\\(\\mathbb{E}\\bar X = \\mu\\)\n\\(\\text{Var}(\\bar X) = \\sigma^2 / n\\)\n\\(\\mathbb{E}\\bar X^2 = \\sigma^2 / n + \\mu^2\\) (this holds in general, regardless of whether \\(\\{X_i\\}\\) are i.i.d. or not).\n\\((n-1) S^2 = (\\sum_{i=1}^n X_i^2) - n \\bar X^2\\)\n\\(\\mathbb{E}S^2 = \\sigma^2\\).\n\nNote that if the variance is finite, then the mean will be finite and exist — e.g., if higher moments exist, then so too do the lower moments.\n\nProof of property 3: \\[\n\\begin{aligned}\n\\text{Var}(\\bar X) & = \\mathbb{E}(\\bar X^2) - (\\mathbb{E}\\bar X)^2  \\\\\n\\mathbb{E}(\\bar X^2) & = \\text{Var}(\\bar X) + (\\mathbb{E}\\bar X)^2  \\\\\n\\mathbb{E}(\\bar X^2) & = \\sigma^2/n + \\mu^2.\n\\end{aligned}\n\\]\nProof of property 4: \\[\n\\begin{aligned}\n(n-1)S^2 & = \\sum_{i=1}^n (X_i - \\bar X)^2 \\\\\n& = \\sum_{i=1}^n (X_i^2 - 2X_i \\bar X + \\bar X^2) \\\\\n& = (\\sum_{i=1}^n X_i^2) - 2 \\bar X \\underbrace{\\sum_{i=1}^n X_i}_{= n \\bar X} + n \\bar X^2 \\\\\n& = (\\sum_{i=1}^n X_i^2) - n \\bar X^2.\n\\end{aligned}\n\\]\nProof of property 5: \\[\n\\begin{aligned}\n\\mathbb{E}(n-1) S^2  & = \\mathbb{E}( \\sum_{i=1}^n X_i^2 ) - \\underbrace{n \\mathbb{E}(\\bar X^2)}_{= n(\\frac{\\sigma^2}{n} + \\mu^2)} \\\\\n& = \\sum_{i=1}^n \\mathbb{E}X_i^2 - n(\\frac{\\sigma^2}{n} + \\mu^2) \\\\\n& = n (\\sigma^2 + \\cancel{\\mu^2}) - (\\sigma^2 + n \\cancel{\\mu^2}) \\\\\n& = (n-1) \\sigma^2 \\\\\n& \\Longrightarrow \\boxed{\\mathbb{E}S^2 = \\sigma^2.}\n\\end{aligned}\n\\]\n\nSuppose \\(\\phi\\) is a population parameter, that is, \\(\\phi\\) is a function of the distribution of \\(X_1, ..., X_n\\).\nA statistic \\(T\\) is an unbiased estimator of \\(\\phi\\) if \\(\\mathbb{E}T = \\phi\\).\nExamples:\n\n\\(\\bar X\\) is an unbiased estimator of \\(\\mu\\)\n\\(S^2\\) is an unbiased estimator of \\(\\sigma^2\\).\n\nWhen the samples are normally distributed, more can be said about the sample mean \\(\\bar X\\) and the sample variance \\(S^2\\).\nIf \\(X_1, ..., X_n \\stackrel{i.i.d.}{\\sim} \\mathcal N(\\mu, \\sigma^2)\\), then\n\n\\(\\bar X\\) and \\(S^2\\) are independent.\n\\(\\bar X \\sim \\mathcal N(\\mu, \\sigma^2/n)\\).\n\\((n-1)S^2 / \\sigma^2 \\sim \\mathcal \\chi^2(n-1)\\).\n\n\nThe rough heuristic for why there’s an \\(n-1\\) in the \\(\\chi^2\\) distribution is that when the \\(-\\bar X\\) is included in the sum used to calculate \\(S^2\\), we are reducing the amount of information contained in the \\(\\{ X_i - \\bar X \\}\\) terms that are summed over."
  },
  {
    "objectID": "week11/week11.html#cochrans-theorem",
    "href": "week11/week11.html#cochrans-theorem",
    "title": "Week 11",
    "section": "Cochran’s Theorem",
    "text": "Cochran’s Theorem\nThe results on the preceding slide can be derived from the following much more general result.\nThe rank of a symmetric matrix \\(A\\) is the number of nonzero eigenvalues.\nCochran’s theorem: Suppose \\(Z \\sim \\mathcal N(0,1)\\) is \\(n\\)-dimensional and \\(A_1, ..., A_k \\in \\mathbb{R}^{n \\times n}\\) are symmetric positive semi-definite matrices. Define \\(Y_i = Z^\\mathtt{T}A_i Z\\). If \\(\\sum_{i=1}^k Y_i = Z^\\mathtt{T}Z\\) then the following are equivalent:\n\n\\(\\sum_{i=1}^k \\text{rank}(A_i) = n\\)\n\\(Y_1, ..., Y_k\\) are independent\n\\(Y_i \\sim \\chi^2(\\text{rank}(A_i))\\) for \\(i = 1, ..., k\\).\n\nCochran’s theorem forms the basis for hypothesis testing in ANOVA and linear regression with unknown outcome variance.\nLooking for a proof reference:\nhttps://en.wikipedia.org/wiki/Cochran%27s_theorem\nhttps://yangfeng.hosting.nyu.edu/slides/cochran%27s-theorem.pdf"
  },
  {
    "objectID": "week11/week11.html#students-t-statistic",
    "href": "week11/week11.html#students-t-statistic",
    "title": "Week 11",
    "section": "Student’s \\(t\\) statistic",
    "text": "Student’s \\(t\\) statistic\nWilliam Sealy Gosset was a statistician and head experimental brewer at Guinness in the early 1900s. He wanted to improve the yield of barley for brewing beer. He developed the theory of \\(t\\) statistics to accurately deal with the uncertainty in small sample sizes. Gosset published under the penname of “Student” since Guinness allowed its scientists to publish under conditions of anonymity, to avoid revealing trade secrets."
  },
  {
    "objectID": "week11/week11.html#z-statistic",
    "href": "week11/week11.html#z-statistic",
    "title": "Week 11",
    "section": "\\(Z\\) Statistic",
    "text": "\\(Z\\) Statistic\nSuppose \\(X_1, ..., X_n \\sim \\mathcal N(\\mu, \\sigma^2)\\) and we want to estimate \\(\\mu\\). The sample mean \\(\\bar X\\) is a natural estimate, but how can we quantify our uncertainty about how close it is to \\(\\mu\\)?\nFirst suppose \\(\\sigma^2\\) is known. Then\n\\[Z = \\frac{\\bar X - \\mu}{\\sigma / \\sqrt{n}} \\sim \\mathcal N(0, 1).\\]\nRemember, the variance of \\(\\bar X\\) is \\(\\sigma^2 / n\\). So the standard deviation of \\(\\bar X\\) is \\(\\sigma / \\sqrt{n}\\).\nWe can use this to form confidence intervals for \\(\\mu\\).\nFor instance, \\(\\bar X \\pm 1.96 \\sigma / \\sqrt{n}\\) is a 95% confidence interval since\n\\[\nP\\left(\\bar X - 1.96 \\frac{\\sigma}{\\sqrt{n}} < \\mu < \\bar X + 1.96 \\frac{\\sigma}{\\sqrt{n}} \\right)\\] \\[ = P(-1.96 < Z < 1.96) \\approx 0.95.\\]\n“If we were to do this experiment repeatedly, say \\(n\\) times, then 95% of the time, this interval will contain the true parameter \\(\\mu\\).”\nWhat if \\(\\sigma^2\\) is unknown? A natural idea is to plug the sample variance \\(S^2\\) into the formula above in place of the true \\(\\sigma^2\\). Unfortunately, when \\(n\\) is small, this is not a good approximation because there is a lot of randomness in \\(S^2\\).\nGosset realized that one could derive the exact distribution of\n\\[T = \\frac{\\bar X - \\mu}{S/\\sqrt{n}}\\]\nand use this to more accurately quantify uncertainty about \\(\\mu\\) when \\(n\\) is small.\n\\(T\\) is called Student’s t statistic and its distribution is called the Student’s t distribution.\nThis distribution is used to form confidence intervals for \\(\\mu\\), construct \\(t\\)-tests, and shows up a lot in hypothesis testing.\nHow could we figure out the distribution of \\(T\\)?\nSuppose \\(X_1, ..., X_n \\stackrel{i.i.d.}{\\sim} \\mathcal N(\\mu, \\sigma^2)\\). We know that \\(\\bar X \\perp\\!\\!\\!\\perp S^2\\) and\n\\[\\bar X \\sim \\mathcal N(\\mu, \\sigma^2/n)\\] \\[(n-1) S^2 / \\sigma^2 \\sim \\chi^2(n-1)\\]\nTherefore,\n\\[T = \\frac{\\bar X - \\mu}{S/\\sqrt{n}} = \\frac{(\\bar X - \\mu)/(\\sigma / \\sqrt{n})}{\\sqrt{S^2/\\sigma^2}} = \\frac{Z}{\\sqrt{V/(n-1)}},\\]\nwhere \\(Z \\sim \\mathcal N(0, 1)\\) and \\(V \\sim \\chi^2(n-1)\\) independently.\nThus, we just need to find the distribution of \\(Z / \\sqrt{V/(n-1)}\\). This can be done using the bivariate transformation formula to get the pdf of \\((T,V)\\) and integrating out \\(V\\). Carrying this out yields Student’s \\(t\\) distribution.\n(The details are that we are defining \\(V = (n-1)S^2 / \\sigma^2\\) and so \\(\\sqrt{V/(n-1)} = \\sqrt{S^2 / \\sigma^2}\\).)\n\n\\[\n\\begin{aligned}\ng : \\begin{pmatrix} Z \\\\ V \\end{pmatrix} \\mapsto & \\begin{pmatrix} \\frac{Z}{\\sqrt{V/(n-1)}} \\\\ V \\end{pmatrix} \\\\\n& = \\begin{pmatrix} T \\\\ V \\end{pmatrix} \\\\\n\\end{aligned}\n\\]\nSo we can carry out the following integration to get the pdf of \\(T\\), \\[\\int f_{T,V}(t,v) \\mathrm d v = f_T(t).\\]\nWhat we find is that Student’s \\(t\\) distribution with \\(\\nu > 0\\) degrees of freedom (can be any \\(\\nu > 0, \\nu \\in \\mathbb R^+\\)), denoted \\(t_\\nu\\), has pdf\n\\[p(x \\mid \\nu) = \\frac{\\Gamma(\\frac{\\nu + 1}{2})}{\\sqrt{\\nu \\pi } \\Gamma \\left( \\frac{\\nu}{2}\\right)} \\left( 1 + \\frac{x^2}{\\nu} \\right)^{-\\frac{\\nu+1}{2}}\\]\nfor \\(x \\in \\mathbb R\\).\n\nIt has the properties that if \\(T \\sim t_\\nu\\), then:\n\n\\(\\mathbb{E}(T) = 0\\) if \\(\\nu > 1\\),\n\\(\\text{Var}(T) = \\frac{\\nu}{\\nu - 2}\\) if \\(\\nu > 2\\).\n\nRelationships:\nIf \\(X_1, ..., X_n \\sim \\mathcal N(\\mu, \\sigma^2)\\) then \\(\\frac{\\bar X - \\mu}{S / \\sqrt{n}} \\sim t_{n-1}\\).\nWhen \\(\\nu =1\\), \\(t_\\nu = \\text{Cauchy}(0,1)\\).\nAs \\(\\nu \\to \\infty\\), then \\(t_\\nu\\) converges to \\(\\mathcal N(0,1)\\).\nThe \\(t\\) distribution is, in some sense, a generalization of the Cauchy distribution, so it is heavy-tailed."
  },
  {
    "objectID": "week11/week11.html#order-statistics",
    "href": "week11/week11.html#order-statistics",
    "title": "Week 11",
    "section": "Order Statistics",
    "text": "Order Statistics\nFor any real numbers, \\(x_1, ..., x_n\\), there is a permutation \\(\\pi\\) that places them in ascending order, that is, \\(x_{\\pi_1}\\) … x_{_n}$. In the context of order statistics, the notation \\(x_{(i)}\\) is used to denote \\(x_{\\pi_i}\\).\nThe order statistics of a random sample \\(X_1, ..., X_n\\) are the sample values placed in ascending order, that is, \\(X_{(1)}, ..., X_{(n)}\\).\nBy construction, \\(X_{(1)} \\leq ... \\leq X_{(n)}\\). \\(X_{(1)} = \\min\\{ X_1, ..., X_n\\}\\) and \\(X_{(n)} = \\max\\{X_1, ..., X_n\\}\\).\nThe sample range is \\(R = X_{(n)} - X_{(1)}\\), the width of the interval spanned by the data.\nThe sample median is\n\\[M = \\left\\{\n\\begin{array}\nX_{((n+1)/2)} \\quad & \\text{ if $n$ is odd} \\\\\n\\frac{1}{2} (X_{(n/2)} + X_{(n/2+1)}) & \\text{if $n$ is even.}\n\\end{array}\n\\right.\\]\n\nMarginal Distributions of Order Statistics\nSuppose \\(X_1, ..., X_n\\) are i.i.d. continuous random variables with pdf \\(f(x)\\) and cdf \\(F(x)\\).\nThe pdf of the \\(j\\)th order statistic, \\(X_{(j)}\\) is\n\\[f_{X_{(j)}} = \\frac{n!}{(j-1)!(n-j)!} f(x) F(x)^{j-1} (1-F(x))^{n-j}\\]\nThe cdf of the \\(j\\)th order statistic, \\(X_{(j)}\\) is\n\\[F_{X_{(j)}} = \\sum_{i=1}^n { n \\choose k } F(x)^k (1-F(x))^{n-k}.\\]\nThe pdf of the maximum, \\(X_{(n)}\\) is\n\\[f_{X_(n)} = nf(x) F(x)^{n-1}.\\]\nThe pdf of the minimum \\(X_{(1)}\\) is\n\\[f_{X_{(1)}} = n f(x) (1-F(x))^{n-1}\\]\nWhen \\(n\\) is odd, the pdf of the median \\(M\\) is\n\\[f_M(x) = \\frac{n!}{\\left( \\frac{n-1}{2} ! \\right)^2 } f(x) F(x)^{\\frac{n-1}{2}} (1-F(x))^{\\frac{n-1}{2}}.\\]"
  },
  {
    "objectID": "week13/week13.html#deterministic-convergence-and-limit-notation",
    "href": "week13/week13.html#deterministic-convergence-and-limit-notation",
    "title": "Week 13",
    "section": "Deterministic Convergence and Limit Notation",
    "text": "Deterministic Convergence and Limit Notation\nLet’s briefly review deterministic convergence.\nSuppose \\(x_1, ..., x_n\\) is a sequence of real numbers.\nWe say that \\(x_n\\) converges to \\(x\\) if for all \\(\\varepsilon > 0\\), there exists \\(N\\) such that for all \\(n \\geq N\\), \\(|x_n - x| < \\varepsilon\\).\nIn other words, for \\(n\\) sufficiently large, \\(|x_n - x|<\\varepsilon\\).\nThe following are equivalent ways of denoting that \\(x_n\\) converges to x:\n\\[x_n \\to \\infty \\; \\; \\text{ as } n \\to \\infty\\]\n\\[x_n \\xrightarrow[n \\to \\infty]{} x\\]\n\\[\\lim_{n \\to \\infty} x_n = x.\\]\nNote that \\(x_n \\to x\\) if and only if \\(\\lim_{n\\to\\infty} |x_n - x| = 0\\)."
  },
  {
    "objectID": "week13/week13.html#weak-law-of-large-numbers",
    "href": "week13/week13.html#weak-law-of-large-numbers",
    "title": "Week 13",
    "section": "Weak Law of Large Numbers",
    "text": "Weak Law of Large Numbers\nRoughly speaking, the law of large numbers (LLN) says that the sample mean converges to the mean.\nThere are many different versions of LLN that apply under different conditions.\nWeak LLN: If \\(Y_1, ..., Y_n\\) are i.i.d. random variables such that \\(\\mathbb{E}Y_1 = \\mu\\) and \\(\\text{Var}(Y_1) < \\infty\\), then for all \\(\\varepsilon > 0\\),\n\\[P \\left( \\left\\vert \\frac{1}{n} \\sum_{i=1}^n Y_i - \\mu \\right\\vert > \\varepsilon \\right) \\to 0\\]\nas \\(n \\to \\infty\\).\nThe proof uses Chebyshev’s inequality."
  },
  {
    "objectID": "week13/week13.html",
    "href": "week13/week13.html",
    "title": "Week 13",
    "section": "",
    "text": "Asymptotics and Convergence\nOutline:\nAsymptotics are an essential part of probability & statistics — but why do we care? After all, we never have infinite data.\nFirst, asymptotically, things usually simplify a lot.\nTwo important ways in which we use asymptotics are:\nIn probability, we consider the asymptotics of random variables, which involves stochastic convergence.\nSuppose \\(1 \\leq p < \\infty\\) and \\(X, X_1, X_2, ... \\in L^p\\), that is \\(\\mathbb{E}|X|^p < \\infty\\) and \\(\\mathbb{E}|X_n|^p < \\infty\\) for all \\(n\\).\nThen \\(X_1, X_2, ...\\) converges in \\(L^p\\) to \\(X\\) if\n\\[\\mathbb{E}|X_n - X|^p \\xrightarrow[n\\to\\infty]{} 0.\\]\nWe write \\(X_n \\xrightarrow[]{L^p} X\\) to denote convergence in \\(L^p\\).\nConvergence in \\(L^p\\) is also called convergence in the \\(p\\)th mean.\nThe most important cases are when \\(p = 1\\) or \\(p = 2\\).\nIf \\(X_n \\xrightarrow[]{L^p} X\\) then \\(\\mathbb{E}|X_n|^p \\to \\mathbb{E}|X|^p\\).\nIf \\(X_n \\xrightarrow[]{a.s.} X\\) then \\(X_n \\xrightarrow[]{p} X\\).\nPartial converse: if \\(X_n \\xrightarrow[]{p} X\\) then there is a subsequence \\(n_1, n_2, ...\\) such that \\(X_{n_k} \\xrightarrow[]{a.s.} X\\).\nIf \\(X_n \\xrightarrow[]{p} X\\) then \\(X \\xrightarrow[]{d} X\\).\nPartial converse: if \\(X_n \\xrightarrow[]{d}\\) and \\(P(X=c) = 1\\) for some constant \\(c\\), then \\(X_n \\xrightarrow[]{p} X\\).\nIf \\(X_n \\xrightarrow[]{L^p} X\\) then \\(X_n \\xrightarrow[]{p} X\\).\nPartial converse: if \\(X_n \\xrightarrow[]{p} X\\) and \\(|X_n| \\leq |Y|\\) for some \\(Y \\in L^p\\), then \\(|X| \\in L^p\\) and \\(X_n \\xrightarrow[]{L^p} X\\).\nOften we know a sequence of \\(X_n\\) converges, but we really want to know about \\(g(X_n)\\) for some function \\(g(x)\\).\nThe Continuous Mapping Theorem states that: Suppose \\(X, X_1, X_2, ... \\in \\mathcal X\\) and \\(g : \\mathcal X \\to \\mathbb R\\) is a function that is continuous at all \\(x\\) in some set \\(A \\subset \\mathcal X\\) such that \\(P(X \\in A) = 1\\).\nIf \\(X_n \\xrightarrow[]{a.s.} X\\) then \\(g(X_n) \\xrightarrow[]{a.s.} g(X)\\). If \\(X_n \\xrightarrow[]{p} X\\) then \\(g(X_n) \\xrightarrow[]{p} g(X)\\). If \\(X_n \\xrightarrow[]{d} X\\) then \\(g(X_n) \\xrightarrow[]{d} g(X)\\).\nExample: If the sample variance \\(S_n^2\\) converges (a.s., or in probability, or in distribution) to \\(\\sigma^2\\) then the sample standard deviation \\(S_n = \\sqrt{S_n^2}\\) converges in the same sense to \\(\\sigma\\).\nSlutsky’s theorem is a result about the convergence of sums, products, or ratios.\nSlutsky’s theorem. If \\(X_1, X_2, \\ldots\\) and \\(Y_1, Y_2, \\ldots\\) are random variables such that \\(X_n \\xrightarrow{d} X\\) and \\(Y_n \\xrightarrow{p} c\\) for some random variable \\(X\\) and constant \\(c\\), then\nConvergence of \\(Y_n\\) to a constant is necessary; the result doesn’t hold in general if \\(Y_n \\xrightarrow{p} Y\\) for a random variable \\(Y\\).\nSuppose we know that \\(\\sqrt{n}(\\bar{X}_n - \\mu) \\xrightarrow{d} \\mathcal{N}(0, \\sigma^2)\\), for instance, by the CLT.\nHowever, what if we are actually interested in \\(g(\\bar{X}_n)\\) for some function \\(g(x)\\)?\nThe delta method tells us how to derive the asymptotic distribution of \\(g(\\bar{X}_n)\\).\nMore generally, the delta method applies not only to sample means \\(\\bar{X}_n\\) but to arbitrary random variables \\(Y_n\\) and a constant \\(\\theta\\) such that \\(\\sqrt{n}(Y_n - \\theta) \\xrightarrow{d} \\mathcal{N}(0, \\sigma^2)\\)."
  },
  {
    "objectID": "week13/week13.html#convergence-in-probability",
    "href": "week13/week13.html#convergence-in-probability",
    "title": "Week 13",
    "section": "Convergence in Probability",
    "text": "Convergence in Probability\nThe Weak LLN is an example of convergence in probability.\nA sequence of random variables \\(X_1, X_2, ...\\) converges in probability to a random variable \\(X\\) if for all \\(\\varepsilon > 0\\),\n\\[P(|X_n - X| > \\varepsilon) \\xrightarrow[n \\to \\infty ]{} 0,\\]\nor equivalently,\n\\[P(|X_n - X| \\leq \\varepsilon) \\xrightarrow[n \\to \\infty ]{} 1.\\]\nWe write \\(X_n \\stackrel{p}{\\longrightarrow} X\\) to denote convergence in probability.\nOther equivalent notations you will see sometimes are\n\\[X_n \\to X \\text{ in probability}\\]\n\\[X \\stackrel{pr}{\\longrightarrow} X\\]\n\\[\\mathop{\\mathrm{p-lim}}_{n \\to \\infty} X_n = X.\\]\nUsually, the limit is a constant, but sometimes the limit is itself a random variable.\n\nFor instance, suppose \\(X \\sim \\mathcal N(0,1)\\) and\n\\[Y_1, Y_2, ... \\mid X = x \\sim \\mathcal N(x, \\sigma^2) \\; \\; \\text{i.i.d.}\\]\nThen \\[\\frac{1}{n} \\sum_{i=1}^n Y_i \\xrightarrow[n \\to \\infty]{p} X.\\]\nHere, the limit is itself a random variable."
  },
  {
    "objectID": "week13/week13.html#almost-sure-convergence",
    "href": "week13/week13.html#almost-sure-convergence",
    "title": "Week 13",
    "section": "Almost Sure Convergence",
    "text": "Almost Sure Convergence\nSometimes, a stronger form of convergence can be established.\nA sequence of random variables \\(X_1, X_2, ...\\) converges almost surely to a random variable \\(X\\) if\n\\[P\\left( \\lim_{n\\to \\infty} X_n = X\\right) = 1.\\]\nMore generally, we say an event \\(E\\) occurs almost surely if \\(P(E) = 1\\).\nWe write \\(X_n \\stackrel{\\text{a.s.}}{\\longrightarrow } X\\) to denote almost sure convergence.\nSome equivalent notations you will see sometimes are:\n\\[X_n \\to X \\; \\; \\text{a.s.}\\] \\[\\lim_{n \\to \\infty } X_n \\stackrel{a.s.} X\\] \\[X_n \\to X \\text{ with probability }1\\]\nRecall that a random variable is defined as a function mapping from the sample space to \\(\\mathbb{R}\\). So for each \\(n\\), \\(X_n : S \\to \\mathbb{R}\\). \\(X(s)\\) is a fixed value for a given \\(s\\).\nSo what we’re really looking at in almost sure convergence is “does \\(X_n(s) \\xrightarrow[n \\to \\infty]{} X(s)\\)?”\nWe could define some event that is all the elements of the sample space for which this happens: \\[E \\coloneqq \\{ s \\in S : X_n(s) \\longrightarrow X(s)\\}.\\]\nSometimes sets like \\(E\\) might fail to be measurable, but in general we won’t worry about that in this class.\n\\(P(E) = 1\\) if and only if \\(X_n \\xrightarrow[]{a.s.} X\\).\n\nHow is almost sure convergence stronger than convergence in probability?\nFirstly, almost sure convergence implies convergene in probability, so it is stronger in the logic sense.\nConsider the case where \\(x\\) is some fixed number.\nFor example, if one were taking draws from a binary random variable. If one drew mostly zeroes but occasionally got a 1, if that 1 happens infinitely rarely, then we would have convergence in probability to 0, but we wouldn’t have almost sure convergence because there is no \\(N\\) such that for some \\(\\varepsilon\\), all of the \\(X_n\\) with \\(n \\geq N\\) are close to zero.\n\nThis makes it relatively easy to reason about a.s. convergence by reducing to reasoning about deterministic sequences.\nWhy is it necessary to allow for a set \\(E^c\\) with probability 0?\n\nSuppose \\(X_1, X_2, ... \\sim \\text{Bernoulli}(q)\\) i.i.d. There are sequences \\(x_1, x_2, ... \\in \\{ 0, 1 \\}\\) such that \\(\\frac{1}{n} \\sum_{i=1}^n x_i\\) doesn’t converge to \\(q\\), but the set of all such sequences has probability 0.\n\nFor example, if one considers sequences of repeated 0s and 1s where one concatenates strings of 0s or 1s (alternating) of length \\(10^i\\) for the \\(i\\)th string. Clearly this sequence that starts with 1 zero, 10 1s, 100 0s, 1000 1s, etc., is just oscillating back and forth in its cumulative mean without converging."
  },
  {
    "objectID": "week13/week13.html#strong-law-of-large-numbers",
    "href": "week13/week13.html#strong-law-of-large-numbers",
    "title": "Week 13",
    "section": "Strong Law of Large Numbers",
    "text": "Strong Law of Large Numbers\nThe Weak LLN shows that the sample mean converges in probability.\nThe strong LLN shows that the sample mean converges almost surely.\nStrong LLN: If \\(X_1, X_2, ...\\) are i.i.d. and \\(\\mathbb{E}|X| < \\infty\\), then \\[\\frac{1}{n} \\sum_{i=1}^n X_i \\xrightarrow[n \\to \\infty]{a.s.} \\mathbb{E}X.\\]\nThis is harder to show, so we won’t prove it here. Optional reading: See Durrett, Section 1.7 for the proof."
  },
  {
    "objectID": "week13/week13.html#equivalent-definition-for-almost-sure-convergence",
    "href": "week13/week13.html#equivalent-definition-for-almost-sure-convergence",
    "title": "Week 13",
    "section": "Equivalent Definition for Almost Sure Convergence",
    "text": "Equivalent Definition for Almost Sure Convergence\nIt turns out that \\(X_n \\stackrel{a.s.}{\\longrightarrow} X\\) if and only if\n\\(P\\left( \\lim_{n\\to\\infty} |X_n - X| < \\varepsilon \\right) = 1.\\)$\nfor all \\(\\varepsilon > 0\\).\nProof: Define the following events: \\[E_{\\varepsilon} = \\{ s \\in S : \\limsup_{n \\to \\infty} |X_n(s) - X(s)| < \\varepsilon\\},\\]\n\\[E = \\{ s \\in S : \\lim_{n\\to\\infty} |X_n(s) - X(s)| = 0\\}.\\]\nWe can analyze these events using properties of deterministic convergence. First, if \\(X_n \\stackrel{a.s.}{\\longrightarrow} X\\) …"
  },
  {
    "objectID": "week13/week13.html#relationship-between-a.s.-convergence-and-in-probability",
    "href": "week13/week13.html#relationship-between-a.s.-convergence-and-in-probability",
    "title": "Week 13",
    "section": "Relationship between a.s. convergence and in probability",
    "text": "Relationship between a.s. convergence and in probability\nIf \\(X_n \\stackrel{a.s.}{\\longrightarrow} X\\) then \\(X_n \\stackrel{p}{\\longrightarrow} X\\).\nHowever, the converse is not true: there are sequences that converge in probability but not almost surely.\nA classic example: Suppose \\(X \\sim \\text{Uniform}(0,1)\\) and\n\\[\n\\begin{aligned}\nY_1 = \\mathbb{1}(0 < X < 1) \\quad & Y_2 = \\mathbb{1}(0 < X < \\frac{1}{2}) \\quad & Y_3 = \\mathbb{1}(\\frac{1}{2} \\leq X < 1) \\\\\nY_4 = \\mathbb{1}(0 < X < \\frac{1}{3}) \\quad & Y_5 = \\mathbb{1}(\\frac{1}{3} \\leq X < \\frac{2}{3}) & Y_6 = \\mathbb{1}(\\frac{2}{3} \\leq X < 1)\n\\end{aligned}\n\\]\nand so forth. Then for any \\(\\varepsilon > 0\\), \\(P(|Y_n| > \\varepsilon) \\to 0\\), so\n\\[Y_n \\stackrel{p}{\\longrightarrow} 0,\\]\nbecause we’re getting more and more zeroes as \\(n\\) increases.\nHowever, \\(Y_n\\) does not converge a.s. since \\(Y_n = 0\\) and \\(Y_n = 1\\) both occur infinitely many times. In fact, in the example, \\(P(Y_n \\text{ fails to converge}) = 1\\).\nWe can make the last statement a little bit more formal by defining the set of events which converge:\n\\[\n\\begin{aligned}\n\\text{Co}&\\text{nvergent Events} = \\\\\n& \\{s \\in S : \\exists y \\in \\mathbb R \\text{ such that } \\\\\n& \\forall \\varepsilon > 0, \\;\\; \\forall \\text{ sufficiently large } n, |Y_n(s) - y| < \\varepsilon \\}\n\\end{aligned}\n\\]\nAnd then take the complement of this set."
  },
  {
    "objectID": "week13/week13.html#convergence-in-distribution-background",
    "href": "week13/week13.html#convergence-in-distribution-background",
    "title": "Week 13",
    "section": "Convergence in Distribution: Background",
    "text": "Convergence in Distribution: Background\nConvergence in probability is often easier to show than a.s. convergence.\nThere is an even weaker form of convergence that holds even more generally, called convergence in distribution.\nA sequence of random variables \\(X_1, X_2,...\\) convergence in distribution to a random variable \\(X\\) if\n\\[F_{X_n}(x) \\xrightarrow[n \\to \\infty]{} F_X(x)\\]\nat all points \\(x \\in \\mathbb{R}\\) where \\(F_X\\) is continuous. Here \\(F_{X_n}\\) and \\(F_X\\) denote the cdfs of \\(X_n\\) and \\(X\\) respectively.\nWe write \\(X_n \\stackrel{d}{\\longrightarrow} X\\) to denote convergence in distribution.\n\nConvergence in distribution is also called “weak convergence.”\nThe following are equivalent ways of writing \\(X_n \\stackrel{d}{\\longrightarrow} X\\).\n\\[\n\\begin{aligned}\nX_n \\stackrel{D}{\\longrightarrow} X \\quad \\quad & X_n \\Rightarrow X \\\\\nX_n \\stackrel{\\mathcal L}{\\longrightarrow} X \\quad \\quad& X_n \\rightsquigarrow X \\\\\n\\mathcal L(X_n) \\longrightarrow \\mathcal L(X) \\quad \\quad & X_n \\to X \\text{ in distribution.}\n\\end{aligned}\n\\]\nIf \\(D\\) is the distribution of \\(X\\), then \\(X_n \\stackrel{d}{\\longrightarrow} D\\) means \\(X_n \\stackrel{d} X\\)."
  },
  {
    "objectID": "week13/week13.html#central-limit-theorem",
    "href": "week13/week13.html#central-limit-theorem",
    "title": "Week 13",
    "section": "Central Limit Theorem",
    "text": "Central Limit Theorem\nThe central limit theorem (CLT) provides an example of convergence in distribution.\nCentral limit theorem: If \\(X_1, X_2, ...\\) are i.i.d. and \\(\\text{Var}(X_1) < \\infty\\), then\n\\[\\frac{1}{\\sqrt{n}} \\sum_{i=1}^n (X_i - \\mu) \\xrightarrow[n\\to\\infty]{d} \\mathcal N(0,\\sigma^2)\\]\nwhere \\(\\mu = \\mathbb{E}X_1\\) and \\(\\sigma^2 = \\text{Var}(X_1)\\).\nThe LLN tells us that \\(\\bar X - \\mu \\longrightarrow 0\\). In contrast, the CLT says that \\(\\sqrt{n}(\\bar X - \\mu)\\) converges in distribution to \\(\\mathcal N(0, \\sigma^2)\\).\nConvergence in distribution is really a property of the sequence of distributions, rather than a the sequence of random variables.\nIn particular, any dependence among \\(X_1, X_2, ...\\) is irrelevant to convergence in distribution. All that matters is their distributions \\(\\mathcal L(X_1), \\mathcal L(X_2), ...\\).\nConvergence in distribution means \\(\\mathcal L(X_n)\\) is close (in a certain sense) to \\(\\mathcal L(X)\\) as \\(n \\to \\infty\\).\nIn contrast, a.s. convergence means \\(X_n(s)\\) gets close to \\(X(s)\\) as \\(n \\to \\infty\\) for all \\(s\\) in some set with probability 1.\nAnd convergence in probability means \\(X_n\\) is close to \\(X\\) with high probability as \\(n \\to \\infty\\).\n\nExample: Minimum of Uniforms\nThis is an example of convergence in probability and in distribution that doesn’t involve the LLN or CLT.\nSuppose \\(X_1, X_2, ... \\sim \\text{Uniform}(0,1)\\) iid and define\n\\[M_n = \\min \\{ X_1, ..., X_n \\}.\\]\nIntuitively, \\(M_n\\) should converge to 0. Formally, for \\(\\varepsilon \\in (0,1),\\)$\n\\[\\begin{aligned}\nP(|M_n - 0| > \\varepsilon) & = P(M_n > \\varepsilon) \\\\\n& = P(X_1 > \\varepsilon, ..., X_n > \\varepsilon) \\\\\n& = \\prod_{i=1}^n P(X_i > \\varepsilon) \\\\\n& = (1-\\varepsilon)^n \\xrightarrow[n\\to\\infty]{} 0.\n\\end{aligned}\\]\nTherefore \\(M_n \\xrightarrow[]{p} 0.\\)\nSo \\(M_n \\xrightarrow[]{p} 0\\), but can we say more? How quickly does \\(M_n\\) converge to 0? What is the asymptotic distribution of \\(M_n\\) near 0?\nFrom the calculation above, if we set \\(\\varepsilon = x/n\\), then \\[P(n M_n \\leq x ) = 1 - P(M_n > x/n) \\xrightarrow[n\\to\\infty]{} 1 - \\exp(-x),\\] the cdf of \\(\\text{Exponential}(1)\\). Therefore,\n\\[nM_n \\xrightarrow[n\\to\\infty]{d} \\text{Exponential}(1).\\]\nThus \\(M_n\\) is approximately distributed as \\(\\text{Exponential}(n)\\) when \\(n\\) is large. This gives us a precise characterization of the asymptotic distribution of \\(M_n\\)."
  },
  {
    "objectID": "week14/week14.html",
    "href": "week14/week14.html",
    "title": "Week 14",
    "section": "",
    "text": "Limit Superiors and Limit Inferiors\nThe limit superior, denoted \\(\\limsup_{n \\to \\infty} x_n\\) is the smallest number \\(u \\in [-\\infty, \\infty]\\) such that for all \\(\\epsilon > 0\\), there exists \\(N\\) such that \\(x_n < u + \\epsilon\\) for all \\(n > N\\).\nThe limit inferior, denoted \\(\\liminf_{n \\to \\infty} x_n\\) is the greatest number \\(l \\in [-\\infty, \\infty]\\) such that for all \\(\\epsilon > 0\\), there exists \\(N\\) such that \\(x_n > l - \\epsilon\\) for all \\(n > N\\).\n\\(\\liminf x_n \\leq \\limsup x_n\\) always and if \\(\\liminf x_n = \\limsup x_n\\) then \\(\\lim x_n\\) exists and all three are equal.\nRoughly speaking, the law of large numbers (LLN) says that the sample mean converges to the mean.\nThere are many different versions of the LLN that apply under different conditions.\nWeak LLN: If \\(Y_1, ... Y_n\\) are iid random variables such that \\(\\mathbb{E}Y_1 = \\mu\\) and \\(\\text{Var}(Y_1) < \\infty\\), then for all \\(\\varepsilon > 0\\),\n\\[P \\left( \\left\\vert \\frac{1}{n} \\sum_{i=1}^n Y_i - \\mu_i \\right\\vert > \\varepsilon \\right) \\longrightarrow 0.\\]\nThe proof is to use Chebyshev’s inequality.\nThe weak LLN is an example of convergence in probability. A sequence of random variables \\(X_1, X_2, ...\\) converges in probability if, for all \\(\\varepsilon > 0\\),\nThere are several versions of the law of large numbers (LLN) and central limit theorem (CLT). Different versions vary in what they show, and the conditions under which they hold. For instance, the weak LLN (WLLN) shows convergence in probability, while the SLLN shows almost sure convergence.\nLet \\(f^{(k)}(t) = \\frac{d^k f}{dt^k}(t)\\) denote the \\(k\\)th derivative of \\(f\\).\nTaylor’s theorem with Lagrange remainder. If \\(f(t)\\) has \\(K\\) derivatives then\n\\[f(t) = \\sum_{k=1}^{K-1} \\frac{f^{(k)}(t_0)}{k!} (t-t_0)^k + \\frac{f^(K)(t_*)}{K!} (t-t_0)^K\\]\nfor some \\(t_*\\) between \\(t\\) and \\(t_0\\).\nFor \\(t\\) near \\(t_0\\), we often approximate \\(f^{(K)}(t_*) \\approx f^{(K)}(t_0).\\)$\nThus a second-order Taylor approximation at \\(t_0 = 0\\) would be\n\\[f(t) \\approx f(0) + f'(0)t + \\frac{1}{2} f''(0)t^2.\\]\nIf \\(\\mathbb{E}(X) = 0\\) and \\(\\text{Var}(X) = \\sigma^2\\), then for \\(t\\) near 0,\n\\[\\phi_X(t) \\approx 1 + i \\mathbb{E}(X)t - \\frac{1}{2} \\mathbb{E}(X^2)t^2 = 1 - \\frac{1}{2}\\sigma^2 t^2.\\]\nIf \\(X,X_1,X_2,...\\) are iid with \\(\\mathbb{E}X = 0\\) and \\(\\sigma^2 = \\text{Var}(X) < \\infty\\), then\n\\[\\begin{aligned}\n\\phi_{\\frac{1}{\\sqrt{n}} \\sum_{k=1}^n X_k}(t) & = \\phi_{\\sum_{k=1}^n X_k}(t / \\sqrt{n}) \\\\\n& = \\prod_{k=1}^n \\phi_{X_k}(t / \\sqrt{n}) \\quad \\text{(by independence)} \\\\\n& = \\phi_X(t / \\sqrt{n})^n \\quad \\text{(by identical distribution)} \\\\\n& \\approx (1 - \\frac{1}{2} \\sigma^2 (t/\\sqrt n )^2)^n \\\\\n& = (1 - \\frac{1}{2} \\sigma^2 t^2 / n)^n \\\\\n& \\xrightarrow[]{n\\to\\infty} \\exp(-\\frac{1}{2} \\sigma^2 t^2 ) = \\phi_{\\mathcal N(0, \\sigma^2)}(t).\n\\end{aligned}\\]\nThus, as long as the approximation above is justified,\n\\[\\frac{1}{\\sqrt{n}} \\sum_{k=1}^n X_k \\xrightarrow[n\\to\\infty]{d} \\mathcal N(0, \\sigma^2).\\]\nTo handle the case of \\(\\mathbb{E}X = \\mu \\neq 0\\), apply this to \\(X_i - \\mu\\).\nSuppose \\(A_1, A_2, ...\\) is a sequence of events. The number of these events containing a given element \\(s \\in S\\) is\n\\[\\sum_{n=1}^\\infty \\mathbb 1(s \\in A_n).\\]\nThe event “\\(A_n\\) infinitely often (i.o.)” is defined as\n\\[\\{ A_n \\; \\mathrm{i.o.}\\} = \\left\\{ s \\in S : \\sum_{n=1}^\\infty \\mathbb 1(s \\in A_n) = \\infty \\right\\}.\\]\nEquivalently, this is more commonly written as\n\\[\\{ A_n \\; \\mathrm{i.o.} \\} = \\bigcap_{m=1}^\\infty \\bigcup_{n=m}^\\infty A_n.\\]\nRoughly, the Borel-Cantelli lemma says that if \\(P(A_n) \\to 0\\) fast enough, then \\(P(A_n \\; \\mathrm{i.o.}) = 0\\).\nBorel-Cantelli Lemma: Suppose \\(A_1, A_2, ...\\) is a sequence of events.\nProof: We’ll only prove 1 here. Suppose \\(\\sum_{n=1}^\\infty P(A_n) < \\infty.\\) Define \\(X(s) = \\sum_{n=1}^\\infty \\mathbb 1(s \\in A_n) \\in \\{ 0 1, 2, ...\\} \\cup \\{ \\infty \\}\\). Then\n\\[P(A_n \\; \\text{i.o.}) = P \\left( \\left\\{ s \\in S : \\sum_{n=1}^\\infty \\mathbb 1(s \\in A_n) = \\infty \\right\\}  \\right) = P(X = \\infty).\\]\nIf \\(P(X = \\infty) > 0\\), then \\(\\mathbb{E}X = \\infty\\). However, …"
  },
  {
    "objectID": "week14/week14.html#weak-lln-for-uncorrelated-variables",
    "href": "week14/week14.html#weak-lln-for-uncorrelated-variables",
    "title": "Week 14",
    "section": "Weak LLN for uncorrelated variables",
    "text": "Weak LLN for uncorrelated variables\nSuppose \\(X_1, X_2, ...\\) are uncorrelated random variables, that is, \\(\\text{Cor}(X_i, X_j) = 0\\) (that is, they could be dependent, just not correlated). Define \\(\\bar X_n = \\frac{1}{n} \\sum_{i=1}^n X_i\\). If \\(\\mathbb{E}\\bar X_{n} \\to \\mu \\in \\mathbb{R}\\) and \\(\\limsup \\frac{1}{n} \\sum_{i=1}^n \\text{Var}(X_i) < \\infty\\), then\n\\[\\bar X_n \\xrightarrow[n\\to\\infty]{p} \\mu.\\]\n(We do not assume \\(X_1, X_2, ...\\) are identically distributed.)\nProof: By Chebyshev’s inequality, for all \\(\\epsilon > 0\\),\n\\[\n\\begin{aligned}\nP(|\\bar X_n - \\mathbb{E}\\bar X_n| > \\varepsilon ) & \\leq \\frac{\\text{Var}(\\bar X_n)}{\\epsilon^2} \\\\\n& = \\frac{\\frac{1}{n^2} \\sum^n_{i,j=1} \\text{Cov}(X_i, X_j)}{\\varepsilon^2} \\\\\n& = \\frac{\\frac{1}{n^2} \\sum^n_{i = 1} \\text{Var}(X_i)}{n \\varepsilon^2} \\xrightarrow[n\\to\\infty]{} 0.\n\\end{aligned}\n\\]\nThe result follows by a trivial application of Slutsky’s theorem:\n\\[\\bar X_n = (\\bar X_n - \\mathbb{E}\\bar X_n) + \\mathbb{E}\\bar X_n \\xrightarrow[p]{n \\to \\infty} 0 + \\mu = \\mu.\\]\n\nA good catch by a student is that to apply to Slutsky’s theorem, one of these need to converge in distribution while the other converges in probability. But since they both converge to a constant, so they both converge in probability and distribution.\nAnother way is to use a sandwich approach:\n\\[|\\bar X_n - \\mathbb{E}\\bar X_n | \\leq |\\bar X_n - \\mu| + \\underbrace{|\\mu - \\mathbb{E}\\bar X_n|}_{\\to 0, \\text{ by assumption}}.\\]"
  },
  {
    "objectID": "week14/week14.html#strong-form-of-the-strong-lln",
    "href": "week14/week14.html#strong-form-of-the-strong-lln",
    "title": "Week 14",
    "section": "Strong form of the Strong LLN",
    "text": "Strong form of the Strong LLN\nThis is sometimes called “Kolmogorov’s Strong Law.”\nKolmogorov’s Strong LLN. Suppose \\(X, X_1, X_2, ...\\) are i.i.d. random variables. Then \\(\\mathbb{E}|X| < \\infty\\) if and only if _{i=1}{n} X_i$ converges almost surely. In this case, the limit equals \\(\\mathbb{E}X\\).\nProof: Kallenberg, Foundations of Modern Probability, Theorem 3.23.\nAs an aside, Kallenberg is a great reference book (not necessarily nice to learn from), but very advanced."
  },
  {
    "objectID": "week14/week14.html#lyapunov-clt",
    "href": "week14/week14.html#lyapunov-clt",
    "title": "Week 14",
    "section": "Lyapunov CLT",
    "text": "Lyapunov CLT\nSuppose \\(X_1, X_2, ...\\) are independent random variables with means \\(\\mu_i = \\mathbb{E}X_i\\) and variances \\(\\sigma_i^2 = \\text{Var}(X_i) < \\infty\\). Define \\(s_n^2 = \\sum_{i=1}^n \\sigma_i^2\\). If there exists \\(\\delta > 0\\) such that\n\\[\\frac{1}{s_n^{2+\\delta}} \\sum_{i=1}^n \\mathbb{E}\\left( | X_i - \\mu_i | ^{2+\\delta} \\right) \\xrightarrow[n\\to\\infty]{} 0,\\]\nthen\n\\[\\frac{1}{s_n} \\sum_{i=1}^n (X_i - \\mu_i) \\xrightarrow[n\\to\\infty]{d} \\mathcal N(0,1).\\]\nIf all of the variances were the same, the \\(s_n\\) term would look familiar as \\(s_n = \\sqrt{n} \\sigma\\).\nMoral: If your sequence of random variables are iid and have finite variances, the CLT holds. If your sequence of random variables are only independent and have finite variance, we need this additional Lyapunov criteria to hold."
  },
  {
    "objectID": "week14/week14.html#multivariate-clt",
    "href": "week14/week14.html#multivariate-clt",
    "title": "Week 14",
    "section": "Multivariate CLT",
    "text": "Multivariate CLT\nSuppose \\(X, X_1, X_2, ...\\) are iid \\(k\\)-dimensional random vectors. If the covariance matrix \\(\\Sigma = \\text{Cov}(X)\\) is well-defined and all of its entries are finite, then\n\\[\\sqrt{n}(\\bar X_n - \\mu ) \\xrightarrow[n\\to\\infty]{d} \\mathcal N(0,\\Sigma)\\]\nwhere \\(\\bar X_n = \\frac{1}{n} \\sum_{i=1}^n X_i\\) and \\(\\mu = \\mathbb{E}X\\).\nNote that we haven’t yet discussed convergence of random vectors in distribution yet.\nWe defined convergence in distributions in terms of the cdfs. However, the cdf-based definition doesn’t generalize well beyond the univariate random variable case. The following equivalent definition is more commonly used and generalizes to random elements in other spaces (such as random graphs, for which there aren’t necessarily cdfs).\nSuppose \\(X, X_1, X_2, ... \\in \\mathcal X\\). We say \\(X_n\\) converges in distribution to \\(X\\) if \\[\\mathbb{E}g(X_n) \\xrightarrow[n\\to\\infty]{} \\mathbb{E}g(X)\\] for all bounded and continuous functions \\(g : \\mathcal X \\to \\mathbb{R}\\).\nIt can be shown that this is equivalent to the cdf-based definition when \\(\\mathcal X = \\mathbb{R}\\).\n\nWhy is it not immediately obvious that this applies to cdfs?\nThey’re not continuous. Let \\(g(x) = \\mathbb 1(x \\leq c)\\). Then\n\\[\\mathbb{E}g(X_n) = P(X_n \\leq c) = F_{X_n}(c),\\]\nbut \\(g\\) is not continuous."
  },
  {
    "objectID": "week14/week14.html#refresher-on-complex-numbers",
    "href": "week14/week14.html#refresher-on-complex-numbers",
    "title": "Week 14",
    "section": "Refresher on Complex Numbers",
    "text": "Refresher on Complex Numbers\nThe imaginary unit \\(i\\) is defined as \\(\\sqrt{-1}\\) so \\(i^2 = -1\\). Algebraically \\(i\\) is treated like any other number, for instance,\n\\[(a + ib) + (a' + ib') = (a + a') + (b + b')i\\] \\[c(a+ib) = ca + icb.\\]\nAn imaginary number \\(z\\) can be represented as \\(z = a+ib\\) where \\(a \\in \\mathbb{R}\\) is the real part and \\(b \\in \\mathbb{R}\\) is the imaginary part.\n\\(a + ib\\) can be visualized as a vector \\((a,b) \\in \\mathbb{R}^2\\).\nThe absolute value or modulus of \\(a+ib\\) is defined as\n\\[|a + ib| = \\sqrt{a^2 + b^2}\\]\nFor \\(\\theta \\in \\mathbb{R}\\), the exponential function \\(\\exp(i \\theta)\\) is defined as \\(\\exp(i\\theta) = \\cos(\\theta) + i \\sin(\\theta)\\).\nThus, the real and imaginary parts of \\(\\exp(i \\theta)\\) trace out the unit circle in \\(\\mathbb{R}^2\\) as \\(\\theta\\) increases. Further \\(|\\exp(i \\theta)| = 1\\).\nStudent-instructor interaction quote: “Are we like getting into imaginary polar coordinates?” “Yeah, you could think of it that way.” “That’s terrible.”"
  },
  {
    "objectID": "week14/week14.html#characteristic-functions-1",
    "href": "week14/week14.html#characteristic-functions-1",
    "title": "Week 14",
    "section": "Characteristic functions",
    "text": "Characteristic functions\nThe characteristic function (cf) of a random variable \\(X\\) denoted \\(\\phi_X(t)\\) is defined for \\(t \\in \\mathbb{R}\\) as\n\\[\\phi_X(t) = \\mathbb{E}\\exp(i t X) = \\mathbb{E}\\cos(t X) + i \\mathbb{E}\\sin (tX).\\]\nThe cf is a complex generalization of the mgf. The advantage is that the cf always exists and is finite.\nProperties: Suppose \\(X, X_1, X_2, ...\\) are random variables.\n\n\\(|\\phi_X(t)| \\leq 1\\) for all \\(t \\in \\mathbb{R}\\).\n\\(\\phi_{cX}(t) = \\phi_X(ct)\\) for all \\(c, t \\in \\mathbb{R}\\).\nIf \\(X_1 \\perp\\!\\!\\!\\perp X_2\\) then \\(\\phi_{X_1 + X_2}(t) = \\phi_{X_1}(t)\\phi_{X_2}(t)\\) for all \\(t \\in \\mathbb{R}\\).\nIf \\(\\mathbb{E}|X|^k < \\infty\\) then \\(\\frac{d^k \\phi_X}{dt^k}(0) = i^k \\mathbb{E}X^k.\\)\n\\(X_1 \\stackrel{d}{=} X_2\\) if and only if \\(\\phi_{X_1}(t) = \\phi_{X_2}(t)\\) for all \\(t \\in \\mathbb{R}\\).\n\\(X_n \\stackrel{d}{\\to} X\\) if and only if \\(\\phi_{X_n} \\to \\phi_X(t)\\) for all \\(t \\in \\mathbb{R}\\).\n\nProperty 1 can be proven with Jensen’s inequality.\nAre there distributions with mgfs that don’t exist but that do have moments? Jeff: I believe if I remember correctly, the mgf exists if all moments exist. So for the pareto distribution and the t distribution, they have some moments but not all moments exist."
  },
  {
    "objectID": "week13/week13.html#example-asymptotics-of-t-statistics-for-non-normal-data",
    "href": "week13/week13.html#example-asymptotics-of-t-statistics-for-non-normal-data",
    "title": "Week 13",
    "section": "Example: Asymptotics of \\(t\\) statistics for non-normal data",
    "text": "Example: Asymptotics of \\(t\\) statistics for non-normal data\nSuppose \\(X_1, \\ldots, X_n\\) i.i.d. with variance \\(\\sigma^2 \\in (0, \\infty)\\), and let \\[ T_n = \\frac{\\sqrt{n}(\\bar{X}_n - \\mu)}{S_n} \\] where \\(\\bar{X}_n\\) is the sample mean, \\(\\mu = \\mathbb{E}X_1\\) is the mean, and \\(S_n\\) is the sample standard deviation.\nBy the CLT, \\(\\sqrt{n}(\\bar{X}_n - \\mu) \\xrightarrow{d} Z\\) where \\(Z \\sim \\mathcal{N}(0, \\sigma^2)\\).\nBy the Strong LLN, \\[ S_n^2 = \\left( \\frac{1}{n-1} \\sum_{i=1}^n X_i^2 \\right) - \\frac{n}{n-1}\\bar{X}_n^2 \\xrightarrow{\\text{a.s.}} \\mathbb{E}(X_1^2)-(\\mathbb{E}X_1)^2 = \\sigma^2. \\]\nThus, \\(S_n^2 \\xrightarrow{p} \\sigma^2\\), so by continuous mapping theorem, \\(S_n \\xrightarrow{p} \\sigma\\).\nTherefore, by Slutsky’s theorem, \\[ T_n = \\frac{\\sqrt{n}(\\bar{X}_n - \\mu)}{S_n} \\xrightarrow{d} \\frac{Z}{\\sigma} \\sim \\mathcal{N}(0, 1). \\] as \\(n \\to \\infty\\)."
  },
  {
    "objectID": "week13/week13.html#example-with-odds-of-an-outcome",
    "href": "week13/week13.html#example-with-odds-of-an-outcome",
    "title": "Week 13",
    "section": "Example with Odds of an Outcome",
    "text": "Example with Odds of an Outcome\nSuppose \\(X_1, \\ldots, X_n \\sim \\text{Bernoulli}(\\theta)\\) i.i.d.\nFor instance, outcomes representing success/failure of a medical treatment.\nThe odds of success are \\(g(\\theta) = \\theta / (1 - \\theta)\\), and we could estimate this via \\(g(\\bar{X}_n) = \\bar{X}_n / (1 - \\bar{X}_n)\\).\nBy the SLLN and the continuous mapping theorem, we know \\(\\bar{X}_n \\xrightarrow{\\text{a.s.}} \\mathbb{E}X_1 = \\theta\\) and \\(g(\\bar{X}_n) \\xrightarrow{\\text{a.s.}} \\theta / (1 - \\theta)\\).\nBut how variable is the estimate \\(g(\\bar{X}_n)\\)? For instance, how could we form approximate standard errors for this estimate?\nThe delta method provides a simple approximation to the distribution of \\(g(\\bar{X}_n)\\) that is asymptotically good.\n\nlibrary(ggplot2)\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ lubridate 1.9.2     ✔ tibble    3.2.1\n✔ purrr     1.0.1     ✔ tidyr     1.3.0\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nlibrary(patchwork)\n\ntheta <- 0.3\n\nn <- 200 \n\ndf <- tibble(x = rbinom(n = 200, size = 10000, prob = .3)  / 10000,\n  y = x/(1-x)\n)\n\ndf_curve <- tibble(x = seq(0, .7, length.out = 1000),\ny = x/(1-x))\n\ntheme_set(theme_bw())\n\nplt1 <- ggplot(df, aes(x = x)) + geom_density(bins = 50, fill = 'cadetblue') + ggtitle(expression(paste(\"Distribution of \", bar(X), \" for \", theta, \" = 0.3\"))) + xlim(c(0,.7))\n\nWarning in geom_density(bins = 50, fill = \"cadetblue\"): Ignoring unknown\nparameters: `bins`\n\nplt2 <- ggplot(df, aes(x = x, y = y)) + geom_line() + ggtitle(\"The odds function\")\nplt3 <- ggplot(df, aes(x = y)) + geom_density(bins = 50, fill = 'cadetblue') + ggtitle(expression(paste(\"Distribution of \", g(bar(X)), \" for \", theta, \" = 0.3\"))) + xlim(c(0,.7))\n\nWarning in geom_density(bins = 50, fill = \"cadetblue\"): Ignoring unknown\nparameters: `bins`\n\nplt1 / plt2 / plt3"
  },
  {
    "objectID": "week13/week13.html#delta-method-1",
    "href": "week13/week13.html#delta-method-1",
    "title": "Week 13",
    "section": "Delta method",
    "text": "Delta method\nSuppose \\(Y_1, Y_2, \\ldots\\) is a sequence of random variables such that\n\\[\n\\sqrt{n}(Y_n - \\theta) \\xrightarrow{d} \\mathcal{N}(0, \\sigma^2)\n\\]\nfor some \\(\\theta \\in \\mathbb{R}\\) and some \\(\\sigma^2 > 0\\). Suppose \\(g\\) is a function such that the derivative \\(g'(\\theta)\\) exists and is nonzero at \\(\\theta\\). Then\n\\[\n\\sqrt{n}(g(Y_n) - g(\\theta)) \\xrightarrow{d} \\mathcal{N}(0, (g'(\\theta))^2 \\sigma^2).\n\\]\nIf \\(Y \\sim \\mathcal{N}(\\theta, \\sigma^2/n)\\) and \\(g(y) = ay + b\\), then by the affine transformation property, \\(g(Y) \\sim \\mathcal{N}(a\\theta + b, a^2\\sigma^2/n)\\), that is,\n\\[\n\\sqrt{n}(g(Y) - g(\\theta)) \\sim \\mathcal{N}(0, (g'(\\theta))^2 \\sigma^2).\n\\]\nThe intuition for the delta method is that \\(g\\) is approximately linear near \\(\\theta\\), and \\(Y_n\\) is close to \\(\\theta\\) when \\(n\\) is large. Thus, \\(g(Y_n)\\) can be approximated as an affine transformation of \\(Y_n\\).\nSuppose \\(X_1, \\ldots, X_n \\sim \\text{Bernoulli}(\\theta)\\) i.i.d. where \\(\\theta \\in (0, 1)\\), and let \\(\\sigma^2 = \\text{Var}(X_1)\\). Note that \\(\\sigma^2 = \\theta(1 - \\theta)\\). By the CLT, since \\(\\mathbb{E}X_1 = \\theta\\) and \\(\\sigma^2 \\in (0, \\infty)\\),\n\\[\n\\sqrt{n}(\\bar{X}_n - \\theta) \\xrightarrow{d} \\mathcal{N}(0, \\sigma^2).\n\\]\nDefine \\(g(\\theta) = \\theta/(1 - \\theta)\\). Then for all \\(\\theta \\in (0, 1)\\),\n\\[\ng'(\\theta) = \\frac{(1 - \\theta) - \\theta(-1)}{(1 - \\theta)^2} = \\frac{1}{(1 - \\theta)^2} \\neq 0.\n\\]\nTherefore, by the delta method,\n\\[\n\\sqrt{n}(g(\\bar{X}_n) - g(\\theta)) \\xrightarrow{d} \\mathcal{N}(0, \\frac{\\theta}{(1 - \\theta)^3}).\n\\]\nThus, using somewhat imprecise notation,\n\\[\ng(\\bar{X}_n) \\approx \\mathcal{N}\\left(g(\\theta), \\frac{\\theta}{n(1 - \\theta)^3}\\right).\n\\]\nIn particular, the standard error of \\(g(\\bar{X}_n)\\) is approximately\n\\[\n\\text{se}(g(\\bar{X}_n)) \\approx \\sqrt{\\frac{\\theta}{n(1 - \\theta)^3}}.\n\\]\nHowever, in practice, we don’t know \\(\\theta\\), so we would need to plug in an estimate of \\(\\theta\\), for instance,\n\\[\n\\text{se}(g(\\bar{X}_n)) \\approx \\sqrt{\\frac{\\bar{X}_n}{n(1 - \\bar{X}_n)^3}}.\n\\]\nWe can use this to form approximate confidence intervals for \\(g(\\theta)\\), for instance, an approximate 95% interval would be\n$$ g({X}_n) (g({X}_n))."
  },
  {
    "objectID": "week15/week15.html",
    "href": "week15/week15.html",
    "title": "Week 15",
    "section": "",
    "text": "Finishing up the Borel-Cantelli Lemma\nSuppose \\(A_1, A_2, ...\\) is a sequence of events.\nProof: We’ll only prove 1 here. Suppose \\(\\sum_{n=1}^\\infty P(A_n) < \\infty.\\) Define \\(X(s) = \\sum_{n=1}^\\infty \\mathbb 1(s \\in A_n) \\in \\{ 0, 1, 2, ... \\} \\cup \\{ \\infty \\}.\\) Then\n\\[P(A_n \\text{ i.o.}) = P\\left( \\left\\{ s \\in S : \\sum_{n=1}^\\infty \\mathbb 1(s \\in A_n) = \\infty \\right\\}\\right) = P(X = \\infty).\\]\nIf \\(P(X = \\infty) > 0\\), then \\(\\mathbb{E}X = \\infty\\). However,\n\\[\\mathbb{E}X = \\sum_{n=1}^\\infty P(A_n) < \\infty\\]\nby assumption, so it must be the case that \\(P(X = \\infty) = 0\\).\nSuppose \\(X_1, X_2, ...\\) iid and \\(P(|X| < c) = 1\\) for some \\(c\\).\nLet \\(\\varepsilon > 0\\). By Hoeffding’s inequality, defining \\(\\mu = \\mathbb{E}X_1\\),\n\\[P(|\\bar X_n - \\mu| > \\varepsilon) \\leq 2 \\exp (-\\frac{1}{2} \\varepsilon^2 n / c^2).\\]\nDefine \\(A_n\\) to be the event that \\(|\\bar X_n - \\mu| > \\varepsilon\\). Then\n\\[\\sum_{n=1}^\\infty P(A_n) \\leq \\sum_{n=1}^\\infty 2 \\exp (-\\frac{1}{2} \\varepsilon^2 n/c^2) < \\infty.\\]\nThus, by Borel-Cantelli, \\(P(A_n \\text{ i.o.}) = 0\\). Consequently,\n\\[P\\left( \\limsup_{n\\to\\infty} |\\bar X_n - \\mu| \\leq \\varepsilon \\right) = P(\\{ A_n \\text{ i.o.}^c\\}) = 1.\\]\nSince this holds for all \\(\\varepsilon > 0\\), this implies that\n\\[\\bar X_n \\xrightarrow[n\\to\\infty]{a.s.} \\mu.\\]\nOutline:"
  },
  {
    "objectID": "week15/week15.html#introduction",
    "href": "week15/week15.html#introduction",
    "title": "Week 15",
    "section": "Introduction",
    "text": "Introduction\nA stochastic process is an infinite-dimensional random object.\nThis sounds complicated, but we’ve already been working with stochastic processes. For example, a sequence of random variables \\(X_1, X_2, ...\\) is a stochastic process.\nMore generally, a stochastic process may be a random function, such as a random function \\(H(x)\\) on the real numbers \\(x \\in \\mathbb{R}\\).\nA Markov chain is a sequence \\(X_1, X_2, ...\\) such that\n\\[X_{t+1} \\perp\\!\\!\\!\\perp X_{1:t-1} \\mid X_{t}.\\]\nBefore looking at Markov chains in general, we’ll look at random walks and branching processes.\n\\[X = (X_1, X_2, ...)\\] \\[X(s) = (X_1(s), X_2(s), ...)\\]\nIt could also be that for every sample in the sample space gives us a different function \\(H(s)(x)\\). A classic example of this is Gaussian Processes. The core idea is that a random function is also a stochastic process."
  },
  {
    "objectID": "week15/week15.html#random-walks",
    "href": "week15/week15.html#random-walks",
    "title": "Week 15",
    "section": "Random Walks",
    "text": "Random Walks\nRandom walks are canonical examples of Markov chains.\nA sequence of random variables \\(Y_0, Y_1, ...\\) is a random ralk if \\(Y_0 = 0\\) (it doesn’t have to) and \\(Y_n = \\sum_{n=1}^n X_i\\) where \\(X_1, X_2, ...\\) are iid.\nIt is a simple random walk with parameter \\(p\\) if \\(X_i \\in \\{ -1, 1\\}\\) such that \\(P(X_i = 1) = p\\) and \\(P(X_i = -1) = q = 1-p\\).\nFrom the LLN and CLT, we know the asymptotics of \\(\\frac{1}{n} (Y_n - \\mathbb{E}Y_n)\\) and \\(\\frac{1}{\\sqrt{n}}(Y_n - \\mathbb{E}Y_n).\\)\nHowever, a lot more can be said about the properties of \\(Y_n\\).\nSince much of statistics involves sample averages, understanding these properties can be useful."
  },
  {
    "objectID": "week15/week15.html#generating-functions",
    "href": "week15/week15.html#generating-functions",
    "title": "Week 15",
    "section": "Generating Functions",
    "text": "Generating Functions\nThe analysis of random walks (and many other combinatorial objects) is simplified by the use of generating functions.\nThe generating function (gf) of a sequence of real numbers,\n$\\(a_0, a_1, a_2, ... \\in \\mathbb{R}\\) is\n\\[G_a(s) = \\sum_{n=0}^\\infty a_n s^n.\\]\nGenerating functions are closely related to mgfs for discrete r.v.s but they afford more flexibility since the \\(a_n\\)s may be negative and do not necessarily sum to 1.\nThe probability generating function (pgf) of a random variable \\(X\\) is\n\\[G_X(s) = \\mathbb{E}(s^X).\\]\nThe relationship between the pgf and mgf is simply that\n\\[M_X(t) = G_X(e^t).\\]\nThe gf of \\(a = (1, 1, 1, 1, ...)\\) is the geometric series\n\\[G_a(s) = \\sum_{n=0}^\\infty s^n = \\frac{1}{1-s}.\\]\nFor a fixed \\(N\\), suppose \\(a_n = { n \\choose n } \\mathbb 1(0 \\leq n \\leq N)\\) is the binomial coefficient. The gf of \\(a_0, a_1, a_2, ...\\) is\n\\[G_a(s) = \\sum_{n=0}^\\infty a_n s^n = \\sum_{n=0}^N { N \\choose n} s^n = (1+s)^N.\\]\n(This uses the Binomial Theorem.)\nSuppose \\(a_n = {2n \\choose n}\\) for \\(n = 0, 1, 2, ...\\). These are called the central binomial coefficients and their gf is\n\\[G_a(s) = \\sum_{n=0}^\\infty { 2n \\choose n} s^n = (1-4s)^{-1/2}.\\]\nThe central binomial coefficients are the elements going straight down the middle of Pascal’s triangle.\n\n\\[G(s) = \\sum_{n=0}^\\infty s_n = \\frac{1}{1-s}\\]\n\\[G'(s) = \\sum_{n=0}^\\infty n s^{n-1} = \\frac{1}{(1-s)^2}.\\]\n\\[s = -1/2\\]\n\\[G'(-1/2) = \\sum_{n=0}^\\infty n (-1/2)^{n-1} = \\frac{1}{(1+1/2)^2} = 4/9.\\]\n\\[\\sum_{n=1}^\\infty (-1)^{n-1} \\frac{n}{2^{n-1}} = \\sum_{k=0}^\\infty (-1)^k \\frac{k+1}{2^k}.\\]\n\nThe convolution of \\(a_n\\) and \\(b_n\\) is the sequence \\(c_n\\) defined by\n\\[c_n = a_0 b_0 + a_1 b_n-1 + ... + a_n b_0 = \\sum_{k=0}^n a_k b_{n-k}.\\]\nThe convolution \\(c\\) is denoted \\(a * b\\).\nConvolution formula. \\(G_{a * b}(s) = G_a(s) G_b(s).\\)\n\\[\n\\begin{aligned}\nG_{a*b}(s) & = \\sum_{n=0}^\\infty \\left( \\sum_{k=0}^n a_k b_{n-k} \\right) s^n  \\\\\n& = \\sum_{n=0}^\\infty \\sum_{k=0}^n a_k s^{k} b_{n-k} s^{n-k} \\\\\n& = \\sum_{n=0}^\\infty \\sum_{k=0}^\\infty a_k s^{k} b_{n-k} s^{n-k} \\\\\n& = \\left( \\sum_{k=0}^\\infty a_k s^k \\right) \\left( \\sum_{j=0}^\\infty b_j s^{j} \\right) = G_a(s) G_b(s). \\\\\n\\end{aligned}\n\\]\n\nThe Fast Fourier Transform underlies a lot of signal processing, and the Fourier transform is quite similar to this. They’re basically a fast way of computing convolutions by first converting sequences to something like the generating function and then multiplying element-wise."
  },
  {
    "objectID": "week15/week15.html#simple-random-walks",
    "href": "week15/week15.html#simple-random-walks",
    "title": "Week 15",
    "section": "Simple Random Walks",
    "text": "Simple Random Walks\nLet \\(Y_n\\) be a simple random walk with parameter \\(p\\) and let \\(T_0 = \\min\\{ n > 0 : Y_n = 0\\}\\) be the first time that \\(Y_n\\) returns to the origin. (If \\(Y_n \\neq 0\\) for all \\(n > 0\\) then we define \\(T_0 = \\infty\\).)\nDefine \\(a_n = P(T_0 = n)\\) and \\(b_n = P(Y_n = 0)\\).\nThe generating functions of these two sequences are\n\\[G_a(s) = \\sum_{n=0}^\\infty a_n s^n \\quad \\text{ and } \\quad G_b(s) = \\sum_{n=0}^\\infty b_n s^n.\\]\nTheorem:\n\n\\(G_b(s) = 1 + G_a(s) G_b(s)\\)\n$G_b(s) = (1-4pqs2){-1/2}\n\\(G_a(s) = 1- (1-4pqs^2)^{1/2}\\)\n\nBefore proving this result, let’s see how it’s useful.\nThe probability of eveer returning to the origin is\n\\[\\sum_{n=0}^\\infty P(T_0 = n) = \\sum_{n=0}^\\infty a_n = G_a(1) = 1-(1-4pq)^{1/2} = 1 - |p-q|\\]\nafter simplifying a bit using the fact that \\(p + q = 1\\).\nThus, after \\(p = q = 1/2\\), then the probability of returning is 1.\nAs \\(p\\) gets smaller, the probability of returning tends to 0.\nIf \\(p = q = 1/2\\), the expected time of first return is\n\\[\\begin{aligned}\n\\sum_{n=0}^\\infty nP(T_0 = n) & = \\sum_{n=0}^\\infty n a_n = G_a'(1)  \\\\\n& = \\frac{d}{ds} \\large \\vert_{s=1} \\left(1-(1-s^2)^{1/2}\\right) \\\\\n& = - \\frac{1}{2} (1-s^2)^{-1/2} (-2s) \\large \\vert_{s=1} = \\infty.\n\\end{aligned}\n\\]"
  },
  {
    "objectID": "week15/week15.html#branching-processes",
    "href": "week15/week15.html#branching-processes",
    "title": "Week 15",
    "section": "Branching Processes",
    "text": "Branching Processes\nA branching process is a mathematical model of the size of a population of individuals over time.\nLet \\(Z_n\\) denote the number of individuals at generation \\(n\\). For simplicity, assume \\(Z_0 = 1\\).\nSuppose that generation \\(n\\), individual \\(i\\) has a random number of offspring, \\(X_{n, i}\\).\nAssume only the offspring survive to the next generation.\nAssume the numbers of offspring \\(X_{n,1}, X_{n,2}, ...\\) are iid with common pgf \\(G_X(s)\\).\nTo analyze this branching process, we’ll use some more properties of generating functions.\n*Compounding formula.$ Suppose \\(X_1, X_2, ...\\) are iid with common pgf \\(G_X(s)\\) and \\(N \\in \\{ 0, 1, 2, ...\\}\\) is a random variable pgf \\(G_N(s)\\). Then the pgf of the compound variable \\(Z=X_1 + ... + X_n\\) is \\[G_Z(s) = G_N(G_X(s)).\\]\nMoment formulas. If \\(X\\) has pgf \\(G(s)\\), then\n\n\\(G(1) = 1.\\)\n\\(\\mathbb{E}(X) = G'(1)\\).\n\\(\\mathbb{E}(X(X-1) \\cdots (X-k+1)) = G^{(k)}(1).\\)\n\\(\\text{Var}(X) = G''(1) + G'(1) (1-G'(1))\\).\n\nAt generation \\(n\\), the total number of offspring is\n\\[Z_{n+1} = X_{n,1} + X_{n,2} + ... + X_{n, Z_n}.\\]\nThus, by the compounding formula, the pgf of \\(Z_{n+1}\\) is\n\\[G_{Z_{n+1}} = G_{Z_n} ( G_X(s)).\\]\nIterating this formula yields that\n\\[\\begin{aligned}\nG_{Z_n}(s) = G_{Z_{n-1}}(G_X(s)) = G_{Z_{n-2}}(G_X(G_X(s))) \\\\\n& = G_X(G_X(\\cdots G_X(s) \\cdots ))\n\\end{aligned}\n\\]"
  }
]