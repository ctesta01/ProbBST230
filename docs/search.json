[
  {
    "objectID": "week1/week1.html#il-famoso-smoking-ra-fisher",
    "href": "week1/week1.html#il-famoso-smoking-ra-fisher",
    "title": "2  Week 1",
    "section": "2.1 il famoso Smoking RA Fisher",
    "text": "2.1 il famoso Smoking RA Fisher\nWe’ll talk about a lot of the methods that Ronald A. Fisher developed. Already in the 1900s it was being observed that there was a strong association between smoking and lung cancer. However, Fisher was a smoker himself and posited that the association between lung cancer and smoking could be explained away by some genetic or biological difference between the smoking and non-smoking population (positing some genes that caused people to desire to smoke).\n\n\n\n\n\nRonald Fisher’s unsupported theory of genetics confounding the smoking-lung cancer relationship\n\n\n\n\nWe’re pretty sure that this was driven not by any substance matter expertise, but rather by Fisher’s love of smoking."
  },
  {
    "objectID": "week1/week1.html#hormone-replacement-therapy",
    "href": "week1/week1.html#hormone-replacement-therapy",
    "title": "2  Week 1",
    "section": "2.2 Hormone Replacement Therapy",
    "text": "2.2 Hormone Replacement Therapy\nIn the mid- to late- 20th century there were a ton of studies linking hormone replacement therapy for older women to better cardiovascular outcomes (lack of coronary heart disease).\nHowever, thankfully due to the Heart and Estrogen/Progestin Study (HERS, in the early 90s) we now know that a lot of those studies were not controlling for socioeconomic status. It turns out that socioeconomic status was highly associated with HRT usage, and associated at least in the US with a lot of better health outcomes across the board.\nIt turned out that HRT when applied at certain times for some people can actually be harmful — but the point is the picture is much muddier than was initially thought and recommendations were rolled back. Later randomized studies were performed that produced reliable bodies of evidence demonstrating either no effect or in some cases harmful effects.\nWe’ll use the baseline data from HERS (not so much interested in the HRT treatment effect), but to investigate the research question:\n\nHow is systolic blood pressure related to age, independently of other well-known cardiovascular risk factors? (Age, diabetes, smoking, etc.)"
  },
  {
    "objectID": "week1/week1.html#prediction-studies",
    "href": "week1/week1.html#prediction-studies",
    "title": "2  Week 1",
    "section": "2.3 Prediction Studies",
    "text": "2.3 Prediction Studies\nTypically in prediction settings, there’s no single exposure of particular interest; mechanisms and confounding is treated as less of a concern (if at all), and the main challenge is that we need to take care to not overfit the data.\nA major theme of this class will be that different tasks require different analysis strategies and diffrent statistical tools."
  },
  {
    "objectID": "week1/week1.html#quantifying-uncertainty",
    "href": "week1/week1.html#quantifying-uncertainty",
    "title": "2  Week 1",
    "section": "2.4 Quantifying Uncertainty",
    "text": "2.4 Quantifying Uncertainty\nTypically standard statistical models have nice theoretical properties because years-and-years ago, we didn’t have much data so people spent their time studying theory instead of data. As a result, we have a lot of nice theories about the uncertainty represented in statistical models.\nAn example of the kind of uncertainty we might be interested in is shown in this figure relating Alzheimer’s disease rates and exposure to PM2.5.\n\n\n\n\n\n\n\n\n\nThis figure is taken from the article Long-term effects of PM2·5 on neurological disorders in the American Medicare population: a longitudinal cohort study by Shi et al, Lancet Planetary Health (2020)."
  },
  {
    "objectID": "week1/week1.html#why-learn-methods-before-study-design",
    "href": "week1/week1.html#why-learn-methods-before-study-design",
    "title": "2  Week 1",
    "section": "2.5 Why Learn Methods Before Study Design",
    "text": "2.5 Why Learn Methods Before Study Design\nAn interesting point made is that it’s important to understand the limitations, strengths of methods, what they can and can’t do, and how to use them before designing a study."
  },
  {
    "objectID": "week1/week1.html#recommended-reading",
    "href": "week1/week1.html#recommended-reading",
    "title": "2  Week 1",
    "section": "2.6 Recommended Reading",
    "text": "2.6 Recommended Reading\nKutner M, Nachtsheim C, Neter J, Li W. Applied Linear Statistical Model. 5th edition. chapters 1-3\nShmueli, G. (2010). To explain or to predict? Statistical Science. https://www.stat.berkeley.edu/~aldous/157/Papers/shmueli.pdf"
  },
  {
    "objectID": "week1/week1.html",
    "href": "week1/week1.html",
    "title": "Week 1",
    "section": "",
    "text": "Introductory Overview\nJeffrey Miller’s office hours: Noon-1pm on Thursdays\nTA: Cathy Xue Office hours: 5:30-6:30pm on Tuesdays in 2-434 (Building 2, Room 434)"
  },
  {
    "objectID": "week1/week1.html#course-description",
    "href": "week1/week1.html#course-description",
    "title": "Week 1",
    "section": "Course description:",
    "text": "Course description:\n\nAxiomatic foundations of probability, independence, conditional probability, joint distributions, transformations, moment generating functions, characteristic functions, moment inequalities, sampling distributions, modes of convergence and their interrelationships, laws of large numbers, central limit theorem, and stochastic processes"
  },
  {
    "objectID": "week1/week1.html#course-readings",
    "href": "week1/week1.html#course-readings",
    "title": "Week 1",
    "section": "Course Readings:",
    "text": "Course Readings:\n\nStatistical Inference (Second Edition), by George Casella and Roger L. Berger. Cengage Learning, 2021.\nProbability: Theory and Examples (Fourth Edition), by Richard Durrett. Cambridge University Press, 2010. (https://services.math.duke.edu/~rtd/PTE/PTE5_011119.pdf)\nIntroduction to Stochastic Processes (Second Edition), by Gregory F. Lawler. Chapman & Hall/CRC, 2006.\n\nThe Durrett book is more measure-theoretic, but covers some things better (according to Miller) than Casella and Berger. Lawler’s book is a gentle introduction to stochastic processes."
  },
  {
    "objectID": "week1/week1.html#labs",
    "href": "week1/week1.html#labs",
    "title": "Week 1",
    "section": "Labs",
    "text": "Labs\nWeekly Tuesdays at 3:45-5:15 in FXB G10"
  },
  {
    "objectID": "week1/week1.html#outline-of-topics",
    "href": "week1/week1.html#outline-of-topics",
    "title": "Week 1",
    "section": "Outline of Topics",
    "text": "Outline of Topics\n\nFundamentals (CB 1.1 - 1.2.2)\n\nSet theory basics, Measure theory basics, Properties of probability measures\n\nProbability basics (CB 1.2.3 - 1.6)\n\nCombinatorics, Conditional probability and Independence, Random variables\n\nTransformations of random variables (CB 2.1)\n\nChange of variable formula for r.v.s, Probability integral transform\n\nExpectations of random variables CB 2.2 - 2.4)\n\nMean and variance, Moments, MGFs, Differentiation and limits of integrals\n\nFamilies of distributions (CB 3.1 - 3.5)\n\nDiscrete and continuous families, exponential families, location-scale families\n\nInequalities (CB 3.6, 3.8, 4.7)\n\nMarkov, Chebyshev, Gauss, Hölder, Cauchy-Schwarz, Minkowski, Jensen\n\nMultiple random variables (CB 4.1 - 4.6)\n\nRandom vectors, conditional distributions, independence, mixtures, covariance\n\nand correlation\nGaussian distributions (Bishop pp. 78-93, in Files/Reading on Canvas site)\n\nMultivariate normal, marginals and conditionals, linear-Gaussian model\n\nStatistics of a random sample (CB 5.1 - 5.4)\n\nSampling distributions, Sums of random variables, Student’s t and Snedecor’s F distribution, Order statistics and friends\n\nAsymptotics (CB 5.5)\n\nModes of convergence, Limit theorems, Delta method, Borel-Cantelli lemma\n\nLaws of large numbers (CB 5.5, D 2.2 - 2.4)\n\nWeak laws of large numbers, Strong laws of large numbers, Generalizations\n\nCentral limit theorems (CB 5.5, D 3.1 - 3.4)\n\nWeak convergence, characteristic functions, central limit theorems\n\nGenerating random samples (CB 5.6)\n\nInverse cdf method, accept/reject method, Markov chain Monte Carlo\n\nStochastics processes (L 1 - 3)\n\nMarkov chains, Random walks, Branching processes, Poisson processes"
  },
  {
    "objectID": "week1/week1.html#introduction",
    "href": "week1/week1.html#introduction",
    "title": "Week 1",
    "section": "Introduction",
    "text": "Introduction\nHow could we tell if either of the two sequences were faked.\n\nstr1 <- \"1 0 0 0 0 1 0 0 1 1 0 0 1 1 0 0 0 1 1 1 1 1 0 0 0 1 1 1 1 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1 1 0 0 0 1 1 0 1 0 0 0 1 1 0 1 1 0 0 0 0 0 1 1 0 1 0 0 1 0 1 1 0 0 0 1 0 1 1 0 1 0 0 1 0 1 0 1 1 0 0 1 0 1\"\n\nstr2 <- \"1 1 0 0 1 0 0 0 1 1 1 0 1 0 1 0 0 0 1 0 1 1 1 0 1 0 0 0 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 0 0 0 1 0 1 1 0 1 0 1 0 0 1 1 1 1 0 1 0 1 0 1 0 0 1 1 0 1 0 0 1 0 1 0 0 1 1 1 0 1 1 1 0 0 1 1 1 0 1 0 1 0 1 0 1 0\"\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.2     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nstr_count(str1, \"0|1\")\n\n[1] 100\n\nstr_count(str1, \"1\")\n\n[1] 43\n\nstr_count(str1, \"0\")\n\n[1] 57\n\nstr_count(str2, \"0|1\")\n\n[1] 100\n\nstr_count(str2, \"1\")\n\n[1] 53\n\nstr_count(str2, \"0\")\n\n[1] 47\n\nstr1_num <- as.numeric(unlist(stringr::str_split(str1, \" \")))\nstr2_num <- as.numeric(unlist(stringr::str_split(str2, \" \")))\n\n# the first way I proposed was to look at the probability of \n# the coin being \"fair\" given the beta distribution parameterized \n# by the observed coinflips \nx <- seq(0,1,0.01)\ncurve(dbeta(x, str_count(str1, \"0\")+1, str_count(str1, \"1\")+1))\n\n\n\ncurve(dbeta(x, str_count(str2, \"0\")+1, str_count(str2, \"1\")+1))\n\n\n\n# then we tried looking at the running mean\nplot(1:100, cummean(str1_num), type='l')\n\n\n\nplot(1:100, cummean(str2_num), type='l')\n\n\n\n# another classmate suggested that the human-generated \n# sequence may have more anti-correlation than the\n# real sequence because more anti-correlation \"looks\" more\n# random \ncor(lag(str1_num), str1_num, use = 'pairwise.complete.obs')\n\n[1] 0.007518797\n\ncor(lag(str2_num), str2_num, use = 'pairwise.complete.obs')\n\n[1] -0.2367884\n\n\nMiller suggests we could also look at it as a sequence of random variables.\n\nstr1_as_geometric_series <- sapply(unlist(stringr::str_split(stringr::str_remove_all(str1, \" \"), \"1\")), nchar)\nunname(str1_as_geometric_series)\n\n [1] 0 4 2 0 2 0 3 0 0 0 0 3 0 0 0 3 3 8 0 0 3 0 1 3 0 1 0 5 0 1 2 1 0 3 1 0 1 2\n[39] 1 1 0 2 1 0\n\nstr2_as_geometric_series <- sapply(unlist(stringr::str_split(stringr::str_remove_all(str2, \" \"), \"1\")), nchar)\nunname(str2_as_geometric_series)\n\n [1] 0 0 2 3 0 0 1 1 3 1 0 0 1 3 0 0 0 3 0 1 1 0 0 3 1 0 1 1 2 0 0 0 1 1 1 2 0 1\n[39] 2 1 2 0 0 1 0 0 2 0 0 1 1 1 1 1\n\ncurve(dgeom(x, prob = .5), from = 0, to = 10, n = 11)"
  },
  {
    "objectID": "week1/week1.html#history-of-probability",
    "href": "week1/week1.html#history-of-probability",
    "title": "Week 1",
    "section": "History of Probability",
    "text": "History of Probability\nGames of chance have been played for millenia. Early dice games were played with “astragali”, or “knucklebones”, from the ankle of a sheep or goat.\n\n\n\n\n\n\n\n\n\nEgyptian tomb paintings from 3500 BC show games played with astragali, and ancient Greek vases show young men tossing the bones into a circle. Gambling in these games was common, so it would have been advantageous to have some understanding of probability.\nInterest in gambling led mathematicians in the 1500-1600s to begin to formalize the rules of probability.\nTwo players put equal money in a pot. The first player to win 8 rounds of a game gets all the money. If they have to stop before finishing, how should the money be divided between them based on how much they would have won, on average?\nAround 1654, Blaise Pascal and Pierre de Fermat developed the concept of expected value to solve this problem.\nChristiaan Huuygens built upon this in his 1657 textbook on probability, “De Ratiociniis in Ludo Aleae” (“The Value of all Chances in Games of Fortune,”)\nIn the early 1700s, Jacob Bernoulli and Abraham De Moivre wrote foundational books on probability.\nThey systematically developed the mathematics of probability, focusing primarily on discrete problems.\nCombinatorial approaches were developed to handle difficult probability calculations.\nBernoulli proved the first version of the law of large numbers.\nIn 1812, Pierre-Simon Laplace published his book “Théorie analytique des probabilités”.\nLaplace developed or advanced many key methods and results in modern probability and statistics.\nGenerating functions, characteristic functions, linear regression, density functions, Bayesian inference, and hypothesis testing.\nHe employed advanced calculus and real/complex analysis,\ntaking probability calculations to a whole new level.\nLaplace proved the first general version of the central limit theorem.\nIn the 1930s, Andrey Kolmogorov introduced the measure theoretic foundations of modern probability.\nMeasure theory had recently been developed to resolve certain paradoxes that arose in defining volume and integration.\nKolmogorov applied measure theory to put probability on solid theoretical footing.\nThis is particularly important for limits and derivatives of integrals, conditional distributions, and stochastic processes."
  },
  {
    "objectID": "week1/week1.html#set-theory-basics",
    "href": "week1/week1.html#set-theory-basics",
    "title": "Week 1",
    "section": "Set Theory Basics",
    "text": "Set Theory Basics\nThe sample space denoted \\(S\\) is the set of possible outcomes of an experiment.\nExamples include \\(S = \\{ H, T \\}\\) for a coin toss, or math SAT scores \\(S = \\{ 200, 201, ..., 799, 800 \\}\\), or time-to-events: \\(S = (0, \\infty)\\).\nWe say that an event \\(E\\) is a subset of \\(S\\) that is \\(E \\subset S\\).\nA set \\(A\\) is a collection of elements. We say that \\[A \\cup B = \\{ x \\colon x \\in A \\text{ or } x \\in B \\}.\\]\n\\[A \\cap B = \\{ x \\colon x \\in A \\text{ and } x \\in B \\}.\\]\n\\[A^c = \\{ x \\in S \\colon x \\not \\in A \\}\\]\n\\[A \\backslash B = \\{ x \\colon x \\in A \\text{ and } x \\not \\in B \\}.\\]\nThe empty set is denoted \\(\\varnothing = \\{\\}\\).\n\\(A \\subset B\\) means that if \\(x \\in A\\) then \\(x \\in B\\).\n\nProperties of Set Operations\nCommutativity:\n\\[A \\cup B = B \\cup A, \\quad A \\cap B = B \\cap A\\]\nAssociativity:\n\\[\nA \\cup (B \\cup C) = (A \\cup B) \\cup C \\quad\nA \\cap (B \\cap C) = (A \\cap B) \\cap C\n\\]\nDistributive Laws:\n\\[ A \\cup (B \\cup C) = (A \\cap B) \\cup (A \\cap C)\n\\quad\nA \\cup (B \\cap C) = (A \\cup B) \\cap (A \\cup C)\\]\nDeMorgan’s Laws:\n\\[ (A \\cup B)^c = A^c \\cap B^c \\quad\n(A \\cap B)^c = A^c \\cup B^c \\]\n\n\nSigma Algebras\nDefinition. Suppose that \\(\\mathcal B\\) is a set of subsets of a sample space \\(S\\). Then \\(\\mathcal B\\) is a sigma-algebra if:\n\n\\(\\varnothing \\in \\mathcal B\\).\nif \\(A \\in \\mathcal B\\) then \\(A^c \\in \\mathcal B\\).\nif \\(A_1, A_2, ... \\in \\mathcal B\\) then \\(\\bigcup_{i=1}^\\infty A_i \\in \\mathcal B\\)\n\n\nThe powerset, denoted \\(2^S\\) is a specific example of a sigma algebra.\n\n\n\nWe have to be very careful that \\(\\varnothing\\) must be an element of \\(\\mathcal B\\) in order for \\(\\mathcal B\\) to be a sigma algebra. \\(\\varnothing\\) is certainly a subset of any set, but \\(\\varnothing\\) needs to be an element of \\(\\mathcal B\\) as a collection of sets.\n\nThe smallest sigma algebra, called the trivial sigma algebra, is \\(\\{ \\varnothing, S \\}\\) for a sample space \\(S\\). When \\(S\\) is uncountable, we usually don’t use the power set as a sigma algebra. Instead, we typically opt for using the Borel sigma algebra.\nFor a topological space \\(S\\), the Borel sigma algebra, denoted \\(\\mathcal B(S)\\) is the smallest sigma algebra containing all open sets.\nDefinition. Let \\(X\\) be a set and \\(\\tau\\) be a family of subsets of \\(X\\). Then \\(\\tau\\) is a topology and \\((X, \\tau)\\) is a topological space if\n\nBoth the empty set and \\(X\\) are elements in \\(\\tau\\).\nAny union of elements of \\(\\tau\\) is an element of \\(\\tau\\).\nAny intersection of finitely many elements of \\(\\tau\\) is an element of \\(\\tau\\).\n\nThe members of \\(\\tau\\) are called open sets in \\(X\\).\nWhen \\(A \\in \\mathcal B\\), we say that \\(A\\) is a measurable set.\n\n\nProbability Measures\nDefinition. If \\((S, \\mathcal B)\\) is a measurable space, then \\(P: \\mathcal B \\to \\mathbb R\\) is a probability measure if:\n\n\\(P(A) \\geq 0\\) for all \\(A \\in \\mathcal B\\). (non-negativity)\n\\(P(S) = 1\\) (unitarity)\nif \\(A_1, A_2, ... \\in \\mathcal B\\) are pairwise disjoint, then \\[P\\left(\\bigcup_{i=1}^\\infty A_i \\right) =\n  \\sum_{i=1}^\\infty P(A_i).\\] (countable additivity)\n\nThese properties are called the axioms of probability, or sometimes Kolmogorov’s axioms.\nIf \\(A \\in \\mathcal B\\) we call \\(A\\) a measurable set.\nIn this course, we may assume that the sets we are working with are measurable. Almost exclusively we will be working with the Borel sigma algebra. While non-measurable sets do exist in this setting, they do not often arise in practice.\n\nTJ asks: “I know it’s possible to demonstrate non-measureable sets non-constructively, but is it possible to demonstrate them constructively?”\nMiller: “I think you have to use infinite series/sets [and the axiom of choice].”"
  },
  {
    "objectID": "week1/week1.html#probability-measure-on-a-countable-set",
    "href": "week1/week1.html#probability-measure-on-a-countable-set",
    "title": "Week 1",
    "section": "Probability measure on a countable set",
    "text": "Probability measure on a countable set\nSuppose that \\(S = \\{ s_1, s_2, ... \\}\\) is a countable set.\nLet \\(p_1, p_2, ... \\geq 0\\) such that \\(\\sum_{i=1}^\\infty p_i = 1\\).\nFor \\(A \\subset S\\), define \\[P(A) = \\sum_{i=1}^\\infty p_i \\mathbb 1(i \\in A).\\]\nWe will write that \\(\\mathbb 1(\\cdot)\\) to denote the indicator function, where\n\\[\\mathbb 1(C) = \\left\\{ \\begin{array}{ll}\n1 & \\text{ if condition } C \\text{ is true} \\\\\n0 & \\text{ if condition } C \\text{ is false}\n\\end{array}\n\\right.\\]\nSuppose you toss a fair coin until you get heads. Then \\(p_k\\), the probability that you toss the coin \\(k\\) times, is \\((1/2)^k\\). This defines a probability measure on \\(S = \\{ 1, 2, ... \\}\\).\nWe could imagine measuring how long a lightbulb lasts until it dies after being left on. It could die at any non-negative time \\(t \\geq 0\\). The probability \\(P([0,t))\\) be the probability the lightbulb dies before time \\(t\\). This defines a probability measure on \\(S = [0,\\infty).\\) (Of course, \\([0,\\infty)\\) is a continuous example and not a countable set).\nFor any probability measure, we have that:\n\n\\(P(A^c) = 1 - P(A)\\)\n\\(P(\\varnothing) = 0\\)\n\\(P(A) \\leq 1\\)\nif \\(A \\subset B\\) then \\(P(A) \\leq P(B)\\)\n\\(P(A) = P(A \\cap B) + P(A \\cap B^c)\\)\n\\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\).\n\nProofs:\n\n\\(P(S) = 1\\), and \\(A, A^c\\) are pairwise disjoint and a partition of \\(S\\), so \\(1 = P(S) = P(A^c \\cup A) = P(A) + P(A^c)\\). Subtracting from both sides, \\(P(A) = 1 - P(A^c)\\).\n\\(\\varnothing\\) and \\(S\\) are disjoint, so \\(1 = P(S) = P(S) + P(\\varnothing)\\). Subtracting from both sides, we have that \\(1 - 1 = P(\\varnothing)\\)\nWe have from 1 that \\(1 - P(A^c) = P(A)\\), and \\(P(A^c) \\geq 0\\), so then \\(P(A) \\leq 1\\)\nWe can write that \\(B \\backslash A\\) and \\(A\\) as disjoint sets since \\(A \\subset B\\). Then \\(P(B) = P(A \\cup B \\backslash A) = P(A) + P(B \\backslash A)\\). Since \\(P(B \\backslash A)\\) we have that \\(P(B) \\geq P(A)\\).\nWe need to show that \\(A = (A \\cap B) \\cup (A \\cup B^c)\\). If \\(a \\in A\\) then either \\(a \\in B\\) or \\(a \\in B^c\\), but not both by the definition of complement. Therefore \\(A \\cap B\\) and \\(A \\cap B^c\\) are disjoint and their union is equal to \\(A\\). Hence \\(P(A) = P(A \\cap B \\bigcup A \\cap B^c) = P(A \\cap B) + P(A \\cap B^c)\\).\nFirst, note that \\(A \\cup B = A \\cap (B \\backslash A)\\). Thus \\(P(A \\cup B) = P(A \\cup (B \\backslash A))\\). Since the latter are disjoint, we establish that \\(P(A \\cup B) = P(A) + P(B \\backslash A)\\). Now if we consider that \\(B = (B \\backslash A) \\cup (A \\cap B)\\), and that these are disjoint sets, we have that \\(P(B) = P(B \\backslash A) + P(A \\cap B)\\). Rearranging, we have that \\(P(B \\backslash A) = P(B) - P(A \\cap B)\\). Substituting, now we have that \\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\) as desired."
  },
  {
    "objectID": "week1/week1.html#properties-of-probability-measures",
    "href": "week1/week1.html#properties-of-probability-measures",
    "title": "Week 1",
    "section": "Properties of Probability Measures",
    "text": "Properties of Probability Measures\nLaw of total probability: for any partition \\(B_1, B_2, ...\\) of \\(S\\), we have that \\[P(A) = \\sum P(A \\cap B_i)\\]\nBoole’s inequality (aka union bound): For \\(A_1, A_2, ...\\),\n\\[P(\\bigcup_{i=1}^\\infty A_i) \\leq \\sum_{i=1}^\\infty P(A_i).\\]\nBonferroni’s inequality: For any \\(A_1, A_2,...\\),\n\\[P\\left(\\bigcap_{i=1}^\\infty A_i \\right) \\geq 1 - \\sum_{i=1}^\\infty P(A_i^c).\\]\nBoole’s inequality is often useful when we want to show that some event has probability near zero. For example, \\(P(E) = P(A_1 \\cup A_2 \\cup A_3) \\leq P(A_1) + P(A_2) + P(A_3) \\leq 3\\epsilon\\)."
  },
  {
    "objectID": "week1/week1.html#selecting-k-items-from-n-options",
    "href": "week1/week1.html#selecting-k-items-from-n-options",
    "title": "Week 1",
    "section": "Selecting \\(k\\) items from \\(n\\) options",
    "text": "Selecting \\(k\\) items from \\(n\\) options\n\n\n\n\nwithout replacement\nwith replacement\n\n\n\n\nordered\n\\(\\frac{n!}{(n-k)!}\\)\n\\(n^k\\)\n\n\nunordered\n\\({n \\choose k}\\)\n\\({ n + k - 1 \\choose k }\\)"
  }
]