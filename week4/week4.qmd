---
title: Week 4
---

::: {.content-hidden}
$$
\newcommand{\E}[0]{\mathbb{E}}
\newcommand{\R}[0]{\mathbb{R}}
\newcommand{\t}[1]{\text{#1}}

% 1 create conditionally independent symbol:
\newcommand\independent{\perp\!\!\!\perp}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}

\newcommand{\Var}[0]{\text{Var}}
\newcommand{\Cov}[0]{\text{Cov}}
\newcommand{\e}[0]{\epsilon}

\newcommand{\argmax}[0]{\text{argmax}}
$$
:::

## Recap: Well-Defined vs. Undefined Expectations

The positive part of $X$ is <span class='vocab'>well-defined</span> if either $E X^+ < \infty$ 
or $E X^- < \infty$. 

A random variable $X$ has the Zeta($s$) distribution for 
$s > 1$, if it has pmf 

$$f_X(k) = P(X = k) = \frac{1}{\zeta(s)k^s} \mathbb 1 (k \in \mathbb \{ 1, 2, ... \})$$

where $\mathbb \zeta (s)$ is the Riemann zeta function. The
use of the Riemann zeta function may seem scary, but it's 
really just acting as the normalizing constant here so that 
this is a proper pmf. 

Since $EX^- = E(-\min(X,0)) = 0$, $EX$ is well-defined. 

Recall that $\zeta(s) = \sum_{i=1}^\infty \frac{1}{k^s}$.

However, if $s \leq 2$ then the mean is infinite: 

$$EX = \sum_{i=1}^\infty k f_X(k) = \sum_{i=1}^\infty 
\frac{1}{\zeta (s) k^{s-1}} = \infty.$$

Now suppose that $Y$ is a discrete random variable with 
pmf 

$$f_Y(k) = P(Y=k) = \frac{1}{2ck^2} \mathbb 1 (|k| \in \{ 1, 2, ... \})$$

where $c = \zeta(2)$. 

Then $EY^+ = \infty$ and $EY^- = \infty$. For example, 

$$EY^+ = \sum_{k=0}^\infty kP(Y^+ = k) = \sum_{k=1}^\infty \frac{1}{2ck} = \infty.$$

So the mean of $Y$ is not well-defined (or, undefined).


### Cauchy Distribution Example 

A random variable $X$ has the Cauchy(0,1) distribution if it has 
the pmf 

$$f_X(x) = \frac{1}{p} \frac{1}{1+x^2}$$

for $x \in \mathbb R$. If $X \sim \text{Cauchy}(0,1)$ 
then $EX$ is undefined. 

```{r, fig.align='center', echo=FALSE, out.width='50%'}
knitr::include_graphics("standalone_figures/cauchy_pdf/cauchy_pdf.svg")
```

The Cauchy distribution has heavy tails, meaning it can take very large values with non-negligible probability. 

## Moments 

Let $k$ be a positive integer. The $k$th moment of $X$ is 
$E(X^k)$. The $k$th central moment of $X$ is $E((X-EX)^k)$. 

The variance of a random variable is the 2nd central
moment: 

$$\Var(X) \stackrel{def}{=} E((X-EX)^2).$$

$\Var(X)$ is sometimes denoted $\sigma^2(X)$ or simply $\sigma^2$. 

The standard deviation of $X$ is $\sqrt{\Var(X)}$. 

Both the variance $\sigma^2$ and the standard deviation
$\sigma$ quantify how spread out a distribution is. However, 
$\sigma$ is more interpretable since it is in the 
same units as $X$. 

:::{.hottip}
If the mean is undefined for a distribution, 
the variance and standard deviaion will also be undefined,
as in the Cauchy distribution. How might we quantify the 
spread of the distribution? One might use quantiles. 
Median absolute deviation. A really simple approach might 
be the difference between the 95th and 5th percentiles. 
:::

#### Properties of Variance 

(1) If $\Var(X) < \infty$, then for any $a,b \in \mathbb R$,

$$\Var(aX + b) = a^2 \Var(X).$$

(2) A useful formula for the variance is $\Var(X) = EX^2 - (EX)^2$. 

(3) Suppose $Y$ is an estimator of some quantity 
$y_0$. Then the mean squared error is 
$$mse = E(|Y - y_0|^2) = (EY - y_0)^2 + E((Y-EY)^2).$$

$$ = \text{bias}^2 + \text{variance}$$

:::{.cooltip}
**Proof of 2.**
$$\Var(X) = E((X - E X)^2)$$
$$ = E(X X - X E X - E X X + (E X)^2)$$
$$ = E X^2 - 2E XE X + (E X)^2$$ 
$$ = E (X^2) - (E X)^2$$


**Proof of 1 using 2.**
Now apply the 2nd to the first question: 

$$\Var(aX+b) = E((aX+b - E(aX+b))^2)$$

or $$\Var(aX+b) = E((aX+b)^2) - E(aX+b)^2 $$
$$ = E(a^2X^2+2abE X + b^2) - (aE X+b)^2 $$

$$ =  (a^2E(X^2)+\cancel{2abE X} + \cancel{b^2}) - (a^2(E X)^2+\cancel{2abE X}+\cancel{b^2})$$
$$ = a^2((E X^2)^2 - E(X)^2) $$
$$ = a^2\Var(X)$$

**Proof of 1 using definitions.**
Using the 2nd central moment formula:

$$\Var(aX+b) = E((aX+b - E(aX+b))^2)$$
$$ = E((aX+b - (aE X+b))^2)$$
$$ = E((aX - (aE X)^2)$$
$$ = a^2E((X - (E X)^2)$$
$$ = a^2\Var(X)$$

**Proof of 3.** Mean squared error is defined as 
$$mse = E((Y - y_0)^2)$$

A nice trick is to add and subtract by the same thing. 

$$ \text{mse} = E((Y - EY + EY - y_0)^2)$$
$$ = E((Y-EY)^2 + 2(Y-EY)(EY-y_0) + (EY - y_0)^2)$$
$$ = E((Y-EY)^2) + 2\underbrace{(EY-EY)}_{=0}(EY-y_0) + (EY - y_0)^2$$
$$ = \underbrace{E((Y-EY)^2)}_{\text{variance}} + \underbrace{(EY - y_0)^2}_{\text{bias}^2}$$
:::

:::{.hottip}
A good illustration of the  bias-variance tradeoff
is in estimating the sample variance of normally distributed
values. 

Suppose that $X_1, X_2, ... \sim \mathcal N(\mu, \sigma^2)$.

$$\bar x = \frac{1}{n} \sum_{i=1}^n X_i$$

$$\hat \sigma^2 = \frac{1}{n-1} \sum_{i=1}^n (X_i-\bar X)^2$$

The above is unbiased, but it's not the estimator with 
lowest mse. One can get a better estimator with lower mean-squared-error by using either $1/n$ or $1/(n+1)$. 
For more details on the $1/(n+1)$ correction, look 
at page 351 in Casella and Berger. 

The usual $1/(n-1)$ correction is known as [Bessel's correction](https://en.wikipedia.org/wiki/Bessel%27s_correction). 

Even further, suppose that $X_1, X_2, ... \sim \mathcal N(\mu_i, \sigma^2)$. 

Naively, one would think that the best estimates for 
$\hat \mu_i$ is just $X_i$, but the [James-Stein estimator/paradox](https://en.wikipedia.org/wiki/James%E2%80%93Stein_estimator) shows that by decreasing the variance 
we can come up with estimators that have lower
mean-squared-error. 
:::

A common misperception is that bias is always bad. In fact, allowing some bias usually improves performance by reducing variance. This is especially important when building a prediction model. Less flexible models tend to have greater bias, since they cannot fit the distributions as closely. 
More flexible models tend to have greater variance, since they
have more parameters to estimate. Since $\text{mse} = \text{bias}^2 + variance$, there is a trade-off, and mse is
minimized by setting the flexibility equal to some critical point.

## Moment Generating Functions 

The <span class='vocab'>moment generating function</span> (mgf)
of a random variable $X$ is 

$$M_X(t) = E[e^{tX}]$$
for $t \in \R$. 

The mgf is said to exist if $M_X(t)$ is finite in a neighborhood of zero. In other words, if there is some $h > 0$ such that 
$M_X(t) < \infty$ whenever $|t| < h$. 

This terminology is a little weird since the function always
exists but might be infinite.

Why is it called the "moment generating function"?

For all $k \in \{ 1, 2, 3, ... \}$,

$$EX^k = \frac{d^k}{dt^k} M_X(t) \lvert_{t=0.}$$

That is, the $k$th moment of $X$ equals the $k$th derivative
of $M_X(t)$ evaluated at $t=0$. 

So $M_X(t)$ is a function from which one can "generate" the moments simply by differentiating and evaluating at $t=0$. 

### Exponential Example 

If $X \sim \text{Exponential}(\lambda)$, then for $|t| < \lambda$, 

$$M_X(t) = E[e^{tx}] = \int_0^\infty \exp(tx) \lambda \exp(-\lambda x) dx$$
$$ = \lambda \int_0^\infty \exp(-(\lambda - t)x) dx$$

$$= \frac{\lambda}{\lambda - 1} \int_0^\infty (\lambda - t)\exp(-(\lambda - t)x)dx$$

In the last step, we multiplied and divided by $\lambda - t$ so that the inside is an exponential pdf with parameter $\lambda - t)$ (and thus has integral 1).

$$ = \frac{\lambda}{\lambda - 1} < \infty$$

for $|t| < \lambda.$

We can easily compute the moments of $X$ using the mgf. 

Without using the mgf, we'd have to use integration by parts to 
solve: 

$$EX^k = \int_0^\infty x^k \lambda e^{-\lambda x} dx,$$
which could be a bit painful for larger $k$. 

So instead, using the mgf, we get that the 1st and 2nd moments are: 

$$E X = \frac{d}{dt} \frac{\lambda}{\lambda - t} \big\lvert_{t=0} = \frac{\lambda}{(\lambda - t)^2} \big\lvert_{t=0} = \frac{1}{\lambda}$$

$$EX^2 = \frac{d^2}{dt^2} \frac{\lambda}{\lambda-t} \big\lvert_{t=0} = \frac{d}{dt} \frac{\lambda}{(\lambda - t)^2} \big\lvert_{t=0} = \frac{2\lambda(\lambda- t)}{(\lambda-t)^4} \big\lvert_{t=0} = \frac{2}{\lambda^2}. $$

Thus the variance of $X$ is 

$$\Var(X) = EX^2 - (EX)^2 = \frac{2}{\lambda^2} - \frac{1}{\lambda^2} = \frac{1}{\lambda^2}.$$

:::{.cooltip}
Could it be that the moments still exist even if the mgf
does not take on a finite value?

Recall that $$e^{tX} = \sum_{k=0}^\infty \frac{(tX)^k}{k!} \geq \frac{t^kX^k}{k!} \quad (X \geq 0)$$

So it might be that the moment generating function doesn't exist while the moments themselves do exist. 

We'll get to the characteristic function soon: 

$$\phi_X(t) = E(e^{itX}).$$

And $|e^{itX}| = 1$. 
:::

### Uniqueness of Moments

$X$ has <span class='vocab'>bounded support</span> if $P(|X| < c) = 1$ for some $c \in \R$.

Suppose $X$ and $Y$ have bounded support. Then $X \stackrel{d}{=} Y$ if and only if $EX^k = EY^k$ for all $k \in \{ 1, 2, ... \}$. 

If $M_X(t)$ and $M_Y(t)$ exist and are equal on a neighborhood of zero, then $X \stackrel{d}{=} Y$. 

:::{.hottip}
This does not hold in general for unbounded distributions. 
There's such an example in Casella and Berger. 
:::

## Differentiating under the integral sign 

Often we want to interchange the order of differentiation 
and integration. 

For example, for mgfs: 

$$\frac{d^k}{dt^k} M_X(t) \lvert_{t=0} = \frac{d^k}{dt^k} E\big(\exp(tX)\big) \lvert_{t=0}$$
$$ = E\big(\frac{d^k}{dt^k} \exp(tX)\lvert_{t=0}\big)$$
$$ = E\big( X^k \exp(tX) \lvert_{t=0}\big)$$
$$ = E(X^k).$$

This is using the fact that $\frac{d}{dt} e^{tx} = x e^{tx}$,
and hence $\frac{d^k}{dt^x} e^{tx} = x^k e^{tx}$.

The step where we swap the order of $\frac{d^k}{dt^k}$ and 
$E$ is called *differentiating under the integral sign.*

However, regularity conditions are needed for this to hold. 

Suppose that $f(x,t)$ is differentiable with respect to $t$ for
each $x$, and there exists a function $g(x,t)$ such that 

  1. for all $x$ and all $t'$ in a neighborhood of $t$
  $$\left\lvert \frac{\partial }{\partial t} f(x,t) \lvert_{t=t'} \right\rvert \leq g(x,t)$$
  
  2. $\int_{-\infty}^\infty g(x,t) dx < \infty$. 
  Then 
  $$\frac{d}{dt} \int_{-\infty}^\infty f(x,t) dx = \int_{-\infty}^\infty \frac{\partial}{\partial t} f(x,t) dx.$$
  
See Casella & Berger (Theorem 2.4.3) for a slightly more
general version. 
This proof uses one of the most important results in measure
theory: the dominated convergence theorem. 

We present a non-measure theoretic version of the 
<span class='vocab'>dominated convergence theorem</span> here, which is not fully 
general but gets the main idea across. 

Suppose that $f(x,t)$ is continuous at $t_0$ for each 
$x$ and there exists $g(x)$ such that 

  1. $|f(x,t)| \leq g(x)$ for all $x$ and all $t$, and 
  2. $\int_{-\infty}^\infty g(x) dx < \infty$.
  
Then 
$$\lim_{t\to t_0} \int_{-\infty}^\infty f(x,t) dx = \int_{-\infty}^{\infty} \lim_{t\to t_0} f(x,t) dx.$$

The dominated convergence theorem allows us to justify switching the order of limits and integrals. 

We can think about the dominated convergence theorem as describing a situation 
where we have a sequence of functions: 

$$f_1(x),\, f_2(x),\, f_3(x),\, \cdots$$

And what we're saying is $$\lim_{n \to \infty} \int f_n(x) dx = \int \left( \lim_{n \to \infty} f_n(x))\right) dx.$$

```{r, echo=FALSE, fig.align='center', out.width = '75%'}
knitr::include_graphics("standalone_figures/converging_normals/converging_normals.svg")
```

:::{.hottip}
A counter-example would be $f_n(x) = 1/n$, so $f_*(x) = 0$, but $\int f_n(x) dx > 0$ for all $n$. 

Another counter-example is $f_n(x) = \mathbb 1(n < x < n+1)$. The limit $f_*(x) = 0$ because for every 
$x$, as $n\to \infty$, there is an $N \in \mathbb N$ such that for all $N' > N$ $f_{N'}(x) = 0$. 

This is what the requirements around the existence of such a function $g(x)$ are telling us (that $g(x)$ is an 
envelope for all $f_n(x)$ and $\int_{-\infty}^\infty g(x) dx < \infty$. 
:::

# Families of Distributions

In statistics, families of distributions play a key role. 
Many statistical methods are based on assuming that the data are distributed according to some family of distributions. 
Estimation and inference then proceeds by finding the parameters of the family that could plausibly have 
generated the observed data.

For instance, if one assumes that data $X_1, ..., X_n$ are $\mathcal N(\mu, \sigma^2)$ distributed, then we could use maximum likelihood to estimate $\mu$ and $\sigma^2$ as: 

$$\hat \mu = \frac{1}{n} \sigma_{i=1}^n X_i \quad \quad \hat \sigma^2 = \frac{1}{n} \sum_{i=1}^n (X_i - \bar X)^2.$$

Many commonly used distributions have special properties that make them well-justified in particular applications.

Examples: 

  * Gaussians have the central limit theorem. 
  * Poissons have the law of small numbers.
  * Exponentials have memorylessness.
  * Pareto distributions have the power law.
  * Exponential families have maximum entropy.
  * The Poisson process is the limit of Bernoulli processes.
  
Selecting, combining, and/or transforming distributions according to the
"physics" of the data generating process is important for good statistical
modeling.

We often write the name of the distribution itself to denote the pdf/pmf. For instance, 
$\text{Uniform(x|a,b)$ denotes the pdf of $\text{Uniform(a,b)}$. 

If the distribution $X$ has been defined, say as $X \sim \text{Uniform}(a,b)$, another shorthand 
is to write $p(x|a,b)$ for the pdf/pmf. 

We will often denote pdfs or pmfs as $p(\cdot)$ instead of $f(\cdot)$. 

When dealing with multiple r.v.s, say $X$ and $Y$, it is common 
to simply write $p(x)$ and $p(y)$ for the pdf/pmf of $X$ and $Y$, respectively,
instead of $p_X(x)$ and $p_Y(y)$. In other words, the letters used ($x$ or $y$)
indicates which random variable we're talking about.

### Discrete Uniform Distribution 

The $\text{Uniform}(N)$ distribution has pmf 

$$p(x|N) = \frac{1}{N} \mathbb 1(x \in \{ 1, 2, ..., N \})$$

$$EX = (N+1)/2$$
$$\Var (X) = (N+1)(N+2)/12$$

More generally, for any set $\mathcal X$, the uniform distribution on $\mathcal X$ is
denoted $\text{Uniform}(\mathcal X)$. 

$\text{Uniform}(\mathcal X)$ represents maximal uncertainty in the outcome of a quantity 
$x \in \mathcal X$. It also maximizes the <span class='vocab'>entropy</span>,

$$H = - \sum_{x \in \mathcal X} p(x) \log p(x),$$

over all distributions of $\mathcal X$, when $\mathcal X$ is a finite set. 

One characteristic of entropy is that it can be defined for $\mathcal X$ that 
is not a subset of $\mathbb R$. 

Boltzmann was the first to study entropy, and it was later studied by Claude Shannon 
in information theory (applied to communication across lossy channels and [en-]coding theory).

### Hypergeometric Distribution 

The $\text{Hypergeometric}(N,M,K)$ distribution has pmf 

$$p(x|N,M,K) = \frac{{M \choose x}{N-M \choose K-x}}{N \choose K} \mathbb 1 (x \in \mathcal X),$$

where $\mathcal X = \{ x \in \{ 0, 1, ..., K\} : K - N + M \leq x \leq M\}.$

The parameters are $N, M, K \in \{ 0, 1, 2, ... \}$. 

The mean and variance of $X \sim \text{Hypergeometric}(N,M,K)$ are 
$$EX = KM/N$$
$$\Var(X) = \frac{KM}{N} \frac{(N-M)(N-K)}{N(N-1)}.$$

Another example, besides drawing balls for a lottery, is the scenario of testing
for defects in manufacturing. If a batch of $N$ items has $M$ defects and $K$
randomly selected items are tested, what is the probability that $x$ of the
tested items are defective?

Approaches outlined: 
  * Assume that $\frac{x}{K} \approx \frac{M}{N} \to x \approx \frac{MK}{N} = \E X$ and 
  choose $\hat M \approx \frac{xN}{K}$. This is called the <span class='vocab'>method of moments</span>. 
  * Look at the probability mass function $p(x|N,M,K) = f(M)$ and maximize with 
  respect to $0 \leq M \leq N$. Basically, let $\hat M = \argmax_{M} p(x|N,M,K)$. 
  This is the <span class='vocab'>maximum likelihood estimator (MLE)</span>.
  * <span class='vocab'>Simulation methods</span> where if, say we didn't know the analytic formula for the mean,
  we could empirically estimate the mean, and search for the distributions with simulated empirical mean 
  close to $x$. This comes up when models are analytically intractable: for example, a weather model. 
  * The Bayesian approach would be to consider the probably that $M$ as a random variable is a particular $m$ given 
  the parameters (the posterior):
  $$p(M=m|N,K,X=x) = \frac{p(X=x | N,K,M=m) p(M=m)}{\sum_{m'=0}^\infty p(X=x|N,K,M=m')p(M=m')$$
  If we've been doing testing for a long time, we may have a good prior for $p(M=m')$. 
  Two important quantities are the posterior mean: 
  $$\hat M = \E(M|N,K,X=x)$$
  And the maximum a posteriori estimate, 
  $$\hat M = \argmax_m p(M=m|N,K,X=x)$$
  
Probability is the forward process of going from the parameters to the
outcome. Inference (statistics) is the inductive process of going from data to
knowledge about the parameters (and generalized knowledge).

### Binomial Distribution 

The $\text{Binomial}(N,q)$ distribution has pmf 

$$p(x|N,q) = {N \choose x} q^x(1-q)^{N-x} \mathbb 1(x \in \mathcal X)$$

where $\mathcal X = \{ 0, 1, ..., N \}$. The parameters are $N \in \{ 1, 2, ... \}$
and $q \in (0,1)$. 

The mean and variance are 

$$\E X = Nq$$

$$\Var(X) = Nq(1-q)$$

## Independence of Random Variables

Random variables $X_1, ..., X_n$ are independent if 

$$P(X_1 \in A_1, ..., X_n \in A_n) = P(X_1 \in A_1) \cdots P(X_n \in A_n)$$
for all measurable subsets $A_1, ..., A_n \subset \R$. 

The $\t{Bernoulli}(q)$ distribution is the special case of 
$\t{Binomial}(N,q)$ when $N=1$. 

If $X_1, ..., X_n \sim \t{Bernoulli}(q)$ are independent, then 

$\sum_{i=1}^n X_i \sim \t{Binomial}(N,q).$

From this, it is easy to derive the mean of the Binomial distribution: 

$$\E\left( \sum_{i=1}^n X_i \right) = \sum_{i=1}^n \E X_i = \sum_{i=1}^n q = nq.$$

### Poisson Distribution 

The $\t{Poisson}(\lambda)$ distribution has pmf 

$$p(x|\lambda) = e^{-\lambda} \frac{\lambda^x}{x!} \mathbb 1(x \in \mathcal X)$$

where $\mathcal X = \{ 0, 1, 2,...\}$. The parameter $\lambda > 0$ is referred to 
as the <span class='vocab'>rate</span>, for reasons that will become clear when we study 
Poisson processes. 

The mean and variance of $X \sim \t{Poisson}(\lambda)$. 

The Poisson model is often a good model for counting the occurrences of independent rare events. 

Examples: 

  * In genomics, the number of reads covering a given locus is well-modeled as Poisson.
  * In physics, the number of photons hitting a detector during a given period of time is Poisson distributed.
  * In ecology, the number of organisms in a given region is often well-modeled as Poisson. 
  
This is all due to a special property of the Poisson distribution jokingly referred to as the 
"law of small numbers". 

The Poisson is a limit of Binomials: if $q_N \in (0,1)$ is such that 
$$N q_N \to \lambda$$
as $N \to \infty$ for some $\lambda > 0$, then for all $x \in \{ 0, 1, 2, ...\}$,

$$\t{Binomial}(x|N,q_N) \longrightarrow \t{Poisson}(x|\lambda).$$

### Geometric Distribution 

The $\t{Geometric}(q)$ distribution has pmf 

$$p(x|q) = (1-q)^{x-1}q \mathbb 1(x \in \mathcal X).$$

$$\E X = 1/q$$

$$\Var(X) = \frac{1-q}{q^2}.$$

